{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "signal-reset",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "analyzed-sponsorship",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy import mean\n",
    "from numpy import std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "outer-nashville",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-frequency",
   "metadata": {},
   "source": [
    "# About this Sprint​ ​\n",
    "### The purpose of this Sprint\n",
    "<li> Know the practical flow of machine learning\n",
    "<li> Complete a model with high generalization performance </li>\n",
    "    \n",
    "### How to learn\n",
    "    \n",
    "After making it possible to perform careful verification, we will proceed with model creation with high generalization performance, referring to the solution of others.\n",
    "\n",
    "## Machine learning flow\n",
    "\n",
    "Using Kaggle's Home Credit Default Risk competition as a starting point, we will learn the practical flow of machine learning. In particular, we will aim to complete a model with a high generalisation performance and appropriate validation.​ ​\n",
    "\n",
    "[Home Credit Default Risk | Kaggle](https://www.kaggle.com/c/home-credit-default-risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-probe",
   "metadata": {},
   "source": [
    "## Problem 1: Cross Validation\n",
    "In the pre-learning period, the verification data was divided first, and the index value was calculated for that, and verification was performed. (**Holdout method**) However, the accuracy varies depending on the division method; in practice **Cross Validation**. This is a method of performing splitting multiple times and performing learning and verification for each. KFold class is provided in scikit-learn for splitting multiple times.\n",
    "\n",
    "\n",
    "Create and execute code that Validation baseline model created in the pre-learning period assignment\n",
    "\n",
    "[sklearn.model_selection.KFold — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-default",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "objective-ministry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511, 122)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('application_train.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-addition",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "integrated-indiana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48744, 121)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('application_test.csv')\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-aurora",
   "metadata": {},
   "source": [
    "### Target Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "further-locking",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    282686\n",
       "1     24825\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['TARGET'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "posted-shakespeare",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['TARGET'].astype(int).plot.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "neither-sport",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float64    65\n",
       "int64      41\n",
       "object     16\n",
       "dtype: int64"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-justice",
   "metadata": {},
   "source": [
    "### Missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "alike-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "cooperative-hundred",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your selected dataframe has 122 columns.\n",
      "There are 67 columns that have missing values.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>% of Total Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>COMMONAREA_MEDI</th>\n",
       "      <td>214865</td>\n",
       "      <td>69.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COMMONAREA_AVG</th>\n",
       "      <td>214865</td>\n",
       "      <td>69.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COMMONAREA_MODE</th>\n",
       "      <td>214865</td>\n",
       "      <td>69.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NONLIVINGAPARTMENTS_MEDI</th>\n",
       "      <td>213514</td>\n",
       "      <td>69.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NONLIVINGAPARTMENTS_MODE</th>\n",
       "      <td>213514</td>\n",
       "      <td>69.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NONLIVINGAPARTMENTS_AVG</th>\n",
       "      <td>213514</td>\n",
       "      <td>69.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FONDKAPREMONT_MODE</th>\n",
       "      <td>210295</td>\n",
       "      <td>68.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LIVINGAPARTMENTS_MODE</th>\n",
       "      <td>210199</td>\n",
       "      <td>68.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LIVINGAPARTMENTS_MEDI</th>\n",
       "      <td>210199</td>\n",
       "      <td>68.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LIVINGAPARTMENTS_AVG</th>\n",
       "      <td>210199</td>\n",
       "      <td>68.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FLOORSMIN_MODE</th>\n",
       "      <td>208642</td>\n",
       "      <td>67.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FLOORSMIN_MEDI</th>\n",
       "      <td>208642</td>\n",
       "      <td>67.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FLOORSMIN_AVG</th>\n",
       "      <td>208642</td>\n",
       "      <td>67.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEARS_BUILD_MODE</th>\n",
       "      <td>204488</td>\n",
       "      <td>66.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEARS_BUILD_MEDI</th>\n",
       "      <td>204488</td>\n",
       "      <td>66.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEARS_BUILD_AVG</th>\n",
       "      <td>204488</td>\n",
       "      <td>66.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OWN_CAR_AGE</th>\n",
       "      <td>202929</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LANDAREA_AVG</th>\n",
       "      <td>182590</td>\n",
       "      <td>59.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LANDAREA_MEDI</th>\n",
       "      <td>182590</td>\n",
       "      <td>59.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LANDAREA_MODE</th>\n",
       "      <td>182590</td>\n",
       "      <td>59.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Missing Values  % of Total Values\n",
       "COMMONAREA_MEDI                   214865               69.9\n",
       "COMMONAREA_AVG                    214865               69.9\n",
       "COMMONAREA_MODE                   214865               69.9\n",
       "NONLIVINGAPARTMENTS_MEDI          213514               69.4\n",
       "NONLIVINGAPARTMENTS_MODE          213514               69.4\n",
       "NONLIVINGAPARTMENTS_AVG           213514               69.4\n",
       "FONDKAPREMONT_MODE                210295               68.4\n",
       "LIVINGAPARTMENTS_MODE             210199               68.4\n",
       "LIVINGAPARTMENTS_MEDI             210199               68.4\n",
       "LIVINGAPARTMENTS_AVG              210199               68.4\n",
       "FLOORSMIN_MODE                    208642               67.8\n",
       "FLOORSMIN_MEDI                    208642               67.8\n",
       "FLOORSMIN_AVG                     208642               67.8\n",
       "YEARS_BUILD_MODE                  204488               66.5\n",
       "YEARS_BUILD_MEDI                  204488               66.5\n",
       "YEARS_BUILD_AVG                   204488               66.5\n",
       "OWN_CAR_AGE                       202929               66.0\n",
       "LANDAREA_AVG                      182590               59.4\n",
       "LANDAREA_MEDI                     182590               59.4\n",
       "LANDAREA_MODE                     182590               59.4"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values = missing_values_table(data)\n",
    "missing_values.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "tender-compilation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NAME_CONTRACT_TYPE             2\n",
       "CODE_GENDER                    3\n",
       "FLAG_OWN_CAR                   2\n",
       "FLAG_OWN_REALTY                2\n",
       "NAME_TYPE_SUITE                7\n",
       "NAME_INCOME_TYPE               8\n",
       "NAME_EDUCATION_TYPE            5\n",
       "NAME_FAMILY_STATUS             6\n",
       "NAME_HOUSING_TYPE              6\n",
       "OCCUPATION_TYPE               18\n",
       "WEEKDAY_APPR_PROCESS_START     7\n",
       "ORGANIZATION_TYPE             58\n",
       "FONDKAPREMONT_MODE             4\n",
       "HOUSETYPE_MODE                 3\n",
       "WALLSMATERIAL_MODE             7\n",
       "EMERGENCYSTATE_MODE            2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select_dtypes('object').apply(pd.Series.nunique, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-labor",
   "metadata": {},
   "source": [
    "### Encoding categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "opening-blake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 columns were label encoded.\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le_count = 0\n",
    "\n",
    "# Iterate through the columns\n",
    "for col in data:\n",
    "    if data[col].dtype == 'object':\n",
    "        # If 2 or fewer unique categories\n",
    "        if len(list(data[col].unique())) <= 2:\n",
    "            # Train on the training data\n",
    "            le.fit(data[col])\n",
    "            # Transform both training and testing data\n",
    "            data[col] = le.transform(data[col])\n",
    "            test_data[col] = le.transform(test_data[col])\n",
    "            \n",
    "            # Keep track of how many columns were label encoded\n",
    "            le_count += 1\n",
    "            \n",
    "print('%d columns were label encoded.' % le_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "smart-ivory",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features shape:  (307511, 122)\n",
      "Testing Features shape:  (48744, 121)\n"
     ]
    }
   ],
   "source": [
    "print('Training Features shape: ', data.shape)\n",
    "print('Testing Features shape: ', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "minimal-tobacco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features shape:  (307511, 242)\n",
      "Testing Features shape:  (48744, 239)\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.get_dummies(data.drop(['TARGET'], axis=1))\n",
    "data_test = pd.get_dummies(test_data)\n",
    "print('Training Features shape: ', data_train.shape)\n",
    "print('Testing Features shape: ', data_test.shape)\n",
    "train_labels = data['TARGET']\n",
    "data_train['TARGET'] = train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-cycling",
   "metadata": {},
   "source": [
    "### Aligning Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "revolutionary-george",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features shape:  (307511, 240)\n",
      "Testing Features shape:  (48744, 239)\n"
     ]
    }
   ],
   "source": [
    "train_labels = data['TARGET']\n",
    "\n",
    "# Align the training and testing data, keep only columns present in both dataframes\n",
    "data_train, data_test = data_train.align(data_test, join = 'inner', axis = 1)\n",
    "\n",
    "# Add the target back in\n",
    "data_train['TARGET'] = train_labels\n",
    "\n",
    "print('Training Features shape: ', data_train.shape)\n",
    "print('Testing Features shape: ', data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-penalty",
   "metadata": {},
   "source": [
    "### Fill in missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "convertible-empty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (307511, 239)\n",
      "Testing data shape:  (48744, 239)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "if 'TARGET' in data_train:\n",
    "    train = data_train.drop(columns = ['TARGET'])\n",
    "else:\n",
    "    train = data_train.copy()\n",
    "\n",
    "# Feature names\n",
    "features = list(train.columns)\n",
    "\n",
    "# Median imputation of missing values\n",
    "imputer = SimpleImputer(strategy = 'median')\n",
    "\n",
    "# Scale each feature to 0-1\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "# Fit on the training data\n",
    "imputer.fit(train)\n",
    "\n",
    "# Transform both training and testing data\n",
    "train = imputer.transform(train)\n",
    "test = imputer.transform(data_test)\n",
    "\n",
    "# Repeat with the scaler\n",
    "scaler.fit(train)\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "print('Training data shape: ', train.shape)\n",
    "print('Testing data shape: ', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-cooper",
   "metadata": {},
   "source": [
    "### Cross Validation using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "musical-feature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.992430 (0.000313)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.99239074, 0.99226041, 0.99226041, 0.99252057, 0.99242301,\n",
       "       0.99245553, 0.99226041, 0.99330103, 0.99209782, 0.99232545])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression Model\n",
    "log_reg = LogisticRegression(C = 0.0001)\n",
    "#Cross Valition  \n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "#Data\n",
    "my_X = train[:,:-1]\n",
    "my_Y = train[:,-1]\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "scores = cross_val_score(log_reg, my_X, my_Y, cv=cv,scoring = 'accuracy',error_score=\"raise\")\n",
    "print('Accuracy: %.6f (%.6f)' % (mean(scores), std(scores)))\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-singer",
   "metadata": {},
   "source": [
    "### Cross Validation using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "unable-meaning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.992430 (0.000313)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.99239074, 0.99226041, 0.99226041, 0.99252057, 0.99242301,\n",
       "       0.99245553, 0.99226041, 0.99330103, 0.99209782, 0.99232545])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "scores = cross_val_score(rf, my_X, my_Y, cv=cv,scoring = 'accuracy',error_score=\"raise\")\n",
    "print('Accuracy: %.6f (%.6f)' % (mean(scores), std(scores)))\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-democrat",
   "metadata": {},
   "source": [
    "## Problem 2: Grid search\n",
    "So far, I haven't touched on the classifier parameters and used the default settings. Details of the parameters will be learned in future sprints. As a prerequisite for machine learning, it is necessary to select the optimum parameters according to the situation. Searching for the optimum parameters is called **parameter tuning**. A simple way to automate parameter tuning to some extent is **grid search**.\n",
    "\n",
    "\n",
    "Please use Scikit-learn's GridSearchCV to create the code for grid search. Then do some parameter tuning on the baseline model. Please refer to the official documentation of the method used to determine which parameters to tune.\n",
    "\n",
    "\n",
    "[sklearn.model_selection.GridSearchCV — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "\n",
    "\n",
    "To the GridSearchCV class, give the model, the search range, and the number of divisions for cross Validation as arguments. You don't need to use KFold class if you want to use it as it also includes cross Validation functionality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "equipped-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#Grid\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [3],\n",
    "    'max_features': [2],\n",
    "    'min_samples_leaf': [3],\n",
    "    'min_samples_split': [8, 10],\n",
    "    'n_estimators': [100, 200]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "widespread-failure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2,\n",
       "             estimator=RandomForestClassifier(max_depth=2, random_state=0),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'bootstrap': [True], 'max_depth': [3],\n",
       "                         'max_features': [2], 'min_samples_leaf': [3],\n",
       "                         'min_samples_split': [8, 10],\n",
       "                         'n_estimators': [100, 200]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 2, n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(my_X, my_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-preference",
   "metadata": {},
   "source": [
    "#### The Best Parameter:\n",
    "I have run GridSearch on many values but it took a lot of time and memory crash so i only reduce to few parameter and and 2 cross validation for the Grid Search and here is the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "atlantic-feeling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 3,\n",
       " 'max_features': 2,\n",
       " 'min_samples_leaf': 3,\n",
       " 'min_samples_split': 8,\n",
       " 'n_estimators': 100}"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-offset",
   "metadata": {},
   "source": [
    "## Problem 3: Survey from Kaggle Notebooks\n",
    "Discover and list various ideas from Kaggle's Notebooks.\n",
    "\n",
    "Some ideas I have found on Kaggle:\n",
    "<li> Use advance models: Random Forest, Gradient Boosting\n",
    "<li> Apply advance Feature Engineering: use domain knowledge features(credit_income_percent,annuity_income_percent,credit_term,days_emplyed_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-bulletin",
   "metadata": {},
   "source": [
    "## Problem 4: Creating a model with high generalization performance\n",
    "\n",
    "Please combine the idea found in Problem 3 and your own idea to create a model with high generalization performance.\n",
    "\n",
    "\n",
    "As a process, please summarize in a table what you did and how much the cross Validation results changed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-writer",
   "metadata": {},
   "source": [
    "### Domain Knowledge Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "revolutionary-nightlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_train = data_train.copy()\n",
    "domain_test = data_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "middle-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_train['CREDIT_INCOME_PERCENT'] = domain_train['AMT_CREDIT'] / domain_train['AMT_INCOME_TOTAL']\n",
    "domain_train['ANNUITY_INCOME_PERCENT'] = domain_train['AMT_ANNUITY'] / domain_train['AMT_INCOME_TOTAL']\n",
    "domain_train['CREDIT_TERM'] = domain_train['AMT_ANNUITY'] / domain_train['AMT_CREDIT']\n",
    "domain_train['DAYS_EMPLOYED_PERCENT'] = domain_train['DAYS_EMPLOYED'] / domain_train['DAYS_BIRTH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "interior-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_test['CREDIT_INCOME_PERCENT'] = domain_test['AMT_CREDIT'] / domain_test['AMT_INCOME_TOTAL']\n",
    "domain_test['ANNUITY_INCOME_PERCENT'] = domain_test['AMT_ANNUITY'] / domain_test['AMT_INCOME_TOTAL']\n",
    "domain_test['CREDIT_TERM'] = domain_test['AMT_ANNUITY'] / domain_test['AMT_CREDIT']\n",
    "domain_test['DAYS_EMPLOYED_PERCENT'] = domain_test['DAYS_EMPLOYED'] / domain_test['DAYS_BIRTH']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-collins",
   "metadata": {},
   "source": [
    "### Training with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "heard-symposium",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=2, random_state=0)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(train,train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-reduction",
   "metadata": {},
   "source": [
    "### Predict using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "incorporated-contractor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48744,)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_pred = rf.predict_proba(test)[:,1]\n",
    "rf_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "biblical-religious",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>0.105535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100005</td>\n",
       "      <td>0.082066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100013</td>\n",
       "      <td>0.080446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100028</td>\n",
       "      <td>0.089807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100038</td>\n",
       "      <td>0.077613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR    TARGET\n",
       "0      100001  0.105535\n",
       "1      100005  0.082066\n",
       "2      100013  0.080446\n",
       "3      100028  0.089807\n",
       "4      100038  0.077613"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit = data_test[['SK_ID_CURR']]\n",
    "submit['TARGET'] = rf_pred\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "confident-bronze",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to CSV\n",
    "submit.to_csv('rf_prediction.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-backing",
   "metadata": {},
   "source": [
    "### Predict using Random Forest for Domain Knowledge Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "round-repair",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511, 242)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "existing-shadow",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   26.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "source": [
    "train_domain = domain_train.drop(columns = 'TARGET')\n",
    "\n",
    "domain_features_names = list(train_domain.columns)\n",
    "\n",
    "# Impute the domainnomial features\n",
    "\n",
    "domain_features = imputer.fit_transform(train_domain)\n",
    "domain_features_test = imputer.transform(domain_test)\n",
    "\n",
    "# Scale the domainnomial features\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "domain_features = scaler.fit_transform(domain_features)\n",
    "domain_features_test = scaler.transform(domain_features_test)\n",
    "\n",
    "random_forest_domain = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n",
    "\n",
    "# Train on the training data\n",
    "random_forest_domain.fit(domain_features, train_labels)\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importance_values_domain = random_forest_domain.feature_importances_\n",
    "feature_importances_domain = pd.DataFrame({'feature': domain_features_names, 'importance': feature_importance_values_domain})\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = random_forest_domain.predict_proba(domain_features_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "considerable-edgar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48744,)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "lesser-kernel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "      <th>...</th>\n",
       "      <th>WALLSMATERIAL_MODE_Panel</th>\n",
       "      <th>WALLSMATERIAL_MODE_Stone, brick</th>\n",
       "      <th>WALLSMATERIAL_MODE_Wooden</th>\n",
       "      <th>EMERGENCYSTATE_MODE_No</th>\n",
       "      <th>EMERGENCYSTATE_MODE_Yes</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>CREDIT_INCOME_PERCENT</th>\n",
       "      <th>ANNUITY_INCOME_PERCENT</th>\n",
       "      <th>CREDIT_TERM</th>\n",
       "      <th>DAYS_EMPLOYED_PERCENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>351000.0</td>\n",
       "      <td>0.018801</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.007889</td>\n",
       "      <td>0.121978</td>\n",
       "      <td>0.060749</td>\n",
       "      <td>0.067329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>1129500.0</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.790750</td>\n",
       "      <td>0.132217</td>\n",
       "      <td>0.027598</td>\n",
       "      <td>0.070862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100004</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>0.010032</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.011814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>297000.0</td>\n",
       "      <td>0.008019</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.316167</td>\n",
       "      <td>0.219900</td>\n",
       "      <td>0.094941</td>\n",
       "      <td>0.159905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>0.028663</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.222222</td>\n",
       "      <td>0.179963</td>\n",
       "      <td>0.042623</td>\n",
       "      <td>0.152418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307506</th>\n",
       "      <td>456251</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>157500.0</td>\n",
       "      <td>254700.0</td>\n",
       "      <td>27558.0</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>0.032561</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.617143</td>\n",
       "      <td>0.174971</td>\n",
       "      <td>0.108198</td>\n",
       "      <td>0.025303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307507</th>\n",
       "      <td>456252</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>269550.0</td>\n",
       "      <td>12001.5</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>0.025164</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.743750</td>\n",
       "      <td>0.166687</td>\n",
       "      <td>0.044524</td>\n",
       "      <td>-17.580890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307508</th>\n",
       "      <td>456253</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>153000.0</td>\n",
       "      <td>677664.0</td>\n",
       "      <td>29979.0</td>\n",
       "      <td>585000.0</td>\n",
       "      <td>0.005002</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.429176</td>\n",
       "      <td>0.195941</td>\n",
       "      <td>0.044239</td>\n",
       "      <td>0.529266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307509</th>\n",
       "      <td>456254</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>171000.0</td>\n",
       "      <td>370107.0</td>\n",
       "      <td>20205.0</td>\n",
       "      <td>319500.0</td>\n",
       "      <td>0.005313</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.164368</td>\n",
       "      <td>0.118158</td>\n",
       "      <td>0.054592</td>\n",
       "      <td>0.400134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307510</th>\n",
       "      <td>456255</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>157500.0</td>\n",
       "      <td>675000.0</td>\n",
       "      <td>49117.5</td>\n",
       "      <td>675000.0</td>\n",
       "      <td>0.046220</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.285714</td>\n",
       "      <td>0.311857</td>\n",
       "      <td>0.072767</td>\n",
       "      <td>0.074869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>307511 rows × 244 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        SK_ID_CURR  NAME_CONTRACT_TYPE  FLAG_OWN_CAR  FLAG_OWN_REALTY  \\\n",
       "0           100002                   0             0                1   \n",
       "1           100003                   0             0                0   \n",
       "2           100004                   1             1                1   \n",
       "3           100006                   0             0                1   \n",
       "4           100007                   0             0                1   \n",
       "...            ...                 ...           ...              ...   \n",
       "307506      456251                   0             0                0   \n",
       "307507      456252                   0             0                1   \n",
       "307508      456253                   0             0                1   \n",
       "307509      456254                   0             0                1   \n",
       "307510      456255                   0             0                0   \n",
       "\n",
       "        CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0                  0          202500.0    406597.5      24700.5   \n",
       "1                  0          270000.0   1293502.5      35698.5   \n",
       "2                  0           67500.0    135000.0       6750.0   \n",
       "3                  0          135000.0    312682.5      29686.5   \n",
       "4                  0          121500.0    513000.0      21865.5   \n",
       "...              ...               ...         ...          ...   \n",
       "307506             0          157500.0    254700.0      27558.0   \n",
       "307507             0           72000.0    269550.0      12001.5   \n",
       "307508             0          153000.0    677664.0      29979.0   \n",
       "307509             0          171000.0    370107.0      20205.0   \n",
       "307510             0          157500.0    675000.0      49117.5   \n",
       "\n",
       "        AMT_GOODS_PRICE  REGION_POPULATION_RELATIVE  ...  \\\n",
       "0              351000.0                    0.018801  ...   \n",
       "1             1129500.0                    0.003541  ...   \n",
       "2              135000.0                    0.010032  ...   \n",
       "3              297000.0                    0.008019  ...   \n",
       "4              513000.0                    0.028663  ...   \n",
       "...                 ...                         ...  ...   \n",
       "307506         225000.0                    0.032561  ...   \n",
       "307507         225000.0                    0.025164  ...   \n",
       "307508         585000.0                    0.005002  ...   \n",
       "307509         319500.0                    0.005313  ...   \n",
       "307510         675000.0                    0.046220  ...   \n",
       "\n",
       "        WALLSMATERIAL_MODE_Panel  WALLSMATERIAL_MODE_Stone, brick  \\\n",
       "0                              0                                1   \n",
       "1                              0                                0   \n",
       "2                              0                                0   \n",
       "3                              0                                0   \n",
       "4                              0                                0   \n",
       "...                          ...                              ...   \n",
       "307506                         0                                1   \n",
       "307507                         0                                1   \n",
       "307508                         1                                0   \n",
       "307509                         0                                1   \n",
       "307510                         1                                0   \n",
       "\n",
       "        WALLSMATERIAL_MODE_Wooden  EMERGENCYSTATE_MODE_No  \\\n",
       "0                               0                       1   \n",
       "1                               0                       1   \n",
       "2                               0                       0   \n",
       "3                               0                       0   \n",
       "4                               0                       0   \n",
       "...                           ...                     ...   \n",
       "307506                          0                       1   \n",
       "307507                          0                       1   \n",
       "307508                          0                       1   \n",
       "307509                          0                       1   \n",
       "307510                          0                       1   \n",
       "\n",
       "        EMERGENCYSTATE_MODE_Yes  TARGET  CREDIT_INCOME_PERCENT  \\\n",
       "0                             0       1               2.007889   \n",
       "1                             0       0               4.790750   \n",
       "2                             0       0               2.000000   \n",
       "3                             0       0               2.316167   \n",
       "4                             0       0               4.222222   \n",
       "...                         ...     ...                    ...   \n",
       "307506                        0       0               1.617143   \n",
       "307507                        0       0               3.743750   \n",
       "307508                        0       0               4.429176   \n",
       "307509                        0       1               2.164368   \n",
       "307510                        0       0               4.285714   \n",
       "\n",
       "        ANNUITY_INCOME_PERCENT  CREDIT_TERM  DAYS_EMPLOYED_PERCENT  \n",
       "0                     0.121978     0.060749               0.067329  \n",
       "1                     0.132217     0.027598               0.070862  \n",
       "2                     0.100000     0.050000               0.011814  \n",
       "3                     0.219900     0.094941               0.159905  \n",
       "4                     0.179963     0.042623               0.152418  \n",
       "...                        ...          ...                    ...  \n",
       "307506                0.174971     0.108198               0.025303  \n",
       "307507                0.166687     0.044524             -17.580890  \n",
       "307508                0.195941     0.044239               0.529266  \n",
       "307509                0.118158     0.054592               0.400134  \n",
       "307510                0.311857     0.072767               0.074869  \n",
       "\n",
       "[307511 rows x 244 columns]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "arctic-flesh",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48744, 239)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "formal-camel",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = data_test[['SK_ID_CURR']]\n",
    "submit['TARGET'] = predictions\n",
    "# submit\n",
    "# # Save the submission dataframe\n",
    "submit.to_csv('rf_domain_knowledge_prediction.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-anime",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "original-christianity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "signal-excess",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape:  (307511, 238)\n",
      "Testing Data Shape:  (48744, 238)\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\ttrain's auc: 0.7989\ttrain's binary_logloss: 0.547642\tvalid's auc: 0.755463\tvalid's binary_logloss: 0.563361\n",
      "[400]\ttrain's auc: 0.82864\ttrain's binary_logloss: 0.518235\tvalid's auc: 0.755595\tvalid's binary_logloss: 0.54495\n",
      "Early stopping, best iteration is:\n",
      "[309]\ttrain's auc: 0.815791\ttrain's binary_logloss: 0.531059\tvalid's auc: 0.755755\tvalid's binary_logloss: 0.55289\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\ttrain's auc: 0.798638\ttrain's binary_logloss: 0.547974\tvalid's auc: 0.758354\tvalid's binary_logloss: 0.56326\n",
      "Early stopping, best iteration is:\n",
      "[282]\ttrain's auc: 0.811912\ttrain's binary_logloss: 0.53493\tvalid's auc: 0.758533\tvalid's binary_logloss: 0.555532\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\ttrain's auc: 0.7977\ttrain's binary_logloss: 0.549358\tvalid's auc: 0.763287\tvalid's binary_logloss: 0.564505\n",
      "Early stopping, best iteration is:\n",
      "[284]\ttrain's auc: 0.811252\ttrain's binary_logloss: 0.536199\tvalid's auc: 0.763823\tvalid's binary_logloss: 0.556295\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\ttrain's auc: 0.798947\ttrain's binary_logloss: 0.547854\tvalid's auc: 0.757823\tvalid's binary_logloss: 0.562315\n",
      "Early stopping, best iteration is:\n",
      "[240]\ttrain's auc: 0.805899\ttrain's binary_logloss: 0.540963\tvalid's auc: 0.758345\tvalid's binary_logloss: 0.558134\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\ttrain's auc: 0.798357\ttrain's binary_logloss: 0.548311\tvalid's auc: 0.758237\tvalid's binary_logloss: 0.564466\n",
      "Early stopping, best iteration is:\n",
      "[255]\ttrain's auc: 0.807459\ttrain's binary_logloss: 0.539362\tvalid's auc: 0.758535\tvalid's binary_logloss: 0.559106\n"
     ]
    }
   ],
   "source": [
    "def gradientboost(features, test_features, encoding = 'ohe', n_folds = 5):\n",
    "    \n",
    "    \"\"\"Train and test a light gradient boosting model using\n",
    "    cross validation. \n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        features (pd.DataFrame): \n",
    "            dataframe of training features to use \n",
    "            for training a model. Must include the TARGET column.\n",
    "        test_features (pd.DataFrame): \n",
    "            dataframe of testing features to use\n",
    "            for making predictions with the model. \n",
    "        encoding (str, default = 'ohe'): \n",
    "            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n",
    "            n_folds (int, default = 5): number of folds to use for cross validation\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        submission (pd.DataFrame): \n",
    "            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n",
    "            predicted by the model.\n",
    "        feature_importances (pd.DataFrame): \n",
    "            dataframe with the feature importances from the model.\n",
    "        valid_metrics (pd.DataFrame): \n",
    "            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the ids\n",
    "    train_ids = features['SK_ID_CURR']\n",
    "    test_ids = test_features['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = features['TARGET']\n",
    "    \n",
    "    # Remove the ids and target\n",
    "    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n",
    "    \n",
    "    \n",
    "    # One Hot Encoding\n",
    "    if encoding == 'ohe':\n",
    "        features = pd.get_dummies(features)\n",
    "        test_features = pd.get_dummies(test_features)\n",
    "        \n",
    "        # Align the dataframes by the columns\n",
    "        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n",
    "        \n",
    "        # No categorical indices to record\n",
    "        cat_indices = 'auto'\n",
    "    \n",
    "    # Integer label encoding\n",
    "    elif encoding == 'le':\n",
    "        \n",
    "        # Create a label encoder\n",
    "        label_encoder = LabelEncoder()\n",
    "        \n",
    "        # List for storing categorical indices\n",
    "        cat_indices = []\n",
    "        \n",
    "        # Iterate through each column\n",
    "        for i, col in enumerate(features):\n",
    "            if features[col].dtype == 'object':\n",
    "                # Map the categorical features to integers\n",
    "                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n",
    "                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n",
    "\n",
    "                # Record the categorical indices\n",
    "                cat_indices.append(i)\n",
    "    \n",
    "    # Catch error if label encoding scheme is not valid\n",
    "    else:\n",
    "        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n",
    "        \n",
    "    print('Training Data Shape: ', features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)\n",
    "    \n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "    \n",
    "    # Convert to np arrays\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "    \n",
    "    # Create the kfold object\n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n",
    "    \n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "    \n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    \n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(features):\n",
    "        \n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        # Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "        \n",
    "        # Create the model\n",
    "        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n",
    "                                   class_weight = 'balanced', learning_rate = 0.05, \n",
    "                                   reg_alpha = 0.1, reg_lambda = 0.1, \n",
    "                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(train_features, train_labels, eval_metric = 'auc',\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n",
    "                  early_stopping_rounds = 100, verbose = 200)\n",
    "        \n",
    "        # Record the best iteration\n",
    "        best_iteration = model.best_iteration_\n",
    "        \n",
    "        # Record the feature importances\n",
    "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "        \n",
    "        # Make predictions\n",
    "        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n",
    "        \n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
    "        \n",
    "        # Record the best score\n",
    "        valid_score = model.best_score_['valid']['auc']\n",
    "        train_score = model.best_score_['train']['auc']\n",
    "        \n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.enable()\n",
    "        del model, train_features, valid_features\n",
    "        gc.collect()\n",
    "        \n",
    "    # Make the submission dataframe\n",
    "    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n",
    "    \n",
    "    # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "    \n",
    "    # Overall validation score\n",
    "    valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "    \n",
    "    # Add the overall scores to the metrics\n",
    "    valid_scores.append(valid_auc)\n",
    "    train_scores.append(np.mean(train_scores))\n",
    "    \n",
    "    # Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "    \n",
    "    # Dataframe of validation scores\n",
    "    metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid': valid_scores}) \n",
    "    \n",
    "    return submission, feature_importances, metrics\n",
    "submission, fi, metrics = gradientboost(data_train, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "tender-miller",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>0.275991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100005</td>\n",
       "      <td>0.564269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100013</td>\n",
       "      <td>0.172737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100028</td>\n",
       "      <td>0.301978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100038</td>\n",
       "      <td>0.677455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48739</th>\n",
       "      <td>456221</td>\n",
       "      <td>0.232281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48740</th>\n",
       "      <td>456222</td>\n",
       "      <td>0.494844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48741</th>\n",
       "      <td>456223</td>\n",
       "      <td>0.259802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48742</th>\n",
       "      <td>456224</td>\n",
       "      <td>0.420240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48743</th>\n",
       "      <td>456250</td>\n",
       "      <td>0.719332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48744 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SK_ID_CURR    TARGET\n",
       "0          100001  0.275991\n",
       "1          100005  0.564269\n",
       "2          100013  0.172737\n",
       "3          100028  0.301978\n",
       "4          100038  0.677455\n",
       "...           ...       ...\n",
       "48739      456221  0.232281\n",
       "48740      456222  0.494844\n",
       "48741      456223  0.259802\n",
       "48742      456224  0.420240\n",
       "48743      456250  0.719332\n",
       "\n",
       "[48744 rows x 2 columns]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "raised-lodging",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('lbg.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "upset-drove",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape:  (307511, 242)\n",
      "Testing Data Shape:  (48744, 242)\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\ttrain's auc: 0.804378\ttrain's binary_logloss: 0.541831\tvalid's auc: 0.761596\tvalid's binary_logloss: 0.557466\n",
      "Early stopping, best iteration is:\n",
      "[262]\ttrain's auc: 0.814303\ttrain's binary_logloss: 0.531734\tvalid's auc: 0.762001\tvalid's binary_logloss: 0.551324\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\ttrain's auc: 0.804064\ttrain's binary_logloss: 0.542335\tvalid's auc: 0.765642\tvalid's binary_logloss: 0.55762\n",
      "Early stopping, best iteration is:\n",
      "[227]\ttrain's auc: 0.808533\ttrain's binary_logloss: 0.537794\tvalid's auc: 0.766055\tvalid's binary_logloss: 0.554817\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\ttrain's auc: 0.803567\ttrain's binary_logloss: 0.543168\tvalid's auc: 0.77026\tvalid's binary_logloss: 0.557961\n",
      "Early stopping, best iteration is:\n",
      "[241]\ttrain's auc: 0.810407\ttrain's binary_logloss: 0.536195\tvalid's auc: 0.770514\tvalid's binary_logloss: 0.553663\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\ttrain's auc: 0.804814\ttrain's binary_logloss: 0.541677\tvalid's auc: 0.765915\tvalid's binary_logloss: 0.556067\n",
      "Early stopping, best iteration is:\n",
      "[282]\ttrain's auc: 0.81839\ttrain's binary_logloss: 0.527917\tvalid's auc: 0.766199\tvalid's binary_logloss: 0.547916\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\ttrain's auc: 0.804689\ttrain's binary_logloss: 0.541666\tvalid's auc: 0.764897\tvalid's binary_logloss: 0.559121\n",
      "[400]\ttrain's auc: 0.834793\ttrain's binary_logloss: 0.510894\tvalid's auc: 0.765347\tvalid's binary_logloss: 0.540473\n",
      "Early stopping, best iteration is:\n",
      "[361]\ttrain's auc: 0.82991\ttrain's binary_logloss: 0.51602\tvalid's auc: 0.76544\tvalid's binary_logloss: 0.543625\n"
     ]
    }
   ],
   "source": [
    "submission_domain, fi_domain, metrics_domain = gradientboost(domain_train, domain_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "pressed-matthew",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_domain\n",
    "submission_domain.to_csv('lgb_domain_feature.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-sunset",
   "metadata": {},
   "source": [
    "## Problem 5: Final model selection\n",
    "Ultimately, choose a model that says it's good and submit the estimated results to Kaggle to see your score. Please describe what kind of idea you adopted and what the score was"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-carter",
   "metadata": {},
   "source": [
    "### Public Score Result:\n",
    "<li> Previous Submission using Logarit Regression: 0.68426 </li>\n",
    "<li> Random Forest: 0.42773 </li>\n",
    "<li> Random Forest with Domain Knowledge: 0.69985 </li>\n",
    "<li> Gradient Boost: 0.74453</li>\n",
    "<li> Gradient Boost with Domain Knowledge: 0.76053</li>\n",
    "\n",
    "### Public Score Result:\n",
    "<li> Previous Submission using Logarit Regression: 0.68426 </li>\n",
    "<li> Random Forest: 0.43378 </li>\n",
    "<li> Random Forest with Domain Knowledge: 0.70319 </li>\n",
    "<li> Gradient Boost: 0.74611 </li>\n",
    "<li> Gradient Boost with Domain Knowledge: 0.76074</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-easter",
   "metadata": {},
   "source": [
    "For all the model and paramter i have tried, Gradient Boosting is the best one. With 0.76053 in public and 0.76074 in private. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "effective-mozambique",
   "metadata": {},
   "source": [
    "## The purpose of this Sprint\n",
    "\n",
    "<li> Understanding logistic regression through scratch </li>\n",
    "<li> Learn the basics about classification problems </li>\n",
    "\n",
    "## How to learn\n",
    "\n",
    "After implementing Logistic Regression with scratch, we will do learning and verification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-proportion",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "alleged-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "stylish-journalism",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-laptop",
   "metadata": {},
   "source": [
    "## 2. Scratch Logistic Regression\n",
    "\n",
    "We will create a class for logistic regression from scratch. We will implement the algorithm using only the minimum library such as NumPy.\n",
    "\n",
    "\n",
    "Below is a template. Add code to this ScratchLogisticRegression class. Unlike linear regression, there are two methods for estimating relations: a predict method that outputs a label and a predict_proba method that outputs a probability.\n",
    "\n",
    "```\n",
    "class ScratchLogisticRegression():\n",
    "    \"\"\"\n",
    "    Scratch implementation of logistic regression\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      Number of iterations\n",
    "    lr : float\n",
    "      Learning rate\n",
    "    no_bias : bool\n",
    "      True if no bias term is included\n",
    "    verbose : bool\n",
    "      True to output the learning process\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : The following form of ndarray, shape (n_features,)\n",
    "      Parameters\n",
    "    self.loss : The following form of ndarray, shape (self.iter,)\n",
    "      Record losses on training data\n",
    "    self.val_loss : The following form of ndarray, shape (self.iter,)\n",
    "      Record loss on validation data\n",
    "    \"\"\"\n",
    "    def __init__(self, num_iter, lr, bias, verbose):\n",
    "        # Record hyperparameters as attributes\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.bias = bias\n",
    "        self.verbose = verbose\n",
    "        # Prepare an array to record the loss\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        Learn logistic regression. If validation data is entered, the loss and accuracy for it are also calculated for each iteration.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            Features of training data\n",
    "        y : The following form of ndarray, shape (n_samples,)\n",
    "            Correct answer value of training data\n",
    "        X_val : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            Features of verification data\n",
    "        y_val : The following form of ndarray, shape (n_samples,)\n",
    "            Correct value of verification data\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            #Output learning process when verbose is set to True\n",
    "            print()\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Estimate the label using logistic regression.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            sample\n",
    "        Returns\n",
    "        -------\n",
    "            The following form of ndarray, shape (n_samples, 1)\n",
    "            Estimated result by logistic regression\n",
    "        \"\"\"\n",
    "        pass\n",
    "        return\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Estimate the probability using logistic regression.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            sample\n",
    "        Returns\n",
    "        -------\n",
    "            The following form of ndarray, shape (n_samples, 1)\n",
    "            Estimated result by logistic regression\n",
    "        \"\"\"\n",
    "        pass\n",
    "        return\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "selected-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchLogisticRegression():\n",
    "    \"\"\"\n",
    "    Scratch implementation of logistic regression\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      Number of iterations\n",
    "    lr : float\n",
    "      Learning rate\n",
    "    no_bias : bool\n",
    "      True if no bias term is included\n",
    "    verbose : bool\n",
    "      True to output the learning process\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : The following form of ndarray, shape (n_features,)\n",
    "      Parameters\n",
    "    self.loss : The following form of ndarray, shape (self.iter,)\n",
    "      Record losses on training data\n",
    "    self.val_loss : The following form of ndarray, shape (self.iter,)\n",
    "      Record loss on validation data\n",
    "    \"\"\"\n",
    "    def __init__(self, num_iter, lr, bias, verbose):\n",
    "        # Record hyperparameters as attributes\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.no_bias = bias\n",
    "        self.verbose = verbose\n",
    "        # Prepare an array to record the loss\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        Learn logistic regression. If validation data is entered, the loss and accuracy for it are also calculated for each iteration.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            Features of training data\n",
    "        y : The following form of ndarray, shape (n_samples,)\n",
    "            Correct answer value of training data\n",
    "        X_val : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            Features of verification data\n",
    "        y_val : The following form of ndarray, shape (n_samples,)\n",
    "            Correct value of verification data\n",
    "        \"\"\"\n",
    "        if self.no_bias is not True:\n",
    "            x1 = self._linear_hypothesis(X)\n",
    "            self.theta = np.random.random(x1.shape[1])\n",
    "        else:\n",
    "            x1 = X\n",
    "            self.theta = np.random.random(x1.shape[1])\n",
    "            \n",
    "        for i in range(self.iter):\n",
    "            y1 = self.sigmoid_function(np.dot(x1,self.theta.T))\n",
    "            error = y1 - y\n",
    "            self.loss[i] += np.mean(error**2)/2\n",
    "            \n",
    "            if X_val is not None:\n",
    "                if self.no_bias is not True:\n",
    "                    x2 = self._linear_hypothesis(X_val)\n",
    "                else:\n",
    "                    x2 = X_val\n",
    "                y2 = self.sigmoid_function(np.dot(x2,self.theta))\n",
    "                error_val = y2 - y_val\n",
    "                self.val_loss[i] += np.mean(error_val**2)/2\n",
    "            \n",
    "            self._gradient_descent(x1,error)\n",
    "                \n",
    "            if self.verbose:\n",
    "                #Output learning process when verbose is set to True\n",
    "                print('n_iter',i,\n",
    "                    'loss',self.loss[i],\n",
    "                    'weight',self.theta)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Estimate the label using logistic regression.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            sample\n",
    "        Returns\n",
    "        -------\n",
    "            The following form of ndarray, shape (n_samples, 1)\n",
    "            Estimated result by logistic regression\n",
    "        \"\"\"\n",
    "        if self.no_bias is not True:\n",
    "            return np.round(self.sigmoid_function(np.dot(self._linear_hypothesis(X),self.theta)))\n",
    "        return np.round(self.sigmoid_function(np.dot(X,self.theta)))\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Estimate the probability using logistic regression.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            sample\n",
    "        Returns\n",
    "        -------\n",
    "            The following form of ndarray, shape (n_samples, 1)\n",
    "            Estimated result by logistic regression\n",
    "        \"\"\"\n",
    "        if self.no_bias is not True:\n",
    "            return self.sigmoid_function(np.dot(self._linear_hypothesis(X),self.theta))\n",
    "        return self.sigmoid_function(np.dot(X,self.theta))\n",
    "    def _gradient_descent(self,X,error):\n",
    "        self.theta = self.theta - self.lr*np.dot(error,X)/len(X)\n",
    "    def sigmoid_function(self,z):\n",
    "        return 1/(1+np.exp(-z))    \n",
    "    def _linear_hypothesis(self,X):\n",
    "        x1 = X\n",
    "        x0 = np.ones(x1.shape[0]).reshape(-1, 1)\n",
    "        return np.concatenate([x0, x1],axis=1)\n",
    "    def getWeight(self):\n",
    "        return self.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "quarterly-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "active-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data \n",
    "target = iris.target\n",
    "names = iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "headed-milan",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X, columns=iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "fiscal-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_axis(['sl', 'sw', 'pl','pw'], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "arranged-queens",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sl</th>\n",
       "      <th>sw</th>\n",
       "      <th>pl</th>\n",
       "      <th>pw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sl   sw   pl   pw\n",
       "0  5.1  3.5  1.4  0.2\n",
       "1  4.9  3.0  1.4  0.2\n",
       "2  4.7  3.2  1.3  0.2\n",
       "3  4.6  3.1  1.5  0.2\n",
       "4  5.0  3.6  1.4  0.2"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "compatible-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['species'] = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "processed-stewart",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sl</th>\n",
       "      <th>sw</th>\n",
       "      <th>pl</th>\n",
       "      <th>pw</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sl   sw   pl   pw  species\n",
       "0   5.1  3.5  1.4  0.2        0\n",
       "1   4.9  3.0  1.4  0.2        0\n",
       "2   4.7  3.2  1.3  0.2        0\n",
       "3   4.6  3.1  1.5  0.2        0\n",
       "4   5.0  3.6  1.4  0.2        0\n",
       "..  ...  ...  ...  ...      ...\n",
       "95  5.7  3.0  4.2  1.2        1\n",
       "96  5.7  2.9  4.2  1.3        1\n",
       "97  6.2  2.9  4.3  1.3        1\n",
       "98  5.1  2.5  3.0  1.1        1\n",
       "99  5.7  2.8  4.1  1.3        1\n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "joined-mobile",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "exterior-governor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "rapid-instrument",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-sight",
   "metadata": {},
   "source": [
    "### Remove setosa class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "mounted-correlation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['species']!=0]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "significant-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['species'] = df['species'].replace(to_replace= [1, 2], value = [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "classified-gnome",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sl</th>\n",
       "      <th>sw</th>\n",
       "      <th>pl</th>\n",
       "      <th>pw</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>7.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>6.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sl   sw   pl   pw  species\n",
       "50   7.0  3.2  4.7  1.4        0\n",
       "51   6.4  3.2  4.5  1.5        0\n",
       "52   6.9  3.1  4.9  1.5        0\n",
       "53   5.5  2.3  4.0  1.3        0\n",
       "54   6.5  2.8  4.6  1.5        0\n",
       "..   ...  ...  ...  ...      ...\n",
       "145  6.7  3.0  5.2  2.3        1\n",
       "146  6.3  2.5  5.0  1.9        1\n",
       "147  6.5  3.0  5.2  2.0        1\n",
       "148  6.2  3.4  5.4  2.3        1\n",
       "149  5.9  3.0  5.1  1.8        1\n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "choice-roulette",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length X: 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sl</th>\n",
       "      <th>sw</th>\n",
       "      <th>pl</th>\n",
       "      <th>pw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>7.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>6.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sl   sw   pl   pw\n",
       "50  7.0  3.2  4.7  1.4\n",
       "51  6.4  3.2  4.5  1.5\n",
       "52  6.9  3.1  4.9  1.5\n",
       "53  5.5  2.3  4.0  1.3\n",
       "54  6.5  2.8  4.6  1.5"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "print(\"Length X:\",len(X))\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "official-sussex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length Y:  100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50    0\n",
       "51    0\n",
       "52    0\n",
       "53    0\n",
       "54    0\n",
       "Name: species, dtype: int64"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = df.iloc[:,-1]\n",
    "print(\"Length Y: \",len(Y))\n",
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "efficient-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.to_numpy(),Y.to_numpy(),test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "junior-unemployment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_trans.shape: (75, 4)\n",
      "X_test_trans.shape: (25, 4)\n",
      "y_train.shape: (75,)\n",
      "y_test.shape: (25,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train_trans.shape:\", X_train.shape)\n",
    "print(\"X_test_trans.shape:\", X_test.shape)\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "print(\"y_test.shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "induced-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_trans = scaler.transform(X_train)\n",
    "X_test_trans = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "single-virgin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 4)\n",
      "(75,)\n",
      "(25, 4)\n",
      "(25,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_trans.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test_trans.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-barbados",
   "metadata": {},
   "source": [
    "## Problem 1: Hypothetical function\n",
    "Please implement the method of logistic regression assumption function in ScratchLogisticRegression class.\n",
    "\n",
    "\n",
    "The assumed function for logistic regression is the assumed function for linear regression passed through the Sigmoid function. The sigmoid function is represented by the following equation\n",
    "\n",
    "<center> $g(z) = \\frac{1}{1 + e^{-z}}$ </center>\n",
    " \n",
    " The linear regression hypothesis function was:\n",
    " \n",
    " <center> $ h_{\\theta}(x) = \\theta^{T}.x $</center>\n",
    "\n",
    " Put together, the logistic regression assumption function is\n",
    " \n",
    " <center> $ h_{\\theta} = \\frac{1}{1 + e^{-\\theta^{T}.x}}  $</center>\n",
    " \n",
    " Consider the following equation where\n",
    " \n",
    "***x***: Feature vector\n",
    "\n",
    "***θ***: Parameter (weight) vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-disability",
   "metadata": {},
   "source": [
    "## Problem 2: Steepest descent\n",
    "Implement the steepest descent method for training. Add a method _gradient_descent that updates the parameters as in the following equation, and\n",
    "Please it call from the fit method.\n",
    "\n",
    "![alt text](fitMethod.png \"Fit Method\")\n",
    "\n",
    "***α***:Learning rate\n",
    "\n",
    "***i***:Sample index\n",
    "\n",
    "***j***:Feature index\n",
    "\n",
    "***m***:Number of data entered\n",
    "\n",
    "***$h_\\theta$***:Hypothtical function\n",
    "\n",
    "Consider the following equation where \n",
    "\n",
    "***x***:Feature vector\n",
    "\n",
    "***$\\theta$***:Parameter(weight) vector\n",
    "\n",
    "***$x(i)$***: Feature vector of i-th sample\n",
    "\n",
    "***$y(i)$***: Correct asnwer label of i-th sample\n",
    "\n",
    "***$ \\theta_{j}$***: jth parameter(weight)\n",
    "\n",
    "***$ \\lambda $***: Regularization parameter\n",
    "\n",
    "The above formula contains a regularization term. Regularization terms are used to prevent overfitting. It is an intercept \n",
    "θ\n",
    "​ ​\n",
    "0\n",
    "Is not included in the regularization term so that the coefficients for features, excluding the intercept, can be discussed from the same point of view."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-firmware",
   "metadata": {},
   "source": [
    "## Problem 3: Estimated\n",
    "Please implement the estimation mechanism. Add to the predict method and predict_proba method included in the template of ScratchLogisticRegression class.\n",
    "\n",
    "Hypothetical function\n",
    "h\n",
    "θ\n",
    "(\n",
    "x\n",
    ")\n",
    "The output of is the return value of predict_proba, and the value is labeled as 1 and 0 with a threshold value, which is the return value of predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-schema",
   "metadata": {},
   "source": [
    "## Problem 4: Objective function\n",
    "Implement the Objective function (loss function) of the logistic regression expressed in the following formula And make sure that this is recorded inself.loss, self.val_loss .\n",
    "\n",
    "\n",
    "Note that this formula contains a regularization term.\n",
    "\n",
    "\n",
    "*If you cannot see the formula, please see DIVER full screen.\n",
    "\n",
    "![alt text](objectiveFunction.png \"Object tive function\")\n",
    "\n",
    "***m***: Number of data entered\n",
    "\n",
    "***h​ ​θ​ ​(​ ​)***: Hypothetical function\n",
    "\n",
    "***θ***:Parameter (weight) vector\n",
    "\n",
    "***x​ ​(​ ​i​ ​)***: Feature vector of i-th sample\n",
    "\n",
    "***y​ ​(​ ​i​ ​)***:Correct answer label of i-th sample\n",
    "\n",
    "***θ​ ​j***: jth parameter (weight)\n",
    " \n",
    " \n",
    "Silhouette factor\n",
    "\n",
    "***n***: Number of features\n",
    " \n",
    "***λ***: Regularization parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-gathering",
   "metadata": {},
   "source": [
    "## Problem 5: Learning and estimation\n",
    "Learn and estimate the scratch implementation for the binary classification of virgicolor and virginica in the iris data set provided in the Introduction to Scratch Machine Learning Sprint.\n",
    "\n",
    "\n",
    "Compare this with the scikit-learn implementation and see if it works correctly.\n",
    "\n",
    "\n",
    "Use scikit-learn for indicator values such as Accuracy, Precision and Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "specified-syndication",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = ScratchLogisticRegression(1000,0.02,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "stopped-blink",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter 0 loss 0.046246028564086356 weight [0.25196681 0.73826353 0.17963304 0.52331139 0.91312316]\n",
      "n_iter 1 loss 0.046149613118377494 weight [0.25100662 0.73789641 0.1791384  0.52500851 0.91559778]\n",
      "n_iter 2 loss 0.046053692949809315 weight [0.25005047 0.73752754 0.17864276 0.52670095 0.91806645]\n",
      "n_iter 3 loss 0.04595826389968233 weight [0.24909834 0.73715696 0.17814613 0.52838874 0.92052919]\n",
      "n_iter 4 loss 0.04586332186166199 weight [0.24815021 0.73678467 0.17764854 0.53007191 0.92298604]\n",
      "n_iter 5 loss 0.04576886278089363 weight [0.24720605 0.7364107  0.17714999 0.53175049 0.92543702]\n",
      "n_iter 6 loss 0.045674882653135705 weight [0.24626584 0.73603507 0.17665051 0.53342451 0.92788218]\n",
      "n_iter 7 loss 0.045581377523910845 weight [0.24532956 0.73565779 0.17615011 0.53509399 0.93032154]\n",
      "n_iter 8 loss 0.04548834348767424 weight [0.24439719 0.73527889 0.17564881 0.53675897 0.93275514]\n",
      "n_iter 9 loss 0.045395776686999186 weight [0.24346871 0.73489838 0.17514661 0.53841946 0.935183  ]\n",
      "n_iter 10 loss 0.045303673311778925 weight [0.2425441  0.73451628 0.17464354 0.54007551 0.93760515]\n",
      "n_iter 11 loss 0.04521202959844495 weight [0.24162334 0.73413261 0.17413962 0.54172713 0.94002163]\n",
      "n_iter 12 loss 0.045120841829200946 weight [0.2407064  0.73374739 0.17363484 0.54337434 0.94243247]\n",
      "n_iter 13 loss 0.045030106331272295 weight [0.23979327 0.73336064 0.17312924 0.54501719 0.9448377 ]\n",
      "n_iter 14 loss 0.04493981947617058 weight [0.23888392 0.73297237 0.17262282 0.54665569 0.94723734]\n",
      "n_iter 15 loss 0.04484997767897286 weight [0.23797834 0.7325826  0.1721156  0.54828987 0.94963142]\n",
      "n_iter 16 loss 0.044760577397615325 weight [0.2370765  0.73219136 0.1716076  0.54991976 0.95201998]\n",
      "n_iter 17 loss 0.04467161513220104 weight [0.23617839 0.73179865 0.17109882 0.55154537 0.95440304]\n",
      "n_iter 18 loss 0.04458308742432139 weight [0.23528399 0.73140449 0.17058928 0.55316674 0.95678063]\n",
      "n_iter 19 loss 0.04449499085639099 weight [0.23439327 0.7310089  0.17007899 0.5547839  0.95915279]\n",
      "n_iter 20 loss 0.04440732205099568 weight [0.23350621 0.7306119  0.16956797 0.55639685 0.96151953]\n",
      "n_iter 21 loss 0.04432007767025332 weight [0.23262281 0.7302135  0.16905623 0.55800564 0.96388089]\n",
      "n_iter 22 loss 0.04423325441518716 weight [0.23174303 0.72981372 0.16854378 0.55961028 0.96623689]\n",
      "n_iter 23 loss 0.04414684902511143 weight [0.23086687 0.72941258 0.16803064 0.56121081 0.96858756]\n",
      "n_iter 24 loss 0.04406085827702885 weight [0.22999429 0.72901008 0.16751682 0.56280723 0.97093294]\n",
      "n_iter 25 loss 0.04397527898503991 weight [0.22912528 0.72860626 0.16700233 0.56439958 0.97327303]\n",
      "n_iter 26 loss 0.043890107999763533 weight [0.22825983 0.72820112 0.16648719 0.56598788 0.97560788]\n",
      "n_iter 27 loss 0.04380534220776888 weight [0.22739791 0.72779467 0.1659714  0.56757215 0.97793751]\n",
      "n_iter 28 loss 0.04372097853101818 weight [0.22653951 0.72738694 0.16545498 0.56915241 0.98026194]\n",
      "n_iter 29 loss 0.043637013926320134 weight [0.22568461 0.72697794 0.16493794 0.5707287  0.98258121]\n",
      "n_iter 30 loss 0.043553445384793846 weight [0.22483319 0.72656768 0.16442029 0.57230102 0.98489533]\n",
      "n_iter 31 loss 0.04347026993134283 weight [0.22398523 0.72615618 0.16390204 0.57386941 0.98720433]\n",
      "n_iter 32 loss 0.04338748462413924 weight [0.22314071 0.72574345 0.16338322 0.57543389 0.98950824]\n",
      "n_iter 33 loss 0.04330508655411758 weight [0.22229962 0.72532951 0.16286382 0.57699447 0.99180709]\n",
      "n_iter 34 loss 0.04322307284447821 weight [0.22146194 0.72491437 0.16234385 0.57855118 0.99410089]\n",
      "n_iter 35 loss 0.0431414406502 weight [0.22062765 0.72449805 0.16182334 0.58010405 0.99638967]\n",
      "n_iter 36 loss 0.043060187157562255 weight [0.21979673 0.72408055 0.16130229 0.58165308 0.99867347]\n",
      "n_iter 37 loss 0.0429793095836755 weight [0.21896917 0.7236619  0.16078071 0.58319831 1.00095229]\n",
      "n_iter 38 loss 0.04289880517602104 weight [0.21814496 0.72324211 0.16025861 0.58473976 1.00322618]\n",
      "n_iter 39 loss 0.04281867121199907 weight [0.21732406 0.72282119 0.15973601 0.58627744 1.00549514]\n",
      "n_iter 40 loss 0.042738904998485094 weight [0.21650647 0.72239915 0.15921291 0.58781138 1.00775921]\n",
      "n_iter 41 loss 0.04265950387139465 weight [0.21569217 0.721976   0.15868932 0.58934159 1.0100184 ]\n",
      "n_iter 42 loss 0.0425804651952559 weight [0.21488114 0.72155177 0.15816526 0.5908681  1.01227275]\n",
      "n_iter 43 loss 0.042501786362790187 weight [0.21407337 0.72112647 0.15764074 0.59239093 1.01452227]\n",
      "n_iter 44 loss 0.042423464794500154 weight [0.21326884 0.72070009 0.15711576 0.5939101  1.01676699]\n",
      "n_iter 45 loss 0.04234549793826546 weight [0.21246754 0.72027267 0.15659033 0.59542562 1.01900693]\n",
      "n_iter 46 loss 0.04226788326894577 weight [0.21166944 0.71984421 0.15606447 0.59693753 1.02124211]\n",
      "n_iter 47 loss 0.04219061828799099 weight [0.21087453 0.71941473 0.15553818 0.59844583 1.02347256]\n",
      "n_iter 48 loss 0.0421137005230585 weight [0.21008279 0.71898423 0.15501148 0.59995054 1.0256983 ]\n",
      "n_iter 49 loss 0.04203712752763726 weight [0.20929422 0.71855272 0.15448437 0.60145169 1.02791935]\n",
      "n_iter 50 loss 0.04196089688067876 weight [0.20850879 0.71812023 0.15395686 0.6029493  1.03013573]\n",
      "n_iter 51 loss 0.04188500618623441 weight [0.20772649 0.71768677 0.15342897 0.60444338 1.03234747]\n",
      "n_iter 52 loss 0.041809453073099606 weight [0.2069473  0.71725233 0.1529007  0.60593395 1.03455458]\n",
      "n_iter 53 loss 0.04173423519446388 weight [0.20617121 0.71681695 0.15237205 0.60742103 1.03675709]\n",
      "n_iter 54 loss 0.04165935022756743 weight [0.2053982  0.71638062 0.15184305 0.60890464 1.03895502]\n",
      "n_iter 55 loss 0.041584795873363674 weight [0.20462826 0.71594336 0.1513137  0.6103848  1.04114839]\n",
      "n_iter 56 loss 0.04151056985618769 weight [0.20386137 0.71550518 0.150784   0.61186152 1.04333722]\n",
      "n_iter 57 loss 0.04143666992343056 weight [0.20309752 0.71506609 0.15025397 0.61333483 1.04552154]\n",
      "n_iter 58 loss 0.04136309384521932 weight [0.20233669 0.71462611 0.14972362 0.61480474 1.04770135]\n",
      "n_iter 59 loss 0.041289839414102594 weight [0.20157886 0.71418524 0.14919295 0.61627126 1.0498767 ]\n",
      "n_iter 60 loss 0.0412169044447416 weight [0.20082403 0.7137435  0.14866196 0.61773443 1.05204758]\n",
      "n_iter 61 loss 0.04114428677360658 weight [0.20007218 0.71330089 0.14813068 0.61919425 1.05421403]\n",
      "n_iter 62 loss 0.04107198425867845 weight [0.19932329 0.71285742 0.14759911 0.62065073 1.05637607]\n",
      "n_iter 63 loss 0.040999994779155585 weight [0.19857735 0.71241312 0.14706725 0.62210391 1.05853371]\n",
      "n_iter 64 loss 0.040928316235165636 weight [0.19783434 0.71196798 0.14653512 0.62355379 1.06068697]\n",
      "n_iter 65 loss 0.040856946547482335 weight [0.19709425 0.71152202 0.14600272 0.6250004  1.06283588]\n",
      "n_iter 66 loss 0.04078588365724708 weight [0.19635707 0.71107525 0.14547006 0.62644374 1.06498045]\n",
      "n_iter 67 loss 0.040715125525695194 weight [0.19562278 0.71062768 0.14493715 0.62788384 1.06712071]\n",
      "n_iter 68 loss 0.04064467013388701 weight [0.19489137 0.71017932 0.14440399 0.62932071 1.06925667]\n",
      "n_iter 69 loss 0.04057451548244331 weight [0.19416282 0.70973017 0.14387059 0.63075437 1.07138834]\n",
      "n_iter 70 loss 0.040504659591285415 weight [0.19343713 0.70928025 0.14333697 0.63218483 1.07351576]\n",
      "n_iter 71 loss 0.04043510049937941 weight [0.19271427 0.70882957 0.14280312 0.63361212 1.07563894]\n",
      "n_iter 72 loss 0.04036583626448484 weight [0.19199423 0.70837814 0.14226906 0.63503624 1.0777579 ]\n",
      "n_iter 73 loss 0.04029686496290757 weight [0.191277   0.70792597 0.14173479 0.63645721 1.07987265]\n",
      "n_iter 74 loss 0.040228184689256694 weight [0.19056257 0.70747306 0.14120033 0.63787505 1.08198322]\n",
      "n_iter 75 loss 0.04015979355620555 weight [0.18985092 0.70701943 0.14066566 0.63928978 1.08408962]\n",
      "n_iter 76 loss 0.04009168969425664 weight [0.18914205 0.70656508 0.14013081 0.6407014  1.08619188]\n",
      "n_iter 77 loss 0.04002387125151043 weight [0.18843593 0.70611003 0.13959579 0.64210994 1.08829   ]\n",
      "n_iter 78 loss 0.039956336393438006 weight [0.18773255 0.70565428 0.13906059 0.6435154  1.09038401]\n",
      "n_iter 79 loss 0.03988908330265742 weight [0.1870319  0.70519785 0.13852522 0.64491782 1.09247393]\n",
      "n_iter 80 loss 0.03982211017871363 weight [0.18633397 0.70474073 0.13798969 0.64631719 1.09455977]\n",
      "n_iter 81 loss 0.03975541523786216 weight [0.18563874 0.70428295 0.13745402 0.64771353 1.09664154]\n",
      "n_iter 82 loss 0.03968899671285617 weight [0.18494621 0.70382451 0.13691819 0.64910687 1.09871928]\n",
      "n_iter 83 loss 0.03962285285273697 weight [0.18425635 0.70336541 0.13638223 0.65049721 1.10079299]\n",
      "n_iter 84 loss 0.03955698192262804 weight [0.18356916 0.70290567 0.13584613 0.65188457 1.1028627 ]\n",
      "n_iter 85 loss 0.039491382203532197 weight [0.18288463 0.7024453  0.13530991 0.65326896 1.10492841]\n",
      "n_iter 86 loss 0.03942605199213209 weight [0.18220274 0.7019843  0.13477357 0.65465039 1.10699015]\n",
      "n_iter 87 loss 0.03936098960059393 weight [0.18152347 0.70152268 0.13423711 0.65602889 1.10904794]\n",
      "n_iter 88 loss 0.039296193356374305 weight [0.18084682 0.70106045 0.13370055 0.65740447 1.11110178]\n",
      "n_iter 89 loss 0.03923166160203006 weight [0.18017278 0.70059762 0.13316388 0.65877713 1.1131517 ]\n",
      "n_iter 90 loss 0.03916739269503118 weight [0.17950133 0.70013419 0.13262712 0.6601469  1.11519771]\n",
      "n_iter 91 loss 0.0391033850075767 weight [0.17883246 0.69967019 0.13209026 0.66151379 1.11723983]\n",
      "n_iter 92 loss 0.03903963692641338 weight [0.17816616 0.6992056  0.13155333 0.6628778  1.11927808]\n",
      "n_iter 93 loss 0.03897614685265742 weight [0.17750241 0.69874044 0.13101631 0.66423897 1.12131247]\n",
      "n_iter 94 loss 0.03891291320161877 weight [0.17684121 0.69827472 0.13047922 0.66559728 1.12334302]\n",
      "n_iter 95 loss 0.038849934402628294 weight [0.17618255 0.69780845 0.12994207 0.66695278 1.12536974]\n",
      "n_iter 96 loss 0.0387872088988676 weight [0.1755264  0.69734163 0.12940485 0.66830545 1.12739265]\n",
      "n_iter 97 loss 0.03872473514720151 weight [0.17487276 0.69687427 0.12886758 0.66965532 1.12941176]\n",
      "n_iter 98 loss 0.03866251161801311 weight [0.17422162 0.69640638 0.12833026 0.67100241 1.1314271 ]\n",
      "n_iter 99 loss 0.03860053679504134 weight [0.17357297 0.69593796 0.12779289 0.67234672 1.13343867]\n",
      "n_iter 100 loss 0.038538809175221116 weight [0.17292679 0.69546903 0.12725548 0.67368826 1.1354465 ]\n",
      "n_iter 101 loss 0.03847732726852587 weight [0.17228308 0.69499958 0.12671804 0.67502706 1.13745059]\n",
      "n_iter 102 loss 0.038416089597812446 weight [0.17164182 0.69452964 0.12618057 0.67636312 1.13945096]\n",
      "n_iter 103 loss 0.03835509469866846 weight [0.171003   0.6940592  0.12564307 0.67769645 1.14144763]\n",
      "n_iter 104 loss 0.03829434111926196 weight [0.1703666  0.69358827 0.12510556 0.67902707 1.14344062]\n",
      "n_iter 105 loss 0.03823382742019325 weight [0.16973263 0.69311686 0.12456803 0.68035499 1.14542993]\n",
      "n_iter 106 loss 0.038173552174349104 weight [0.16910107 0.69264498 0.1240305  0.68168023 1.14741558]\n",
      "n_iter 107 loss 0.03811351396675902 weight [0.1684719  0.69217263 0.12349296 0.68300279 1.14939759]\n",
      "n_iter 108 loss 0.03805371139445381 weight [0.16784512 0.69169982 0.12295542 0.68432268 1.15137597]\n",
      "n_iter 109 loss 0.03799414306632609 weight [0.16722071 0.69122655 0.12241789 0.68563993 1.15335073]\n",
      "n_iter 110 loss 0.037934807602993066 weight [0.16659867 0.69075284 0.12188037 0.68695454 1.1553219 ]\n",
      "n_iter 111 loss 0.037875703636661186 weight [0.16597898 0.69027869 0.12134287 0.68826652 1.15728948]\n",
      "n_iter 112 loss 0.03781682981099286 weight [0.16536164 0.6898041  0.12080538 0.68957588 1.15925349]\n",
      "n_iter 113 loss 0.03775818478097525 weight [0.16474662 0.68932909 0.12026792 0.69088264 1.16121394]\n",
      "n_iter 114 loss 0.03769976721279078 weight [0.16413393 0.68885365 0.11973049 0.69218681 1.16317085]\n",
      "n_iter 115 loss 0.037641575783689714 weight [0.16352355 0.6883778  0.1191931  0.6934884  1.16512423]\n",
      "n_iter 116 loss 0.03758360918186457 weight [0.16291547 0.68790155 0.11865574 0.69478743 1.1670741 ]\n",
      "n_iter 117 loss 0.03752586610632632 weight [0.16230968 0.68742489 0.11811843 0.69608389 1.16902046]\n",
      "n_iter 118 loss 0.037468345266782416 weight [0.16170618 0.68694783 0.11758116 0.69737781 1.17096334]\n",
      "n_iter 119 loss 0.03741104538351659 weight [0.16110494 0.68647038 0.11704395 0.6986692  1.17290274]\n",
      "n_iter 120 loss 0.03735396518727041 weight [0.16050596 0.68599255 0.11650679 0.69995806 1.17483868]\n",
      "n_iter 121 loss 0.03729710341912649 weight [0.15990923 0.68551435 0.11596969 0.70124441 1.17677118]\n",
      "n_iter 122 loss 0.03724045883039337 weight [0.15931475 0.68503577 0.11543266 0.70252826 1.17870024]\n",
      "n_iter 123 loss 0.037184030182492236 weight [0.15872249 0.68455682 0.11489569 0.70380962 1.18062588]\n",
      "n_iter 124 loss 0.03712781624684495 weight [0.15813245 0.68407752 0.1143588  0.7050885  1.18254811]\n",
      "n_iter 125 loss 0.03707181580476389 weight [0.15754462 0.68359786 0.11382199 0.70636491 1.18446695]\n",
      "n_iter 126 loss 0.037016027647343325 weight [0.15695899 0.68311785 0.11328525 0.70763887 1.18638241]\n",
      "n_iter 127 loss 0.03696045057535226 weight [0.15637556 0.6826375  0.1127486  0.70891038 1.1882945 ]\n",
      "n_iter 128 loss 0.036905083399128846 weight [0.1557943  0.68215682 0.11221204 0.71017945 1.19020323]\n",
      "n_iter 129 loss 0.03684992493847628 weight [0.15521522 0.6816758  0.11167558 0.7114461  1.19210862]\n",
      "n_iter 130 loss 0.036794974022560156 weight [0.15463829 0.68119446 0.11113921 0.71271033 1.19401068]\n",
      "n_iter 131 loss 0.03674022948980717 weight [0.15406352 0.68071279 0.11060294 0.71397216 1.19590943]\n",
      "n_iter 132 loss 0.03668569018780544 weight [0.15349089 0.68023081 0.11006677 0.7152316  1.19780487]\n",
      "n_iter 133 loss 0.036631354973205965 weight [0.1529204  0.67974852 0.10953071 0.71648865 1.19969701]\n",
      "n_iter 134 loss 0.03657722271162567 weight [0.15235203 0.67926593 0.10899477 0.71774332 1.20158588]\n",
      "n_iter 135 loss 0.03652329227755155 weight [0.15178577 0.67878304 0.10845894 0.71899564 1.20347148]\n",
      "n_iter 136 loss 0.0364695625542465 weight [0.15122162 0.67829985 0.10792322 0.7202456  1.20535382]\n",
      "n_iter 137 loss 0.03641603243365601 weight [0.15065956 0.67781638 0.10738763 0.72149321 1.20723293]\n",
      "n_iter 138 loss 0.03636270081631641 weight [0.15009959 0.67733262 0.10685217 0.72273849 1.2091088 ]\n",
      "n_iter 139 loss 0.03630956661126436 weight [0.1495417  0.67684859 0.10631684 0.72398145 1.21098145]\n",
      "n_iter 140 loss 0.036256628735947455 weight [0.14898588 0.67636428 0.10578164 0.7252221  1.21285089]\n",
      "n_iter 141 loss 0.036203886116136084 weight [0.14843212 0.6758797  0.10524657 0.72646044 1.21471714]\n",
      "n_iter 142 loss 0.03615133768583659 weight [0.1478804  0.67539486 0.10471165 0.72769649 1.21658021]\n",
      "n_iter 143 loss 0.03609898238720543 weight [0.14733073 0.67490976 0.10417686 0.72893025 1.2184401 ]\n",
      "n_iter 144 loss 0.03604681917046459 weight [0.14678309 0.67442441 0.10364223 0.73016173 1.22029683]\n",
      "n_iter 145 loss 0.035994846993818164 weight [0.14623748 0.67393881 0.10310774 0.73139095 1.22215042]\n",
      "n_iter 146 loss 0.035943064823369945 weight [0.14569388 0.67345296 0.10257341 0.73261792 1.22400087]\n",
      "n_iter 147 loss 0.03589147163304226 weight [0.14515229 0.67296688 0.10203923 0.73384264 1.22584819]\n",
      "n_iter 148 loss 0.035840066404495684 weight [0.14461269 0.67248056 0.10150521 0.73506512 1.2276924 ]\n",
      "n_iter 149 loss 0.03578884812705 weight [0.14407509 0.67199401 0.10097135 0.73628537 1.22953351]\n",
      "n_iter 150 loss 0.035737815797606154 weight [0.14353946 0.67150723 0.10043766 0.7375034  1.23137152]\n",
      "n_iter 151 loss 0.035686968420569204 weight [0.14300581 0.67102024 0.09990414 0.73871922 1.23320646]\n",
      "n_iter 152 loss 0.035636305007772276 weight [0.14247412 0.67053303 0.09937079 0.73993284 1.23503833]\n",
      "n_iter 153 loss 0.03558582457840156 weight [0.14194439 0.67004561 0.09883761 0.74114427 1.23686714]\n",
      "n_iter 154 loss 0.03553552615892226 weight [0.1414166  0.66955798 0.09830461 0.74235352 1.2386929 ]\n",
      "n_iter 155 loss 0.03548540878300556 weight [0.14089075 0.66907014 0.09777179 0.74356059 1.24051562]\n",
      "n_iter 156 loss 0.035435471491456434 weight [0.14036684 0.66858211 0.09723916 0.7447655  1.24233532]\n",
      "n_iter 157 loss 0.03538571333214248 weight [0.13984484 0.66809389 0.09670671 0.74596825 1.24415201]\n",
      "n_iter 158 loss 0.03533613335992364 weight [0.13932476 0.66760547 0.09617445 0.74716885 1.24596569]\n",
      "n_iter 159 loss 0.035286730636582796 weight [0.13880658 0.66711687 0.09564238 0.74836731 1.24777638]\n",
      "n_iter 160 loss 0.035237504230757395 weight [0.1382903  0.66662809 0.0951105  0.74956365 1.24958409]\n",
      "n_iter 161 loss 0.03518845321787173 weight [0.13777591 0.66613913 0.09457882 0.75075786 1.25138883]\n",
      "n_iter 162 loss 0.03513957668007025 weight [0.1372634  0.66565    0.09404734 0.75194996 1.2531906 ]\n",
      "n_iter 163 loss 0.03509087370615166 weight [0.13675276 0.6651607  0.09351607 0.75313995 1.25498943]\n",
      "n_iter 164 loss 0.03504234339150394 weight [0.13624399 0.66467123 0.092985   0.75432785 1.25678531]\n",
      "n_iter 165 loss 0.03499398483803992 weight [0.13573708 0.66418161 0.09245413 0.75551365 1.25857827]\n",
      "n_iter 166 loss 0.03494579715413412 weight [0.13523201 0.66369182 0.09192348 0.75669738 1.2603683 ]\n",
      "n_iter 167 loss 0.03489777945455991 weight [0.13472879 0.66320188 0.09139304 0.75787904 1.26215543]\n",
      "n_iter 168 loss 0.034849930860427786 weight [0.1342274  0.6627118  0.09086281 0.75905864 1.26393965]\n",
      "n_iter 169 loss 0.034802250499124304 weight [0.13372784 0.66222157 0.09033281 0.76023617 1.26572099]\n",
      "n_iter 170 loss 0.0347547375042517 weight [0.1332301  0.66173119 0.08980302 0.76141167 1.26749945]\n",
      "n_iter 171 loss 0.03470739101556844 weight [0.13273416 0.66124068 0.08927345 0.76258512 1.26927504]\n",
      "n_iter 172 loss 0.034660210178930353 weight [0.13224003 0.66075004 0.08874412 0.76375654 1.27104777]\n",
      "n_iter 173 loss 0.034613194146232554 weight [0.1317477  0.66025927 0.088215   0.76492594 1.27281765]\n",
      "n_iter 174 loss 0.034566342075352086 weight [0.13125715 0.65976837 0.08768612 0.76609333 1.27458469]\n",
      "n_iter 175 loss 0.034519653130091206 weight [0.13076839 0.65927734 0.08715747 0.7672587  1.2763489 ]\n",
      "n_iter 176 loss 0.0344731264801215 weight [0.1302814  0.6587862  0.08662906 0.76842208 1.27811029]\n",
      "n_iter 177 loss 0.03442676130092853 weight [0.12979617 0.65829494 0.08610088 0.76958347 1.27986887]\n",
      "n_iter 178 loss 0.034380556773757294 weight [0.1293127  0.65780357 0.08557294 0.77074287 1.28162465]\n",
      "n_iter 179 loss 0.03433451208555818 weight [0.12883098 0.6573121  0.08504524 0.7719003  1.28337764]\n",
      "n_iter 180 loss 0.03428862642893377 weight [0.12835101 0.65682051 0.08451779 0.77305575 1.28512784]\n",
      "n_iter 181 loss 0.03424289900208612 weight [0.12787277 0.65632883 0.08399058 0.77420925 1.28687528]\n",
      "n_iter 182 loss 0.03419732900876483 weight [0.12739626 0.65583705 0.08346362 0.77536079 1.28861995]\n",
      "n_iter 183 loss 0.03415191565821554 weight [0.12692147 0.65534517 0.08293691 0.77651039 1.29036187]\n",
      "n_iter 184 loss 0.0341066581651293 weight [0.1264484  0.65485321 0.08241045 0.77765805 1.29210104]\n",
      "n_iter 185 loss 0.03406155574959226 weight [0.12597704 0.65436115 0.08188424 0.77880378 1.29383748]\n",
      "n_iter 186 loss 0.03401660763703622 weight [0.12550737 0.65386901 0.08135829 0.77994758 1.2955712 ]\n",
      "n_iter 187 loss 0.03397181305818959 weight [0.1250394  0.65337679 0.0808326  0.78108947 1.29730219]\n",
      "n_iter 188 loss 0.033927171249029 weight [0.12457312 0.65288449 0.08030717 0.78222945 1.29903048]\n",
      "n_iter 189 loss 0.03388268145073148 weight [0.12410851 0.65239212 0.079782   0.78336753 1.30075607]\n",
      "n_iter 190 loss 0.03383834290962724 weight [0.12364558 0.65189968 0.0792571  0.78450371 1.30247898]\n",
      "n_iter 191 loss 0.033794154877152906 weight [0.12318432 0.65140717 0.07873246 0.78563801 1.3041992 ]\n",
      "n_iter 192 loss 0.033750116609805376 weight [0.12272471 0.65091459 0.07820809 0.78677042 1.30591675]\n",
      "n_iter 193 loss 0.03370622736909619 weight [0.12226675 0.65042195 0.07768399 0.78790097 1.30763163]\n",
      "n_iter 194 loss 0.033662486421506516 weight [0.12181044 0.64992925 0.07716016 0.78902964 1.30934387]\n",
      "n_iter 195 loss 0.03361889303844249 weight [0.12135577 0.6494365  0.0766366  0.79015646 1.31105346]\n",
      "n_iter 196 loss 0.03357544649619123 weight [0.12090273 0.64894369 0.07611332 0.79128142 1.31276041]\n",
      "n_iter 197 loss 0.0335321460758773 weight [0.12045132 0.64845084 0.07559032 0.79240454 1.31446473]\n",
      "n_iter 198 loss 0.03348899106341964 weight [0.12000152 0.64795794 0.0750676  0.79352582 1.31616644]\n",
      "n_iter 199 loss 0.03344598074948903 weight [0.11955333 0.64746499 0.07454515 0.79464527 1.31786553]\n",
      "n_iter 200 loss 0.0334031144294661 weight [0.11910675 0.64697201 0.07402299 0.7957629  1.31956203]\n",
      "n_iter 201 loss 0.033360391403399685 weight [0.11866177 0.64647898 0.07350112 0.7968787  1.32125593]\n",
      "n_iter 202 loss 0.03331781097596578 weight [0.11821838 0.64598592 0.07297953 0.7979927  1.32294724]\n",
      "n_iter 203 loss 0.033275372456426804 weight [0.11777658 0.64549283 0.07245822 0.79910489 1.32463598]\n",
      "n_iter 204 loss 0.03323307515859165 weight [0.11733635 0.64499971 0.07193721 0.80021528 1.32632215]\n",
      "n_iter 205 loss 0.03319091840077574 weight [0.1168977  0.64450656 0.07141649 0.80132388 1.32800576]\n",
      "n_iter 206 loss 0.0331489015057619 weight [0.11646061 0.64401339 0.07089606 0.80243069 1.32968682]\n",
      "n_iter 207 loss 0.03310702380076144 weight [0.11602509 0.6435202  0.07037592 0.80353573 1.33136533]\n",
      "n_iter 208 loss 0.03306528461737588 weight [0.11559111 0.64302699 0.06985608 0.804639   1.33304131]\n",
      "n_iter 209 loss 0.03302368329155888 weight [0.11515869 0.64253377 0.06933654 0.8057405  1.33471477]\n",
      "n_iter 210 loss 0.032982219163578776 weight [0.1147278  0.64204053 0.06881729 0.80684024 1.3363857 ]\n",
      "n_iter 211 loss 0.03294089157798143 weight [0.11429845 0.64154728 0.06829835 0.80793822 1.33805413]\n",
      "n_iter 212 loss 0.03289969988355353 weight [0.11387063 0.64105403 0.06777971 0.80903447 1.33972005]\n",
      "n_iter 213 loss 0.03285864343328631 weight [0.11344433 0.64056077 0.06726137 0.81012897 1.34138347]\n",
      "n_iter 214 loss 0.0328177215843396 weight [0.11301955 0.64006751 0.06674333 0.81122174 1.34304441]\n",
      "n_iter 215 loss 0.03277693369800639 weight [0.11259627 0.63957425 0.06622561 0.81231278 1.34470287]\n",
      "n_iter 216 loss 0.03273627913967767 weight [0.11217451 0.63908099 0.06570819 0.8134021  1.34635886]\n",
      "n_iter 217 loss 0.03269575727880774 weight [0.11175423 0.63858774 0.06519108 0.8144897  1.34801239]\n",
      "n_iter 218 loss 0.03265536748887979 weight [0.11133545 0.63809449 0.06467428 0.8155756  1.34966346]\n",
      "n_iter 219 loss 0.03261510914737201 weight [0.11091816 0.63760126 0.06415779 0.81665979 1.35131208]\n",
      "n_iter 220 loss 0.032574981635723935 weight [0.11050235 0.63710804 0.06364161 0.81774229 1.35295826]\n",
      "n_iter 221 loss 0.03253498433930321 weight [0.11008801 0.63661483 0.06312575 0.81882309 1.35460201]\n",
      "n_iter 222 loss 0.03249511664737274 weight [0.10967513 0.63612164 0.06261021 0.81990221 1.35624334]\n",
      "n_iter 223 loss 0.032455377953058084 weight [0.10926372 0.63562847 0.06209498 0.82097965 1.35788225]\n",
      "n_iter 224 loss 0.03241576765331538 weight [0.10885377 0.63513533 0.06158007 0.82205542 1.35951875]\n",
      "n_iter 225 loss 0.032376285148899425 weight [0.10844527 0.63464221 0.06106548 0.82312953 1.36115285]\n",
      "n_iter 226 loss 0.03233692984433224 weight [0.10803821 0.63414911 0.06055122 0.82420197 1.36278455]\n",
      "n_iter 227 loss 0.0322977011478719 weight [0.10763259 0.63365605 0.06003727 0.82527275 1.36441387]\n",
      "n_iter 228 loss 0.03225859847148168 weight [0.1072284  0.63316302 0.05952365 0.82634189 1.3660408 ]\n",
      "n_iter 229 loss 0.03221962123079957 weight [0.10682565 0.63267002 0.05901035 0.82740938 1.36766537]\n",
      "n_iter 230 loss 0.03218076884510816 weight [0.10642431 0.63217706 0.05849738 0.82847524 1.36928757]\n",
      "n_iter 231 loss 0.032142040737304714 weight [0.10602439 0.63168414 0.05798474 0.82953946 1.37090741]\n",
      "n_iter 232 loss 0.03210343633387162 weight [0.10562588 0.63119125 0.05747243 0.83060206 1.3725249 ]\n",
      "n_iter 233 loss 0.032064955064847224 weight [0.10522877 0.63069842 0.05696044 0.83166303 1.37414005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter 234 loss 0.03202659636379686 weight [0.10483307 0.63020562 0.05644879 0.83272239 1.37575286]\n",
      "n_iter 235 loss 0.03198835966778424 weight [0.10443876 0.62971288 0.05593747 0.83378015 1.37736334]\n",
      "n_iter 236 loss 0.03195024441734312 weight [0.10404583 0.62922018 0.05542648 0.83483629 1.3789715 ]\n",
      "n_iter 237 loss 0.03191225005644932 weight [0.10365429 0.62872753 0.05491582 0.83589084 1.38057735]\n",
      "n_iter 238 loss 0.031874376032492886 weight [0.10326413 0.62823494 0.0544055  0.8369438  1.38218089]\n",
      "n_iter 239 loss 0.03183662179625075 weight [0.10287533 0.62774241 0.05389552 0.83799517 1.38378213]\n",
      "n_iter 240 loss 0.03179898680185945 weight [0.10248791 0.62724993 0.05338588 0.83904495 1.38538108]\n",
      "n_iter 241 loss 0.0317614705067884 weight [0.10210184 0.62675751 0.05287657 0.84009316 1.38697774]\n",
      "n_iter 242 loss 0.03172407237181307 weight [0.10171714 0.62626516 0.0523676  0.8411398  1.38857211]\n",
      "n_iter 243 loss 0.031686791860988835 weight [0.10133378 0.62577287 0.05185897 0.84218488 1.39016422]\n",
      "n_iter 244 loss 0.0316496284416248 weight [0.10095176 0.62528064 0.05135069 0.84322839 1.39175406]\n",
      "n_iter 245 loss 0.031612581584258004 weight [0.10057109 0.62478848 0.05084274 0.84427035 1.39334165]\n",
      "n_iter 246 loss 0.03157565076262793 weight [0.10019175 0.6242964  0.05033514 0.84531075 1.39492697]\n",
      "n_iter 247 loss 0.03153883545365114 weight [0.09981374 0.62380438 0.04982789 0.84634962 1.39651006]\n",
      "n_iter 248 loss 0.03150213513739636 weight [0.09943706 0.62331244 0.04932098 0.84738694 1.3980909 ]\n",
      "n_iter 249 loss 0.03146554929705956 weight [0.09906169 0.62282057 0.04881441 0.84842273 1.39966952]\n",
      "n_iter 250 loss 0.03142907741893959 weight [0.09868764 0.62232878 0.0483082  0.84945699 1.40124591]\n",
      "n_iter 251 loss 0.03139271899241374 weight [0.09831489 0.62183707 0.04780233 0.85048973 1.40282008]\n",
      "n_iter 252 loss 0.03135647350991385 weight [0.09794345 0.62134545 0.04729681 0.85152094 1.40439203]\n",
      "n_iter 253 loss 0.031320340466902415 weight [0.09757331 0.6208539  0.04679164 0.85255065 1.40596178]\n",
      "n_iter 254 loss 0.031284319361849014 weight [0.09720446 0.62036244 0.04628682 0.85357884 1.40752934]\n",
      "n_iter 255 loss 0.031248409696207135 weight [0.0968369  0.61987107 0.04578236 0.85460553 1.4090947 ]\n",
      "n_iter 256 loss 0.03121261097439089 weight [0.09647062 0.61937978 0.04527824 0.85563073 1.41065787]\n",
      "n_iter 257 loss 0.031176922703752254 weight [0.09610562 0.61888859 0.04477448 0.85665443 1.41221887]\n",
      "n_iter 258 loss 0.031141344394558443 weight [0.09574189 0.61839749 0.04427108 0.85767664 1.41377769]\n",
      "n_iter 259 loss 0.031105875559969434 weight [0.09537943 0.61790648 0.04376803 0.85869737 1.41533434]\n",
      "n_iter 260 loss 0.031070515716015792 weight [0.09501824 0.61741557 0.04326533 0.85971661 1.41688883]\n",
      "n_iter 261 loss 0.031035264381576742 weight [0.0946583  0.61692475 0.04276299 0.86073439 1.41844117]\n",
      "n_iter 262 loss 0.031000121078358315 weight [0.09429962 0.61643403 0.04226101 0.86175069 1.41999137]\n",
      "n_iter 263 loss 0.0309650853308719 weight [0.09394219 0.61594342 0.04175939 0.86276554 1.42153942]\n",
      "n_iter 264 loss 0.03093015666641279 weight [0.093586   0.61545291 0.04125813 0.86377892 1.42308533]\n",
      "n_iter 265 loss 0.030895334615039155 weight [0.09323105 0.6149625  0.04075722 0.86479085 1.42462912]\n",
      "n_iter 266 loss 0.030860618709551046 weight [0.09287733 0.61447219 0.04025668 0.86580132 1.42617078]\n",
      "n_iter 267 loss 0.030826008485469694 weight [0.09252484 0.613982   0.0397565  0.86681036 1.42771033]\n",
      "n_iter 268 loss 0.03079150348101698 weight [0.09217358 0.61349191 0.03925668 0.86781795 1.42924776]\n",
      "n_iter 269 loss 0.030757103237095088 weight [0.09182354 0.61300193 0.03875722 0.86882411 1.43078309]\n",
      "n_iter 270 loss 0.030722807297266393 weight [0.09147471 0.61251207 0.03825813 0.86982883 1.43231633]\n",
      "n_iter 271 loss 0.0306886152077335 weight [0.0911271  0.61202232 0.0377594  0.87083213 1.43384747]\n",
      "n_iter 272 loss 0.030654526517319532 weight [0.09078069 0.61153268 0.03726103 0.87183401 1.43537652]\n",
      "n_iter 273 loss 0.030620540777448493 weight [0.09043548 0.61104316 0.03676303 0.87283447 1.43690349]\n",
      "n_iter 274 loss 0.030586657542125983 weight [0.09009147 0.61055376 0.0362654  0.87383352 1.43842839]\n",
      "n_iter 275 loss 0.030552876367919913 weight [0.08974865 0.61006448 0.03576813 0.87483116 1.43995122]\n",
      "n_iter 276 loss 0.030519196813941576 weight [0.08940702 0.60957532 0.03527123 0.8758274  1.44147199]\n",
      "n_iter 277 loss 0.030485618441826744 weight [0.08906657 0.60908628 0.0347747  0.87682223 1.4429907 ]\n",
      "n_iter 278 loss 0.03045214081571705 weight [0.0887273  0.60859737 0.03427854 0.87781568 1.44450736]\n",
      "n_iter 279 loss 0.030418763502241528 weight [0.0883892  0.60810858 0.03378274 0.87880773 1.44602198]\n",
      "n_iter 280 loss 0.030385486070498225 weight [0.08805228 0.60761992 0.03328732 0.8797984  1.44753456]\n",
      "n_iter 281 loss 0.030352308092036172 weight [0.08771651 0.60713139 0.03279227 0.88078769 1.4490451 ]\n",
      "n_iter 282 loss 0.030319229140837352 weight [0.08738191 0.60664299 0.03229758 0.8817756  1.45055361]\n",
      "n_iter 283 loss 0.0302862487932989 weight [0.08704847 0.60615472 0.03180327 0.88276213 1.45206011]\n",
      "n_iter 284 loss 0.03025336662821551 weight [0.08671618 0.60566658 0.03130934 0.8837473  1.45356458]\n",
      "n_iter 285 loss 0.03022058222676192 weight [0.08638503 0.60517858 0.03081577 0.88473111 1.45506705]\n",
      "n_iter 286 loss 0.03018789517247562 weight [0.08605503 0.60469072 0.03032258 0.88571356 1.45656751]\n",
      "n_iter 287 loss 0.030155305051239728 weight [0.08572617 0.60420299 0.02982976 0.88669465 1.45806598]\n",
      "n_iter 288 loss 0.03012281145126597 weight [0.08539844 0.60371539 0.02933731 0.88767439 1.45956244]\n",
      "n_iter 289 loss 0.03009041396307783 weight [0.08507184 0.60322794 0.02884525 0.88865278 1.46105692]\n",
      "n_iter 290 loss 0.030058112179493907 weight [0.08474637 0.60274063 0.02835355 0.88962984 1.46254942]\n",
      "n_iter 291 loss 0.030025905695611387 weight [0.08442201 0.60225346 0.02786223 0.89060555 1.46403994]\n",
      "n_iter 292 loss 0.02999379410878962 weight [0.08409878 0.60176644 0.02737129 0.89157993 1.46552849]\n",
      "n_iter 293 loss 0.029961777018633964 weight [0.08377666 0.60127956 0.02688073 0.89255298 1.46701508]\n",
      "n_iter 294 loss 0.029929854026979622 weight [0.08345565 0.60079282 0.02639054 0.89352471 1.4684997 ]\n",
      "n_iter 295 loss 0.029898024737875773 weight [0.08313574 0.60030624 0.02590073 0.89449511 1.46998237]\n",
      "n_iter 296 loss 0.029866288757569734 weight [0.08281693 0.5998198  0.0254113  0.8954642  1.47146309]\n",
      "n_iter 297 loss 0.029834645694491353 weight [0.08249922 0.59933351 0.02492224 0.89643197 1.47294186]\n",
      "n_iter 298 loss 0.029803095159237483 weight [0.0821826  0.59884737 0.02443357 0.89739844 1.47441869]\n",
      "n_iter 299 loss 0.02977163676455658 weight [0.08186707 0.59836139 0.02394527 0.8983636  1.4758936 ]\n",
      "n_iter 300 loss 0.02974027012533357 weight [0.08155262 0.59787556 0.02345736 0.89932745 1.47736657]\n",
      "n_iter 301 loss 0.029708994858574627 weight [0.08123926 0.59738988 0.02296983 0.90029002 1.47883762]\n",
      "n_iter 302 loss 0.029677810583392302 weight [0.08092697 0.59690436 0.02248267 0.90125128 1.48030675]\n",
      "n_iter 303 loss 0.029646716920990693 weight [0.08061575 0.596419   0.0219959  0.90221126 1.48177397]\n",
      "n_iter 304 loss 0.0296157134946507 weight [0.08030559 0.59593379 0.02150951 0.90316996 1.48323928]\n",
      "n_iter 305 loss 0.029584799929715522 weight [0.07999651 0.59544875 0.0210235  0.90412737 1.48470269]\n",
      "n_iter 306 loss 0.02955397585357614 weight [0.07968848 0.59496386 0.02053788 0.90508351 1.48616421]\n",
      "n_iter 307 loss 0.029523240895657134 weight [0.07938151 0.59447914 0.02005263 0.90603837 1.48762383]\n",
      "n_iter 308 loss 0.029492594687402375 weight [0.07907559 0.59399458 0.01956777 0.90699196 1.48908156]\n",
      "n_iter 309 loss 0.029462036862261048 weight [0.07877071 0.59351019 0.0190833  0.90794429 1.49053742]\n",
      "n_iter 310 loss 0.029431567055673676 weight [0.07846688 0.59302596 0.01859921 0.90889536 1.49199139]\n",
      "n_iter 311 loss 0.02940118490505835 weight [0.0781641  0.59254189 0.0181155  0.90984516 1.4934435 ]\n",
      "n_iter 312 loss 0.029370890049797 weight [0.07786234 0.59205799 0.01763217 0.91079372 1.49489374]\n",
      "n_iter 313 loss 0.029340682131221842 weight [0.07756162 0.59157427 0.01714923 0.91174102 1.49634212]\n",
      "n_iter 314 loss 0.029310560792601907 weight [0.07726193 0.59109071 0.01666668 0.91268708 1.49778864]\n",
      "n_iter 315 loss 0.02928052567912975 weight [0.07696326 0.59060732 0.01618451 0.91363189 1.49923331]\n",
      "n_iter 316 loss 0.029250576437908193 weight [0.07666562 0.5901241  0.01570273 0.91457547 1.50067614]\n",
      "n_iter 317 loss 0.029220712717937202 weight [0.07636899 0.58964106 0.01522133 0.91551781 1.50211712]\n",
      "n_iter 318 loss 0.029190934170100942 weight [0.07607337 0.58915819 0.01474032 0.91645892 1.50355627]\n",
      "n_iter 319 loss 0.02916124044715487 weight [0.07577877 0.58867549 0.0142597  0.9173988  1.50499358]\n",
      "n_iter 320 loss 0.02913163120371293 weight [0.07548517 0.58819297 0.01377947 0.91833745 1.50642907]\n",
      "n_iter 321 loss 0.029102106096235 weight [0.07519257 0.58771063 0.01329962 0.91927489 1.50786274]\n",
      "n_iter 322 loss 0.02907266478301418 weight [0.07490097 0.58722846 0.01282016 0.92021111 1.50929459]\n",
      "n_iter 323 loss 0.029043306924164524 weight [0.07461036 0.58674647 0.01234108 0.92114612 1.51072463]\n",
      "n_iter 324 loss 0.029014032181608536 weight [0.07432074 0.58626466 0.0118624  0.92207991 1.51215286]\n",
      "n_iter 325 loss 0.028984840219065066 weight [0.07403212 0.58578304 0.0113841  0.9230125  1.51357929]\n",
      "n_iter 326 loss 0.028955730702037077 weight [0.07374447 0.58530159 0.01090619 0.92394389 1.51500392]\n",
      "n_iter 327 loss 0.028926703297799724 weight [0.07345781 0.58482033 0.01042867 0.92487408 1.51642676]\n",
      "n_iter 328 loss 0.02889775767538831 weight [0.07317212 0.58433925 0.00995154 0.92580307 1.51784781]\n",
      "n_iter 329 loss 0.028868893505586576 weight [0.0728874  0.58385835 0.0094748  0.92673087 1.51926707]\n",
      "n_iter 330 loss 0.028840110460914885 weight [0.07260366 0.58337764 0.00899845 0.92765749 1.52068456]\n",
      "n_iter 331 loss 0.02881140821561863 weight [0.07232088 0.58289712 0.00852249 0.92858292 1.52210027]\n",
      "n_iter 332 loss 0.02878278644565673 weight [0.07203906 0.58241678 0.00804691 0.92950716 1.52351422]\n",
      "n_iter 333 loss 0.028754244828690143 weight [0.0717582  0.58193663 0.00757173 0.93043023 1.5249264 ]\n",
      "n_iter 334 loss 0.028725783044070603 weight [0.07147829 0.58145667 0.00709694 0.93135213 1.52633681]\n",
      "n_iter 335 loss 0.0286974007728293 weight [0.07119933 0.5809769  0.00662254 0.93227285 1.52774548]\n",
      "n_iter 336 loss 0.02866909769766576 weight [0.07092133 0.58049732 0.00614853 0.93319241 1.52915239]\n",
      "n_iter 337 loss 0.02864087350293681 weight [0.07064427 0.58001793 0.00567491 0.9341108  1.53055755]\n",
      "n_iter 338 loss 0.028612727874645597 weight [0.07036814 0.57953874 0.00520168 0.93502803 1.53196098]\n",
      "n_iter 339 loss 0.028584660500430712 weight [0.07009296 0.57905973 0.00472884 0.9359441  1.53336266]\n",
      "n_iter 340 loss 0.028556671069555395 weight [0.06981871 0.57858092 0.0042564  0.93685902 1.53476261]\n",
      "n_iter 341 loss 0.028528759272896848 weight [0.06954539 0.57810231 0.00378435 0.93777279 1.53616084]\n",
      "n_iter 342 loss 0.028500924802935673 weight [0.069273   0.57762389 0.00331268 0.93868541 1.53755734]\n",
      "n_iter 343 loss 0.028473167353745266 weight [0.06900153 0.57714567 0.00284141 0.93959689 1.53895212]\n",
      "n_iter 344 loss 0.028445486620981453 weight [0.06873098 0.57666765 0.00237053 0.94050723 1.54034518]\n",
      "n_iter 345 loss 0.028417882301872095 weight [0.06846135 0.57618982 0.00190005 0.94141642 1.54173654]\n",
      "n_iter 346 loss 0.028390354095206866 weight [6.81926359e-02 5.75712192e-01 1.42995342e-03 9.42324489e-01\n",
      " 1.54312619e+00]\n",
      "n_iter 347 loss 0.028362901701327044 weight [6.79248293e-02 5.75234764e-01 9.60251843e-04 9.43231422e-01\n",
      " 1.54451413e+00]\n",
      "n_iter 348 loss 0.02833552482211542 weight [6.76579297e-02 5.74757536e-01 4.90942815e-04 9.44137228e-01\n",
      " 1.54590038e+00]\n",
      "n_iter 349 loss 0.028308223160986284 weight [6.73919338e-02 5.74280510e-01 2.20264593e-05 9.45041909e-01\n",
      " 1.54728493e+00]\n",
      "n_iter 350 loss 0.028280996422875494 weight [ 6.71268384e-02  5.73803686e-01 -4.46497111e-04  9.45945469e-01\n",
      "  1.54866780e+00]\n",
      "n_iter 351 loss 0.02825384431423061 weight [ 6.68626402e-02  5.73327065e-01 -9.14627787e-04  9.46847911e-01\n",
      "  1.55004898e+00]\n",
      "n_iter 352 loss 0.02822676654300114 weight [ 6.65993359e-02  5.72850647e-01 -1.38236546e-03  9.47749238e-01\n",
      "  1.55142848e+00]\n",
      "n_iter 353 loss 0.028199762818628838 weight [ 0.06633692  0.57237443 -0.00184971  0.94864945  1.55280631]\n",
      "n_iter 354 loss 0.028172832852038036 weight [ 0.0660754   0.57189842 -0.00231666  0.94954856  1.55418246]\n",
      "n_iter 355 loss 0.0281459763556262 weight [ 0.06581475  0.57142262 -0.00278322  0.95044656  1.55555694]\n",
      "n_iter 356 loss 0.02811919304325439 weight [ 0.06555499  0.57094702 -0.00324938  0.95134346  1.55692977]\n",
      "n_iter 357 loss 0.028092482630237895 weight [ 0.06529611  0.57047163 -0.00371516  0.95223926  1.55830093]\n",
      "n_iter 358 loss 0.028065844833336916 weight [ 0.0650381   0.56999645 -0.00418053  0.95313396  1.55967044]\n",
      "n_iter 359 loss 0.028039279370747346 weight [ 0.06478096  0.56952148 -0.00464552  0.95402757  1.56103829]\n",
      "n_iter 360 loss 0.028012785962091585 weight [ 0.0645247   0.56904672 -0.00511011  0.95492009  1.5624045 ]\n",
      "n_iter 361 loss 0.027986364328409424 weight [ 0.06426929  0.56857216 -0.0055743   0.95581153  1.56376907]\n",
      "n_iter 362 loss 0.02796001419214908 weight [ 0.06401475  0.56809782 -0.00603811  0.95670187  1.565132  ]\n",
      "n_iter 363 loss 0.027933735277158197 weight [ 0.06376107  0.56762369 -0.00650152  0.95759114  1.56649329]\n",
      "n_iter 364 loss 0.027907527308675018 weight [ 0.06350824  0.56714977 -0.00696453  0.95847933  1.56785296]\n",
      "n_iter 365 loss 0.02788139001331949 weight [ 0.06325627  0.56667606 -0.00742715  0.95936645  1.56921099]\n",
      "n_iter 366 loss 0.027855323119084653 weight [ 0.06300514  0.56620256 -0.00788938  0.96025249  1.57056741]\n",
      "n_iter 367 loss 0.027829326355327824 weight [ 0.06275486  0.56572928 -0.00835121  0.96113747  1.57192221]\n",
      "n_iter 368 loss 0.02780339945276212 weight [ 0.06250543  0.56525622 -0.00881265  0.96202138  1.57327539]\n",
      "n_iter 369 loss 0.027777542143447822 weight [ 0.06225683  0.56478337 -0.0092737   0.96290423  1.57462696]\n",
      "n_iter 370 loss 0.027751754160783997 weight [ 0.06200908  0.56431073 -0.00973435  0.96378601  1.57597693]\n",
      "n_iter 371 loss 0.0277260352395 weight [ 0.06176216  0.56383831 -0.01019461  0.96466674  1.5773253 ]\n",
      "n_iter 372 loss 0.02770038511564724 weight [ 0.06151606  0.56336611 -0.01065447  0.96554642  1.57867206]\n",
      "n_iter 373 loss 0.027674803526590804 weight [ 0.0612708   0.56289413 -0.01111394  0.96642504  1.58001724]\n",
      "n_iter 374 loss 0.027649290211001324 weight [ 0.06102637  0.56242236 -0.01157301  0.96730262  1.58136082]\n",
      "n_iter 375 loss 0.02762384490884681 weight [ 0.06078275  0.56195081 -0.01203169  0.96817915  1.58270282]\n",
      "n_iter 376 loss 0.027598467361384535 weight [ 0.06053996  0.56147948 -0.01248998  0.96905464  1.58404323]\n",
      "n_iter 377 loss 0.027573157311153076 weight [ 0.06029798  0.56100838 -0.01294787  0.96992909  1.58538207]\n",
      "n_iter 378 loss 0.027547914501964327 weight [ 0.06005681  0.56053749 -0.01340537  0.9708025   1.58671933]\n",
      "n_iter 379 loss 0.027522738678895535 weight [ 0.05981646  0.56006682 -0.01386247  0.97167488  1.58805502]\n",
      "n_iter 380 loss 0.027497629588281624 weight [ 0.05957691  0.55959638 -0.01431918  0.97254622  1.58938914]\n",
      "n_iter 381 loss 0.027472586977707252 weight [ 0.05933817  0.55912616 -0.0147755   0.97341654  1.5907217 ]\n",
      "n_iter 382 loss 0.027447610595999178 weight [ 0.05910023  0.55865616 -0.01523142  0.97428584  1.59205271]\n",
      "n_iter 383 loss 0.02742270019321861 weight [ 0.0588631   0.55818638 -0.01568695  0.97515411  1.59338215]\n",
      "n_iter 384 loss 0.027397855520653564 weight [ 0.05862675  0.55771683 -0.01614208  0.97602136  1.59471005]\n",
      "n_iter 385 loss 0.027373076330811378 weight [ 0.0583912   0.5572475  -0.01659682  0.97688759  1.59603639]\n",
      "n_iter 386 loss 0.027348362377411152 weight [ 0.05815644  0.5567784  -0.01705116  0.97775281  1.59736119]\n",
      "n_iter 387 loss 0.02732371341537639 weight [ 0.05792247  0.55630953 -0.01750512  0.97861701  1.59868446]\n",
      "n_iter 388 loss 0.027299129200827603 weight [ 0.05768929  0.55584087 -0.01795867  0.97948021  1.60000618]\n",
      "n_iter 389 loss 0.027274609491074978 weight [ 0.05745689  0.55537245 -0.01841184  0.9803424   1.60132637]\n",
      "n_iter 390 loss 0.027250154044611143 weight [ 0.05722526  0.55490425 -0.01886461  0.98120359  1.60264504]\n",
      "n_iter 391 loss 0.027225762621103957 weight [ 0.05699441  0.55443628 -0.01931698  0.98206378  1.60396217]\n",
      "n_iter 392 loss 0.027201434981389324 weight [ 0.05676434  0.55396854 -0.01976896  0.98292297  1.60527779]\n",
      "n_iter 393 loss 0.02717717088746415 weight [ 0.05653504  0.55350103 -0.02022055  0.98378116  1.60659189]\n",
      "n_iter 394 loss 0.027152970102479253 weight [ 0.05630651  0.55303375 -0.02067175  0.98463836  1.60790447]\n",
      "n_iter 395 loss 0.027128832390732428 weight [ 0.05607874  0.55256669 -0.02112255  0.98549457  1.60921554]\n",
      "n_iter 396 loss 0.02710475751766141 weight [ 0.05585173  0.55209987 -0.02157296  0.9863498   1.61052511]\n",
      "n_iter 397 loss 0.027080745249837078 weight [ 0.05562549  0.55163328 -0.02202297  0.98720403  1.61183317]\n",
      "n_iter 398 loss 0.02705679535495659 weight [ 0.0554      0.55116691 -0.0224726   0.98805729  1.61313973]\n",
      "n_iter 399 loss 0.027032907601836586 weight [ 0.05517527  0.55070078 -0.02292182  0.98890957  1.61444479]\n",
      "n_iter 400 loss 0.027009081760406447 weight [ 0.05495129  0.55023488 -0.02337066  0.98976087  1.61574836]\n",
      "n_iter 401 loss 0.026985317601701635 weight [ 0.05472806  0.54976922 -0.0238191   0.99061119  1.61705044]\n",
      "n_iter 402 loss 0.026961614897857047 weight [ 0.05450558  0.54930378 -0.02426715  0.99146054  1.61835104]\n",
      "n_iter 403 loss 0.026937973422100393 weight [ 0.05428384  0.54883858 -0.02471481  0.99230893  1.61965015]\n",
      "n_iter 404 loss 0.02691439294874574 weight [ 0.05406285  0.54837362 -0.02516207  0.99315634  1.62094779]\n",
      "n_iter 405 loss 0.02689087325318693 weight [ 0.05384259  0.54790888 -0.02560894  0.9940028   1.62224395]\n",
      "n_iter 406 loss 0.026867414111891173 weight [ 0.05362307  0.54744438 -0.02605542  0.99484829  1.62353864]\n",
      "n_iter 407 loss 0.0268440153023927 weight [ 0.05340428  0.54698012 -0.02650151  0.99569282  1.62483186]\n",
      "n_iter 408 loss 0.026820676603286308 weight [ 0.05318623  0.54651609 -0.0269472   0.99653639  1.62612361]\n",
      "n_iter 409 loss 0.026797397794221182 weight [ 0.0529689   0.5460523  -0.0273925   0.99737901  1.6274139 ]\n",
      "n_iter 410 loss 0.02677417865589453 weight [ 0.0527523   0.54558875 -0.02783741  0.99822068  1.62870274]\n",
      "n_iter 411 loss 0.026751018970045438 weight [ 0.05253643  0.54512543 -0.02828193  0.9990614   1.62999012]\n",
      "n_iter 412 loss 0.026727918519448692 weight [ 0.05232127  0.54466235 -0.02872605  0.99990118  1.63127605]\n",
      "n_iter 413 loss 0.026704877087908627 weight [ 0.05210683  0.5441995  -0.02916979  1.00074001  1.63256053]\n",
      "n_iter 414 loss 0.026681894460253115 weight [ 0.05189311  0.5437369  -0.02961313  1.00157789  1.63384357]\n",
      "n_iter 415 loss 0.026658970422327444 weight [ 0.05168011  0.54327453 -0.03005608  1.00241484  1.63512517]\n",
      "n_iter 416 loss 0.026636104760988412 weight [ 0.05146781  0.5428124  -0.03049864  1.00325085  1.63640533]\n",
      "n_iter 417 loss 0.026613297264098328 weight [ 0.05125622  0.54235051 -0.03094081  1.00408593  1.63768405]\n",
      "n_iter 418 loss 0.026590547720519154 weight [ 0.05104534  0.54188886 -0.03138258  1.00492008  1.63896134]\n",
      "n_iter 419 loss 0.026567855920106603 weight [ 0.05083516  0.54142745 -0.03182397  1.00575329  1.64023721]\n",
      "n_iter 420 loss 0.026545221653704324 weight [ 0.05062569  0.54096628 -0.03226496  1.00658558  1.64151165]\n",
      "n_iter 421 loss 0.026522644713138168 weight [ 0.05041691  0.54050535 -0.03270557  1.00741694  1.64278467]\n",
      "n_iter 422 loss 0.026500124891210414 weight [ 0.05020883  0.54004466 -0.03314578  1.00824738  1.64405628]\n",
      "n_iter 423 loss 0.02647766198169408 weight [ 0.05000144  0.53958421 -0.0335856   1.0090769   1.64532647]\n",
      "n_iter 424 loss 0.02645525577932728 weight [ 0.04979474  0.539124   -0.03402503  1.00990551  1.64659524]\n",
      "n_iter 425 loss 0.02643290607980763 weight [ 0.04958873  0.53866404 -0.03446408  1.01073319  1.64786261]\n",
      "n_iter 426 loss 0.026410612679786616 weight [ 0.04938341  0.53820431 -0.03490273  1.01155997  1.64912858]\n",
      "n_iter 427 loss 0.026388375376864115 weight [ 0.04917877  0.53774484 -0.03534099  1.01238583  1.65039314]\n",
      "n_iter 428 loss 0.0263661939695829 weight [ 0.04897482  0.5372856  -0.03577886  1.01321079  1.65165631]\n",
      "n_iter 429 loss 0.02634406825742314 weight [ 0.04877154  0.5368266  -0.03621635  1.01403484  1.65291808]\n",
      "n_iter 430 loss 0.026321998040797035 weight [ 0.04856894  0.53636786 -0.03665344  1.01485799  1.65417846]\n",
      "n_iter 431 loss 0.026299983121043403 weight [ 0.04836702  0.53590935 -0.03709014  1.01568023  1.65543745]\n",
      "n_iter 432 loss 0.026278023300422376 weight [ 0.04816577  0.53545109 -0.03752646  1.01650158  1.65669506]\n",
      "n_iter 433 loss 0.02625611838211004 weight [ 0.04796519  0.53499307 -0.03796239  1.01732202  1.65795128]\n",
      "n_iter 434 loss 0.02623426817019322 weight [ 0.04776527  0.5345353  -0.03839792  1.01814158  1.65920612]\n",
      "n_iter 435 loss 0.026212472469664235 weight [ 0.04756602  0.53407777 -0.03883307  1.01896024  1.66045959]\n",
      "n_iter 436 loss 0.026190731086415724 weight [ 0.04736744  0.53362049 -0.03926783  1.01977801  1.66171169]\n",
      "n_iter 437 loss 0.026169043827235446 weight [ 0.04716951  0.53316345 -0.03970221  1.0205949   1.66296241]\n",
      "n_iter 438 loss 0.026147410499801207 weight [ 0.04697225  0.53270667 -0.04013619  1.02141089  1.66421177]\n",
      "n_iter 439 loss 0.02612583091267576 weight [ 0.04677564  0.53225012 -0.04056979  1.02222601  1.66545977]\n",
      "n_iter 440 loss 0.026104304875301736 weight [ 0.04657968  0.53179383 -0.041003    1.02304024  1.66670641]\n",
      "n_iter 441 loss 0.026082832197996685 weight [ 0.04638438  0.53133778 -0.04143582  1.0238536   1.66795168]\n",
      "n_iter 442 loss 0.026061412691948022 weight [ 0.04618972  0.53088198 -0.04186825  1.02466608  1.66919561]\n",
      "n_iter 443 loss 0.02604004616920817 weight [ 0.04599571  0.53042642 -0.0423003   1.02547768  1.67043818]\n",
      "n_iter 444 loss 0.026018732442689575 weight [ 0.04580235  0.52997111 -0.04273196  1.02628841  1.67167941]\n",
      "n_iter 445 loss 0.025997471326159875 weight [ 0.04560963  0.52951606 -0.04316323  1.02709827  1.67291929]\n",
      "n_iter 446 loss 0.025976262634237024 weight [ 0.04541755  0.52906125 -0.04359412  1.02790727  1.67415783]\n",
      "n_iter 447 loss 0.02595510618238456 weight [ 0.04522611  0.52860668 -0.04402462  1.02871539  1.67539502]\n",
      "n_iter 448 loss 0.025934001786906715 weight [ 0.0450353   0.52815237 -0.04445474  1.02952266  1.67663089]\n",
      "n_iter 449 loss 0.025912949264943783 weight [ 0.04484513  0.52769831 -0.04488447  1.03032906  1.67786542]\n",
      "n_iter 450 loss 0.025891948434467318 weight [ 0.04465559  0.52724449 -0.04531381  1.0311346   1.67909862]\n",
      "n_iter 451 loss 0.025870999114275525 weight [ 0.04446668  0.52679093 -0.04574277  1.03193929  1.68033049]\n",
      "n_iter 452 loss 0.025850101123988597 weight [ 0.0442784   0.52633762 -0.04617134  1.03274312  1.68156104]\n",
      "n_iter 453 loss 0.025829254284044083 weight [ 0.04409074  0.52588455 -0.04659953  1.0335461   1.68279027]\n",
      "n_iter 454 loss 0.0258084584156923 weight [ 0.0439037   0.52543174 -0.04702733  1.03434822  1.68401818]\n",
      "n_iter 455 loss 0.025787713340991853 weight [ 0.04371729  0.52497917 -0.04745475  1.0351495   1.68524477]\n",
      "n_iter 456 loss 0.025767018882805002 weight [ 0.0435315   0.52452686 -0.04788178  1.03594993  1.68647005]\n",
      "n_iter 457 loss 0.025746374864793278 weight [ 0.04334632  0.5240748  -0.04830843  1.03674952  1.68769403]\n",
      "n_iter 458 loss 0.025725781111412965 weight [ 0.04316175  0.52362299 -0.04873469  1.03754826  1.68891669]\n",
      "n_iter 459 loss 0.02570523744791068 weight [ 0.0429778   0.52317143 -0.04916057  1.03834616  1.69013806]\n",
      "n_iter 460 loss 0.025684743700318983 weight [ 0.04279446  0.52272013 -0.04958607  1.03914323  1.69135812]\n",
      "n_iter 461 loss 0.025664299695452045 weight [ 0.04261173  0.52226907 -0.05001118  1.03993945  1.69257688]\n",
      "n_iter 462 loss 0.025643905260901215 weight [ 0.0424296   0.52181827 -0.05043591  1.04073485  1.69379435]\n",
      "n_iter 463 loss 0.025623560225030814 weight [ 0.04224808  0.52136772 -0.05086026  1.04152941  1.69501053]\n",
      "n_iter 464 loss 0.025603264416973793 weight [ 0.04206715  0.52091742 -0.05128423  1.04232314  1.69622542]\n",
      "n_iter 465 loss 0.02558301766662748 weight [ 0.04188683  0.52046738 -0.05170781  1.04311604  1.69743902]\n",
      "n_iter 466 loss 0.025562819804649396 weight [ 0.04170711  0.52001759 -0.05213101  1.04390812  1.69865134]\n",
      "n_iter 467 loss 0.02554267066245303 weight [ 0.04152798  0.51956805 -0.05255383  1.04469937  1.69986237]\n",
      "n_iter 468 loss 0.02552257007220368 weight [ 0.04134945  0.51911877 -0.05297627  1.0454898   1.70107213]\n",
      "n_iter 469 loss 0.025502517866814303 weight [ 0.04117151  0.51866974 -0.05339832  1.04627941  1.70228062]\n",
      "n_iter 470 loss 0.025482513879941455 weight [ 0.04099416  0.51822096 -0.05381999  1.0470682   1.70348783]\n",
      "n_iter 471 loss 0.025462557945981106 weight [ 0.04081739  0.51777244 -0.05424129  1.04785618  1.70469377]\n",
      "n_iter 472 loss 0.025442649900064694 weight [ 0.04064121  0.51732417 -0.0546622   1.04864334  1.70589844]\n",
      "n_iter 473 loss 0.025422789578055025 weight [ 0.04046562  0.51687615 -0.05508273  1.04942968  1.70710186]\n",
      "n_iter 474 loss 0.02540297681654229 weight [ 0.0402906   0.51642839 -0.05550288  1.05021522  1.70830401]\n",
      "n_iter 475 loss 0.0253832114528401 weight [ 0.04011617  0.51598089 -0.05592265  1.05099995  1.7095049 ]\n",
      "n_iter 476 loss 0.025363493324981525 weight [ 0.03994232  0.51553364 -0.05634204  1.05178387  1.71070453]\n",
      "n_iter 477 loss 0.02534382227171515 weight [ 0.03976904  0.51508664 -0.05676105  1.05256699  1.71190291]\n",
      "n_iter 478 loss 0.02532419813250123 weight [ 0.03959633  0.51463991 -0.05717968  1.0533493   1.71310004]\n",
      "n_iter 479 loss 0.025304620747507767 weight [ 0.0394242   0.51419342 -0.05759794  1.05413082  1.71429593]\n",
      "n_iter 480 loss 0.025285089957606685 weight [ 0.03925263  0.51374719 -0.05801581  1.05491153  1.71549057]\n",
      "n_iter 481 loss 0.02526560560437001 weight [ 0.03908164  0.51330122 -0.05843331  1.05569145  1.71668396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter 482 loss 0.02524616753006606 weight [ 0.03891121  0.5128555  -0.05885042  1.05647057  1.71787612]\n",
      "n_iter 483 loss 0.025226775577655697 weight [ 0.03874134  0.51241004 -0.05926716  1.0572489   1.71906704]\n",
      "n_iter 484 loss 0.025207429590788574 weight [ 0.03857204  0.51196484 -0.05968352  1.05802644  1.72025672]\n",
      "n_iter 485 loss 0.025188129413799377 weight [ 0.0384033   0.51151989 -0.06009951  1.05880319  1.72144518]\n",
      "n_iter 486 loss 0.025168874891704184 weight [ 0.03823511  0.5110752  -0.06051511  1.05957915  1.7226324 ]\n",
      "n_iter 487 loss 0.025149665870196753 weight [ 0.03806748  0.51063077 -0.06093034  1.06035433  1.7238184 ]\n",
      "n_iter 488 loss 0.025130502195644888 weight [ 0.03790041  0.51018659 -0.06134519  1.06112872  1.72500318]\n",
      "n_iter 489 loss 0.025111383715086807 weight [ 0.03773389  0.50974267 -0.06175967  1.06190233  1.72618673]\n",
      "n_iter 490 loss 0.025092310276227534 weight [ 0.03756792  0.50929901 -0.06217377  1.06267516  1.72736906]\n",
      "n_iter 491 loss 0.025073281727435334 weight [ 0.0374025   0.5088556  -0.06258749  1.06344721  1.72855018]\n",
      "n_iter 492 loss 0.02505429791773816 weight [ 0.03723763  0.50841245 -0.06300084  1.06421848  1.72973009]\n",
      "n_iter 493 loss 0.02503535869682011 weight [ 0.0370733   0.50796956 -0.06341381  1.06498898  1.73090878]\n",
      "n_iter 494 loss 0.02501646391501789 weight [ 0.03690952  0.50752693 -0.0638264   1.06575871  1.73208627]\n",
      "n_iter 495 loss 0.024997613423317404 weight [ 0.03674628  0.50708455 -0.06423863  1.06652766  1.73326255]\n",
      "n_iter 496 loss 0.024978807073350207 weight [ 0.03658357  0.50664243 -0.06465047  1.06729585  1.73443763]\n",
      "n_iter 497 loss 0.024960044717390103 weight [ 0.03642141  0.50620057 -0.06506194  1.06806327  1.7356115 ]\n",
      "n_iter 498 loss 0.024941326208349733 weight [ 0.03625978  0.50575897 -0.06547304  1.06882992  1.73678418]\n",
      "n_iter 499 loss 0.024922651399777142 weight [ 0.03609869  0.50531763 -0.06588377  1.06959581  1.73795567]\n",
      "n_iter 500 loss 0.024904020145852447 weight [ 0.03593813  0.50487655 -0.06629412  1.07036094  1.73912596]\n",
      "n_iter 501 loss 0.02488543230138444 weight [ 0.0357781   0.50443572 -0.06670409  1.0711253   1.74029506]\n",
      "n_iter 502 loss 0.02486688772180729 weight [ 0.0356186   0.50399515 -0.0671137   1.07188891  1.74146297]\n",
      "n_iter 503 loss 0.024848386263177195 weight [ 0.03545962  0.50355484 -0.06752293  1.07265176  1.7426297 ]\n",
      "n_iter 504 loss 0.02482992778216915 weight [ 0.03530118  0.50311479 -0.06793179  1.07341386  1.74379524]\n",
      "n_iter 505 loss 0.024811512136073598 weight [ 0.03514325  0.502675   -0.06834028  1.0741752   1.74495961]\n",
      "n_iter 506 loss 0.024793139182793227 weight [ 0.03498585  0.50223547 -0.06874839  1.07493579  1.74612279]\n",
      "n_iter 507 loss 0.024774808780839777 weight [ 0.03482897  0.5017962  -0.06915613  1.07569563  1.7472848 ]\n",
      "n_iter 508 loss 0.02475652078933076 weight [ 0.03467261  0.50135718 -0.0695635   1.07645473  1.74844564]\n",
      "n_iter 509 loss 0.024738275067986287 weight [ 0.03451676  0.50091843 -0.0699705   1.07721307  1.74960531]\n",
      "n_iter 510 loss 0.024720071477125984 weight [ 0.03436143  0.50047994 -0.07037713  1.07797068  1.7507638 ]\n",
      "n_iter 511 loss 0.0247019098776657 weight [ 0.03420661  0.5000417  -0.07078339  1.07872753  1.75192114]\n",
      "n_iter 512 loss 0.024683790131114536 weight [ 0.0340523   0.49960372 -0.07118928  1.07948365  1.75307731]\n",
      "n_iter 513 loss 0.024665712099571618 weight [ 0.03389851  0.49916601 -0.0715948   1.08023903  1.75423231]\n",
      "n_iter 514 loss 0.024647675645723056 weight [ 0.03374522  0.49872855 -0.07199995  1.08099367  1.75538617]\n",
      "n_iter 515 loss 0.02462968063283888 weight [ 0.03359244  0.49829136 -0.07240473  1.08174757  1.75653886]\n",
      "n_iter 516 loss 0.024611726924770004 weight [ 0.03344016  0.49785442 -0.07280914  1.08250074  1.7576904 ]\n",
      "n_iter 517 loss 0.024593814385945136 weight [ 0.03328839  0.49741774 -0.07321318  1.08325318  1.75884079]\n",
      "n_iter 518 loss 0.02457594288136785 weight [ 0.03313711  0.49698133 -0.07361686  1.08400488  1.75999003]\n",
      "n_iter 519 loss 0.024558112276613513 weight [ 0.03298634  0.49654517 -0.07402016  1.08475586  1.76113812]\n",
      "n_iter 520 loss 0.024540322437826383 weight [ 0.03283607  0.49610928 -0.0744231   1.08550611  1.76228507]\n",
      "n_iter 521 loss 0.024522573231716607 weight [ 0.03268629  0.49567364 -0.07482567  1.08625563  1.76343088]\n",
      "n_iter 522 loss 0.024504864525557314 weight [ 0.03253701  0.49523826 -0.07522787  1.08700442  1.76457555]\n",
      "n_iter 523 loss 0.024487196187181648 weight [ 0.03238822  0.49480315 -0.0756297   1.0877525   1.76571908]\n",
      "n_iter 524 loss 0.02446956808497995 weight [ 0.03223992  0.4943683  -0.07603117  1.08849985  1.76686148]\n",
      "n_iter 525 loss 0.024451980087896796 weight [ 0.03209211  0.4939337  -0.07643227  1.08924648  1.76800274]\n",
      "n_iter 526 loss 0.02443443206542819 weight [ 0.03194479  0.49349937 -0.07683301  1.08999239  1.76914288]\n",
      "n_iter 527 loss 0.024416923887618677 weight [ 0.03179795  0.4930653  -0.07723338  1.09073759  1.77028189]\n",
      "n_iter 528 loss 0.024399455425058548 weight [ 0.0316516   0.49263149 -0.07763338  1.09148208  1.77141977]\n",
      "n_iter 529 loss 0.024382026548881004 weight [ 0.03150574  0.49219794 -0.07803302  1.09222585  1.77255653]\n",
      "n_iter 530 loss 0.024364637130759394 weight [ 0.03136035  0.49176465 -0.0784323   1.0929689   1.77369216]\n",
      "n_iter 531 loss 0.024347287042904384 weight [ 0.03121545  0.49133162 -0.07883121  1.09371125  1.77482668]\n",
      "n_iter 532 loss 0.02432997615806127 weight [ 0.03107103  0.49089885 -0.07922975  1.09445289  1.77596008]\n",
      "n_iter 533 loss 0.02431270434950717 weight [ 0.03092708  0.49046634 -0.07962793  1.09519382  1.77709237]\n",
      "n_iter 534 loss 0.02429547149104835 weight [ 0.0307836   0.4900341  -0.08002575  1.09593405  1.77822355]\n",
      "n_iter 535 loss 0.024278277457017484 weight [ 0.03064061  0.48960211 -0.0804232   1.09667357  1.77935362]\n",
      "n_iter 536 loss 0.024261122122270957 weight [ 0.03049808  0.48917039 -0.08082029  1.09741239  1.78048258]\n",
      "n_iter 537 loss 0.024244005362186235 weight [ 0.03035602  0.48873893 -0.08121702  1.09815051  1.78161043]\n",
      "n_iter 538 loss 0.024226927052659154 weight [ 0.03021444  0.48830773 -0.08161339  1.09888794  1.78273718]\n",
      "n_iter 539 loss 0.02420988707010131 weight [ 0.03007332  0.48787679 -0.08200939  1.09962466  1.78386284]\n",
      "n_iter 540 loss 0.024192885291437413 weight [ 0.02993266  0.48744611 -0.08240503  1.10036069  1.78498739]\n",
      "n_iter 541 loss 0.024175921594102708 weight [ 0.02979247  0.48701569 -0.08280031  1.10109602  1.78611085]\n",
      "n_iter 542 loss 0.024158995856040328 weight [ 0.02965275  0.48658553 -0.08319523  1.10183066  1.78723321]\n",
      "n_iter 543 loss 0.02414210795569877 weight [ 0.02951348  0.48615564 -0.08358979  1.10256461  1.78835448]\n",
      "n_iter 544 loss 0.0241252577720293 weight [ 0.02937468  0.48572601 -0.08398398  1.10329787  1.78947467]\n",
      "n_iter 545 loss 0.02410844518448342 weight [ 0.02923633  0.48529663 -0.08437782  1.10403044  1.79059376]\n",
      "n_iter 546 loss 0.024091670073010326 weight [ 0.02909844  0.48486752 -0.0847713   1.10476233  1.79171177]\n",
      "n_iter 547 loss 0.024074932318054403 weight [ 0.02896101  0.48443868 -0.08516441  1.10549352  1.7928287 ]\n",
      "n_iter 548 loss 0.024058231800552696 weight [ 0.02882403  0.48401009 -0.08555717  1.10622404  1.79394455]\n",
      "n_iter 549 loss 0.024041568401932487 weight [ 0.0286875   0.48358176 -0.08594957  1.10695387  1.79505931]\n",
      "n_iter 550 loss 0.02402494200410873 weight [ 0.02855143  0.4831537  -0.08634161  1.10768302  1.79617301]\n",
      "n_iter 551 loss 0.024008352489481695 weight [ 0.0284158   0.4827259  -0.08673329  1.1084115   1.79728562]\n",
      "n_iter 552 loss 0.023991799740934454 weight [ 0.02828062  0.48229836 -0.08712461  1.10913929  1.79839717]\n",
      "n_iter 553 loss 0.023975283641830485 weight [ 0.02814589  0.48187108 -0.08751557  1.10986641  1.79950764]\n",
      "n_iter 554 loss 0.023958804076011243 weight [ 0.0280116   0.48144406 -0.08790618  1.11059285  1.80061705]\n",
      "n_iter 555 loss 0.023942360927793796 weight [ 0.02787775  0.4810173  -0.08829643  1.11131862  1.80172539]\n",
      "n_iter 556 loss 0.023925954081968424 weight [ 0.02774435  0.48059081 -0.08868632  1.11204372  1.80283267]\n",
      "n_iter 557 loss 0.0239095834237962 weight [ 0.02761139  0.48016458 -0.08907586  1.11276815  1.80393889]\n",
      "n_iter 558 loss 0.023893248839006734 weight [ 0.02747887  0.4797386  -0.08946504  1.11349191  1.80504404]\n",
      "n_iter 559 loss 0.023876950213795756 weight [ 0.02734678  0.47931289 -0.08985386  1.114215    1.80614814]\n",
      "n_iter 560 loss 0.02386068743482282 weight [ 0.02721513  0.47888745 -0.09024233  1.11493742  1.80725119]\n",
      "n_iter 561 loss 0.02384446038920897 weight [ 0.02708392  0.47846226 -0.09063045  1.11565918  1.80835318]\n",
      "n_iter 562 loss 0.023828268964534487 weight [ 0.02695314  0.47803734 -0.09101821  1.11638028  1.80945412]\n",
      "n_iter 563 loss 0.02381211304883655 weight [ 0.02682279  0.47761267 -0.09140561  1.11710071  1.81055401]\n",
      "n_iter 564 loss 0.023795992530606984 weight [ 0.02669287  0.47718827 -0.09179266  1.11782049  1.81165286]\n",
      "n_iter 565 loss 0.023779907298790034 weight [ 0.02656338  0.47676413 -0.09217935  1.1185396   1.81275066]\n",
      "n_iter 566 loss 0.02376385724278005 weight [ 0.02643432  0.47634025 -0.0925657   1.11925806  1.81384741]\n",
      "n_iter 567 loss 0.02374784225241933 weight [ 0.02630569  0.47591664 -0.09295168  1.11997586  1.81494313]\n",
      "n_iter 568 loss 0.023731862217995867 weight [ 0.02617748  0.47549328 -0.09333732  1.12069301  1.81603781]\n",
      "n_iter 569 loss 0.023715917030241142 weight [ 0.02604969  0.47507019 -0.0937226   1.1214095   1.81713145]\n",
      "n_iter 570 loss 0.023700006580327913 weight [ 0.02592233  0.47464736 -0.09410753  1.12212534  1.81822405]\n",
      "n_iter 571 loss 0.023684130759868107 weight [ 0.02579538  0.47422479 -0.09449211  1.12284053  1.81931563]\n",
      "n_iter 572 loss 0.023668289460910565 weight [ 0.02566886  0.47380248 -0.09487634  1.12355507  1.82040617]\n",
      "n_iter 573 loss 0.023652482575938952 weight [ 0.02554275  0.47338043 -0.09526021  1.12426897  1.82149568]\n",
      "n_iter 574 loss 0.02363670999786959 weight [ 0.02541706  0.47295865 -0.09564373  1.12498221  1.82258417]\n",
      "n_iter 575 loss 0.023620971620049325 weight [ 0.02529179  0.47253713 -0.09602691  1.12569482  1.82367164]\n",
      "n_iter 576 loss 0.023605267336253438 weight [ 0.02516693  0.47211586 -0.09640973  1.12640678  1.82475808]\n",
      "n_iter 577 loss 0.02358959704068351 weight [ 0.02504248  0.47169486 -0.0967922   1.12711809  1.8258435 ]\n",
      "n_iter 578 loss 0.023573960627965344 weight [ 0.02491845  0.47127412 -0.09717432  1.12782877  1.8269279 ]\n",
      "n_iter 579 loss 0.023558357993146904 weight [ 0.02479482  0.47085365 -0.0975561   1.1285388   1.82801128]\n",
      "n_iter 580 loss 0.023542789031696217 weight [ 0.0246716   0.47043343 -0.09793752  1.1292482   1.82909365]\n",
      "n_iter 581 loss 0.023527253639499347 weight [ 0.02454879  0.47001348 -0.0983186   1.12995696  1.83017501]\n",
      "n_iter 582 loss 0.02351175171285832 weight [ 0.02442639  0.46959378 -0.09869932  1.13066508  1.83125536]\n",
      "n_iter 583 loss 0.02349628314848913 weight [ 0.02430439  0.46917435 -0.0990797   1.13137257  1.83233469]\n",
      "n_iter 584 loss 0.023480847843519698 weight [ 0.0241828   0.46875518 -0.09945973  1.13207943  1.83341302]\n",
      "n_iter 585 loss 0.02346544569548787 weight [ 0.0240616   0.46833627 -0.09983941  1.13278566  1.83449035]\n",
      "n_iter 586 loss 0.02345007660233942 weight [ 0.02394081  0.46791762 -0.10021875  1.13349125  1.83556667]\n",
      "n_iter 587 loss 0.023434740462426076 weight [ 0.02382041  0.46749924 -0.10059774  1.13419622  1.836642  ]\n",
      "n_iter 588 loss 0.02341943717450352 weight [ 0.02370042  0.46708111 -0.10097638  1.13490056  1.83771632]\n",
      "n_iter 589 loss 0.02340416663772947 weight [ 0.02358082  0.46666325 -0.10135468  1.13560427  1.83878964]\n",
      "n_iter 590 loss 0.023388928751661705 weight [ 0.02346162  0.46624564 -0.10173263  1.13630736  1.83986197]\n",
      "n_iter 591 loss 0.023373723416256105 weight [ 0.02334281  0.4658283  -0.10211023  1.13700983  1.84093331]\n",
      "n_iter 592 loss 0.02335855053186477 weight [ 0.02322439  0.46541122 -0.10248749  1.13771167  1.84200366]\n",
      "n_iter 593 loss 0.02334340999923408 weight [ 0.02310636  0.4649944  -0.10286441  1.13841289  1.84307301]\n",
      "n_iter 594 loss 0.023328301719502797 weight [ 0.02298873  0.46457784 -0.10324098  1.13911349  1.84414138]\n",
      "n_iter 595 loss 0.023313225594200147 weight [ 0.02287149  0.46416154 -0.10361721  1.13981347  1.84520876]\n",
      "n_iter 596 loss 0.023298181525243974 weight [ 0.02275463  0.4637455  -0.10399309  1.14051284  1.84627516]\n",
      "n_iter 597 loss 0.023283169414938836 weight [ 0.02263816  0.46332973 -0.10436863  1.14121159  1.84734058]\n",
      "n_iter 598 loss 0.02326818916597418 weight [ 0.02252207  0.46291421 -0.10474382  1.14190972  1.84840502]\n",
      "n_iter 599 loss 0.02325324068142245 weight [ 0.02240637  0.46249895 -0.10511868  1.14260724  1.84946847]\n",
      "n_iter 600 loss 0.02323832386473727 weight [ 0.02229105  0.46208396 -0.10549319  1.14330415  1.85053095]\n",
      "n_iter 601 loss 0.0232234386197516 weight [ 0.02217612  0.46166923 -0.10586736  1.14400045  1.85159246]\n",
      "n_iter 602 loss 0.02320858485067595 weight [ 0.02206156  0.46125475 -0.10624118  1.14469614  1.852653  ]\n",
      "n_iter 603 loss 0.02319376246209653 weight [ 0.02194738  0.46084054 -0.10661467  1.14539122  1.85371256]\n",
      "n_iter 604 loss 0.023178971358973472 weight [ 0.02183358  0.46042659 -0.10698781  1.14608569  1.85477115]\n",
      "n_iter 605 loss 0.02316421144663905 weight [ 0.02172016  0.4600129  -0.10736061  1.14677956  1.85582878]\n",
      "n_iter 606 loss 0.02314948263079587 weight [ 0.02160712  0.45959946 -0.10773308  1.14747283  1.85688544]\n",
      "n_iter 607 loss 0.02313478481751514 weight [ 0.02149444  0.45918629 -0.1081052   1.14816548  1.85794114]\n",
      "n_iter 608 loss 0.023120117913234883 weight [ 0.02138214  0.45877338 -0.10847698  1.14885754  1.85899587]\n",
      "n_iter 609 loss 0.023105481824758224 weight [ 0.02127022  0.45836073 -0.10884842  1.149549    1.86004965]\n",
      "n_iter 610 loss 0.023090876459251584 weight [ 0.02115866  0.45794834 -0.10921953  1.15023986  1.86110247]\n",
      "n_iter 611 loss 0.023076301724243048 weight [ 0.02104747  0.45753621 -0.10959029  1.15093011  1.86215433]\n",
      "n_iter 612 loss 0.02306175752762055 weight [ 0.02093665  0.45712434 -0.10996072  1.15161978  1.86320524]\n",
      "n_iter 613 loss 0.023047243777630254 weight [ 0.0208262   0.45671273 -0.11033081  1.15230884  1.86425519]\n",
      "n_iter 614 loss 0.023032760382874756 weight [ 0.02071612  0.45630138 -0.11070056  1.15299731  1.86530419]\n",
      "n_iter 615 loss 0.023018307252311507 weight [ 0.0206064   0.45589029 -0.11106997  1.15368519  1.86635225]\n",
      "n_iter 616 loss 0.023003884295251017 weight [ 0.02049704  0.45547946 -0.11143904  1.15437247  1.86739935]\n",
      "n_iter 617 loss 0.0229894914213553 weight [ 0.02038804  0.45506889 -0.11180778  1.15505917  1.86844551]\n",
      "n_iter 618 loss 0.022975128540636085 weight [ 0.02027941  0.45465858 -0.11217618  1.15574527  1.86949073]\n",
      "n_iter 619 loss 0.022960795563453307 weight [ 0.02017114  0.45424853 -0.11254425  1.15643078  1.87053501]\n",
      "n_iter 620 loss 0.022946492400513326 weight [ 0.02006322  0.45383873 -0.11291198  1.15711571  1.87157834]\n",
      "n_iter 621 loss 0.022932218962867415 weight [ 0.01995566  0.4534292  -0.11327938  1.15780005  1.87262073]\n",
      "n_iter 622 loss 0.02291797516191004 weight [ 0.01984846  0.45301993 -0.11364644  1.15848381  1.87366219]\n",
      "n_iter 623 loss 0.022903760909377294 weight [ 0.01974162  0.45261091 -0.11401316  1.15916698  1.87470272]\n",
      "n_iter 624 loss 0.02288957611734529 weight [ 0.01963513  0.45220216 -0.11437955  1.15984957  1.87574231]\n",
      "n_iter 625 loss 0.022875420698228545 weight [ 0.01952899  0.45179366 -0.11474561  1.16053157  1.87678096]\n",
      "n_iter 626 loss 0.022861294564778405 weight [ 0.01942321  0.45138543 -0.11511133  1.161213    1.87781869]\n",
      "n_iter 627 loss 0.022847197630081452 weight [ 0.01931777  0.45097745 -0.11547672  1.16189385  1.87885549]\n",
      "n_iter 628 loss 0.022833129807557976 weight [ 0.01921269  0.45056973 -0.11584178  1.16257412  1.87989137]\n",
      "n_iter 629 loss 0.022819091010960333 weight [ 0.01910795  0.45016227 -0.1162065   1.16325381  1.88092631]\n",
      "n_iter 630 loss 0.022805081154371472 weight [ 0.01900357  0.44975507 -0.11657089  1.16393292  1.88196034]\n",
      "n_iter 631 loss 0.022791100152203356 weight [ 0.01889953  0.44934813 -0.11693495  1.16461147  1.88299344]\n",
      "n_iter 632 loss 0.022777147919195398 weight [ 0.01879583  0.44894144 -0.11729868  1.16528943  1.88402563]\n",
      "n_iter 633 loss 0.022763224370413008 weight [ 0.01869248  0.44853502 -0.11766207  1.16596683  1.88505689]\n",
      "n_iter 634 loss 0.02274932942124599 weight [ 0.01858947  0.44812885 -0.11802514  1.16664365  1.88608724]\n",
      "n_iter 635 loss 0.02273546298740709 weight [ 0.0184868   0.44772294 -0.11838787  1.16731991  1.88711668]\n",
      "n_iter 636 loss 0.022721624984930473 weight [ 0.01838448  0.44731729 -0.11875027  1.16799559  1.8881452 ]\n",
      "n_iter 637 loss 0.02270781533017022 weight [ 0.01828249  0.4469119  -0.11911235  1.16867071  1.88917281]\n",
      "n_iter 638 loss 0.022694033939798866 weight [ 0.01818085  0.44650677 -0.11947409  1.16934526  1.89019951]\n",
      "n_iter 639 loss 0.022680280730805878 weight [ 0.01807954  0.44610189 -0.1198355   1.17001924  1.8912253 ]\n",
      "n_iter 640 loss 0.02266655562049627 weight [ 0.01797857  0.44569728 -0.12019659  1.17069266  1.89225019]\n",
      "n_iter 641 loss 0.02265285852648903 weight [ 0.01787793  0.44529292 -0.12055735  1.17136552  1.89327417]\n",
      "n_iter 642 loss 0.022639189366715752 weight [ 0.01777763  0.44488882 -0.12091777  1.17203781  1.89429725]\n",
      "n_iter 643 loss 0.022625548059419163 weight [ 0.01767766  0.44448497 -0.12127787  1.17270954  1.89531943]\n",
      "n_iter 644 loss 0.02261193452315167 weight [ 0.01757802  0.44408139 -0.12163765  1.17338072  1.89634071]\n",
      "n_iter 645 loss 0.022598348676773974 weight [ 0.01747872  0.44367806 -0.12199709  1.17405133  1.89736109]\n",
      "n_iter 646 loss 0.022584790439453575 weight [ 0.01737974  0.44327499 -0.12235621  1.17472139  1.89838057]\n",
      "n_iter 647 loss 0.022571259730663446 weight [ 0.0172811   0.44287217 -0.122715    1.17539088  1.89939916]\n",
      "n_iter 648 loss 0.022557756470180552 weight [ 0.01718278  0.44246962 -0.12307347  1.17605983  1.90041685]\n",
      "n_iter 649 loss 0.022544280578084507 weight [ 0.01708479  0.44206732 -0.12343161  1.17672822  1.90143365]\n",
      "n_iter 650 loss 0.02253083197475613 weight [ 0.01698713  0.44166527 -0.12378942  1.17739605  1.90244957]\n",
      "n_iter 651 loss 0.022517410580876116 weight [ 0.01688979  0.44126349 -0.12414691  1.17806333  1.90346459]\n",
      "n_iter 652 loss 0.022504016317423624 weight [ 0.01679277  0.44086196 -0.12450407  1.17873006  1.90447873]\n",
      "n_iter 653 loss 0.0224906491056749 weight [ 0.01669608  0.44046069 -0.12486091  1.17939625  1.90549198]\n",
      "n_iter 654 loss 0.022477308867201964 weight [ 0.01659971  0.44005967 -0.12521742  1.18006188  1.90650435]\n",
      "n_iter 655 loss 0.022463995523871214 weight [ 0.01650366  0.43965892 -0.12557361  1.18072696  1.90751584]\n",
      "n_iter 656 loss 0.022450708997842076 weight [ 0.01640794  0.43925841 -0.12592948  1.1813915   1.90852644]\n",
      "n_iter 657 loss 0.02243744921156571 weight [ 0.01631253  0.43885817 -0.12628502  1.18205549  1.90953617]\n",
      "n_iter 658 loss 0.02242421608778362 weight [ 0.01621743  0.43845818 -0.12664024  1.18271893  1.91054502]\n",
      "n_iter 659 loss 0.02241100954952639 weight [ 0.01612266  0.43805845 -0.12699514  1.18338183  1.911553  ]\n",
      "n_iter 660 loss 0.022397829520112305 weight [ 0.0160282   0.43765897 -0.12734972  1.18404419  1.9125601 ]\n",
      "n_iter 661 loss 0.02238467592314609 weight [ 0.01593405  0.43725975 -0.12770397  1.18470601  1.91356632]\n",
      "n_iter 662 loss 0.022371548682517572 weight [ 0.01584022  0.43686079 -0.1280579   1.18536729  1.91457168]\n",
      "n_iter 663 loss 0.022358447722400417 weight [ 0.01574671  0.43646208 -0.12841151  1.18602802  1.91557617]\n",
      "n_iter 664 loss 0.02234537296725082 weight [ 0.0156535   0.43606362 -0.1287648   1.18668822  1.91657979]\n",
      "n_iter 665 loss 0.0223323243418062 weight [ 0.01556061  0.43566543 -0.12911777  1.18734788  1.91758254]\n",
      "n_iter 666 loss 0.022319301771083978 weight [ 0.01546802  0.43526748 -0.12947042  1.188007    1.91858443]\n",
      "n_iter 667 loss 0.022306305180380275 weight [ 0.01537574  0.4348698  -0.12982275  1.18866559  1.91958545]\n",
      "n_iter 668 loss 0.022293334495268657 weight [ 0.01528378  0.43447237 -0.13017476  1.18932364  1.92058562]\n",
      "n_iter 669 loss 0.022280389641598848 weight [ 0.01519212  0.43407519 -0.13052645  1.18998116  1.92158492]\n",
      "n_iter 670 loss 0.022267470545495563 weight [ 0.01510076  0.43367827 -0.13087782  1.19063815  1.92258336]\n",
      "n_iter 671 loss 0.02225457713335717 weight [ 0.01500971  0.4332816  -0.13122888  1.19129461  1.92358095]\n",
      "n_iter 672 loss 0.022241709331854535 weight [ 0.01491896  0.43288519 -0.13157961  1.19195053  1.92457768]\n",
      "n_iter 673 loss 0.02222886706792973 weight [ 0.01482852  0.43248904 -0.13193003  1.19260593  1.92557356]\n",
      "n_iter 674 loss 0.02221605026879485 weight [ 0.01473838  0.43209313 -0.13228013  1.19326079  1.92656859]\n",
      "n_iter 675 loss 0.022203258861930805 weight [ 0.01464854  0.43169749 -0.13262991  1.19391513  1.92756276]\n",
      "n_iter 676 loss 0.022190492775086054 weight [ 0.014559    0.43130209 -0.13297938  1.19456895  1.92855608]\n",
      "n_iter 677 loss 0.022177751936275487 weight [ 0.01446976  0.43090696 -0.13332853  1.19522224  1.92954856]\n",
      "n_iter 678 loss 0.02216503627377915 weight [ 0.01438082  0.43051207 -0.13367736  1.195875    1.93054019]\n",
      "n_iter 679 loss 0.02215234571614109 weight [ 0.01429218  0.43011744 -0.13402588  1.19652724  1.93153097]\n",
      "n_iter 680 loss 0.022139680192168203 weight [ 0.01420383  0.42972307 -0.13437408  1.19717896  1.93252091]\n",
      "n_iter 681 loss 0.022127039630928977 weight [ 0.01411578  0.42932894 -0.13472197  1.19783015  1.93351001]\n",
      "n_iter 682 loss 0.022114423961752408 weight [ 0.01402802  0.42893508 -0.13506954  1.19848083  1.93449827]\n",
      "n_iter 683 loss 0.02210183311422677 weight [ 0.01394056  0.42854146 -0.13541679  1.19913099  1.93548569]\n",
      "n_iter 684 loss 0.022089267018198484 weight [ 0.01385339  0.4281481  -0.13576374  1.19978062  1.93647227]\n",
      "n_iter 685 loss 0.02207672560377098 weight [ 0.01376651  0.42775499 -0.13611037  1.20042974  1.93745801]\n",
      "n_iter 686 loss 0.02206420880130352 weight [ 0.01367992  0.42736214 -0.13645668  1.20107835  1.93844292]\n",
      "n_iter 687 loss 0.02205171654141007 weight [ 0.01359362  0.42696954 -0.13680269  1.20172644  1.939427  ]\n",
      "n_iter 688 loss 0.02203924875495816 weight [ 0.01350762  0.42657719 -0.13714838  1.20237401  1.94041024]\n",
      "n_iter 689 loss 0.022026805373067778 weight [ 0.0134219   0.42618509 -0.13749375  1.20302107  1.94139265]\n",
      "n_iter 690 loss 0.022014386327110228 weight [ 0.01333646  0.42579325 -0.13783882  1.20366762  1.94237424]\n",
      "n_iter 691 loss 0.022001991548707 weight [ 0.01325132  0.42540166 -0.13818357  1.20431365  1.943355  ]\n",
      "n_iter 692 loss 0.021989620969728695 weight [ 0.01316645  0.42501033 -0.13852802  1.20495918  1.94433493]\n",
      "n_iter 693 loss 0.021977274522293894 weight [ 0.01308188  0.42461924 -0.13887215  1.20560419  1.94531403]\n",
      "n_iter 694 loss 0.021964952138768073 weight [ 0.01299759  0.42422841 -0.13921597  1.2062487   1.94629232]\n",
      "n_iter 695 loss 0.021952653751762485 weight [ 0.01291358  0.42383783 -0.13955948  1.20689269  1.94726978]\n",
      "n_iter 696 loss 0.021940379294133103 weight [ 0.01282985  0.4234475  -0.13990268  1.20753619  1.94824642]\n",
      "n_iter 697 loss 0.02192812869897952 weight [ 0.0127464   0.42305743 -0.14024557  1.20817917  1.94922224]\n",
      "n_iter 698 loss 0.02191590189964388 weight [ 0.01266323  0.4226676  -0.14058815  1.20882165  1.95019724]\n",
      "n_iter 699 loss 0.021903698829709804 weight [ 0.01258035  0.42227803 -0.14093042  1.20946363  1.95117143]\n",
      "n_iter 700 loss 0.02189151942300131 weight [ 0.01249774  0.42188871 -0.14127238  1.2101051   1.9521448 ]\n",
      "n_iter 701 loss 0.0218793636135818 weight [ 0.01241541  0.42149964 -0.14161404  1.21074607  1.95311736]\n",
      "n_iter 702 loss 0.021867231335752944 weight [ 0.01233335  0.42111082 -0.14195539  1.21138654  1.95408911]\n",
      "n_iter 703 loss 0.02185512252405368 weight [ 0.01225157  0.42072226 -0.14229643  1.21202651  1.95506005]\n",
      "n_iter 704 loss 0.02184303711325914 weight [ 0.01217007  0.42033394 -0.14263716  1.21266598  1.95603018]\n",
      "n_iter 705 loss 0.021830975038379642 weight [ 0.01208884  0.41994588 -0.14297758  1.21330495  1.9569995 ]\n",
      "n_iter 706 loss 0.02181893623465961 weight [ 0.01200788  0.41955807 -0.1433177   1.21394342  1.95796801]\n",
      "n_iter 707 loss 0.021806920637576607 weight [ 0.0119272   0.4191705  -0.14365751  1.2145814   1.95893572]\n",
      "n_iter 708 loss 0.021794928182840265 weight [ 0.01184678  0.41878319 -0.14399702  1.21521888  1.95990263]\n",
      "n_iter 709 loss 0.021782958806391296 weight [ 0.01176664  0.41839613 -0.14433622  1.21585586  1.96086873]\n",
      "n_iter 710 loss 0.02177101244440047 weight [ 0.01168677  0.41800932 -0.14467511  1.21649236  1.96183403]\n",
      "n_iter 711 loss 0.02175908903326761 weight [ 0.01160716  0.41762276 -0.1450137   1.21712836  1.96279854]\n",
      "n_iter 712 loss 0.021747188509620576 weight [ 0.01152783  0.41723645 -0.14535198  1.21776386  1.96376224]\n",
      "n_iter 713 loss 0.02173531081031431 weight [ 0.01144876  0.41685039 -0.14568996  1.21839888  1.96472515]\n",
      "n_iter 714 loss 0.02172345587242981 weight [ 0.01136996  0.41646458 -0.14602764  1.2190334   1.96568726]\n",
      "n_iter 715 loss 0.02171162363327315 weight [ 0.01129142  0.41607902 -0.14636501  1.21966744  1.96664858]\n",
      "n_iter 716 loss 0.0216998140303745 weight [ 0.01121315  0.41569371 -0.14670208  1.22030098  1.96760911]\n",
      "n_iter 717 loss 0.021688027001487167 weight [ 0.01113515  0.41530865 -0.14703885  1.22093404  1.96856884]\n",
      "n_iter 718 loss 0.02167626248458661 weight [ 0.0110574   0.41492383 -0.14737531  1.22156662  1.96952778]\n",
      "n_iter 719 loss 0.02166452041786947 weight [ 0.01097992  0.41453927 -0.14771147  1.2221987   1.97048594]\n",
      "n_iter 720 loss 0.02165280073975262 weight [ 0.0109027   0.41415496 -0.14804733  1.2228303   1.97144331]\n",
      "n_iter 721 loss 0.021641103388872207 weight [ 0.01082574  0.41377089 -0.14838289  1.22346142  1.97239989]\n",
      "n_iter 722 loss 0.02162942830408271 weight [ 0.01074904  0.41338708 -0.14871814  1.22409206  1.97335569]\n",
      "n_iter 723 loss 0.021617775424455952 weight [ 0.0106726   0.41300351 -0.1490531   1.22472221  1.9743107 ]\n",
      "n_iter 724 loss 0.021606144689280227 weight [ 0.01059642  0.41262019 -0.14938775  1.22535188  1.97526494]\n",
      "n_iter 725 loss 0.021594536038059314 weight [ 0.01052049  0.41223712 -0.14972211  1.22598107  1.97621839]\n",
      "n_iter 726 loss 0.021582949410511556 weight [ 0.01044483  0.4118543  -0.15005616  1.22660978  1.97717106]\n",
      "n_iter 727 loss 0.021571384746568945 weight [ 0.01036941  0.41147172 -0.15038992  1.22723801  1.97812295]\n",
      "n_iter 728 loss 0.021559841986376192 weight [ 0.01029426  0.4110894  -0.15072337  1.22786576  1.97907407]\n",
      "n_iter 729 loss 0.0215483210702898 weight [ 0.01021935  0.41070732 -0.15105653  1.22849304  1.98002441]\n",
      "n_iter 730 loss 0.021536821938877165 weight [ 0.0101447   0.41032549 -0.15138938  1.22911984  1.98097398]\n",
      "n_iter 731 loss 0.02152534453291567 weight [ 0.01007031  0.40994391 -0.15172194  1.22974616  1.98192278]\n",
      "n_iter 732 loss 0.021513888793391778 weight [ 0.00999616  0.40956257 -0.1520542   1.23037201  1.9828708 ]\n",
      "n_iter 733 loss 0.021502454661500102 weight [ 0.00992227  0.40918148 -0.15238617  1.23099738  1.98381805]\n",
      "n_iter 734 loss 0.021491042078642594 weight [ 0.00984863  0.40880064 -0.15271783  1.23162229  1.98476454]\n",
      "n_iter 735 loss 0.021479650986427554 weight [ 0.00977523  0.40842005 -0.1530492   1.23224672  1.98571025]\n",
      "n_iter 736 loss 0.021468281326668805 weight [ 0.00970209  0.4080397  -0.15338027  1.23287068  1.9866552 ]\n",
      "n_iter 737 loss 0.021456933041384803 weight [ 0.00962919  0.4076596  -0.15371105  1.23349417  1.98759939]\n",
      "n_iter 738 loss 0.021445606072797753 weight [ 0.00955654  0.40727975 -0.15404153  1.23411719  1.98854281]\n",
      "n_iter 739 loss 0.021434300363332746 weight [ 0.00948414  0.40690014 -0.15437171  1.23473974  1.98948547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter 740 loss 0.02142301585561688 weight [ 0.00941198  0.40652078 -0.1547016   1.23536182  1.99042737]\n",
      "n_iter 741 loss 0.021411752492478404 weight [ 0.00934007  0.40614167 -0.15503119  1.23598344  1.9913685 ]\n",
      "n_iter 742 loss 0.02140051021694586 weight [ 0.0092684   0.4057628  -0.15536049  1.23660459  1.99230888]\n",
      "n_iter 743 loss 0.02138928897224722 weight [ 0.00919698  0.40538418 -0.15568949  1.23722527  1.9932485 ]\n",
      "n_iter 744 loss 0.02137808870180907 weight [ 0.0091258   0.4050058  -0.1560182   1.23784549  1.99418737]\n",
      "n_iter 745 loss 0.021366909349255683 weight [ 0.00905486  0.40462767 -0.15634662  1.23846525  1.99512548]\n",
      "n_iter 746 loss 0.021355750858408278 weight [ 0.00898416  0.40424979 -0.15667474  1.23908454  1.99606283]\n",
      "n_iter 747 loss 0.021344613173284118 weight [ 0.0089137   0.40387215 -0.15700257  1.23970337  1.99699944]\n",
      "n_iter 748 loss 0.021333496238095696 weight [ 0.00884348  0.40349475 -0.15733011  1.24032174  1.99793529]\n",
      "n_iter 749 loss 0.021322399997249886 weight [ 0.0087735   0.4031176  -0.15765735  1.24093965  1.99887039]\n",
      "n_iter 750 loss 0.021311324395347178 weight [ 0.00870376  0.4027407  -0.1579843   1.2415571   1.99980475]\n",
      "n_iter 751 loss 0.02130026937718077 weight [ 0.00863426  0.40236404 -0.15831096  1.24217409  2.00073835]\n",
      "n_iter 752 loss 0.021289234887735816 weight [ 0.00856499  0.40198763 -0.15863733  1.24279063  2.00167121]\n",
      "n_iter 753 loss 0.0212782208721886 weight [ 0.00849596  0.40161146 -0.15896341  1.2434067   2.00260333]\n",
      "n_iter 754 loss 0.021267227275905704 weight [ 0.00842716  0.40123553 -0.1592892   1.24402232  2.0035347 ]\n",
      "n_iter 755 loss 0.021256254044443233 weight [ 0.0083586   0.40085985 -0.15961469  1.24463749  2.00446533]\n",
      "n_iter 756 loss 0.021245301123545987 weight [ 0.00829027  0.40048441 -0.1599399   1.2452522   2.00539521]\n",
      "n_iter 757 loss 0.0212343684591467 weight [ 0.00822218  0.40010922 -0.16026482  1.24586645  2.00632436]\n",
      "n_iter 758 loss 0.021223455997365206 weight [ 0.00815432  0.39973427 -0.16058944  1.24648025  2.00725277]\n",
      "n_iter 759 loss 0.02121256368450768 weight [ 0.00808669  0.39935956 -0.16091378  1.2470936   2.00818044]\n",
      "n_iter 760 loss 0.021201691467065836 weight [ 0.00801929  0.3989851  -0.16123783  1.2477065   2.00910737]\n",
      "n_iter 761 loss 0.021190839291716158 weight [ 0.00795212  0.39861088 -0.16156159  1.24831895  2.01003357]\n",
      "n_iter 762 loss 0.02118000710531913 weight [ 0.00788518  0.3982369  -0.16188506  1.24893095  2.01095903]\n",
      "n_iter 763 loss 0.02116919485491842 weight [ 0.00781847  0.39786317 -0.16220825  1.2495425   2.01188376]\n",
      "n_iter 764 loss 0.02115840248774017 weight [ 0.00775198  0.39748968 -0.16253115  1.2501536   2.01280776]\n",
      "n_iter 765 loss 0.0211476299511922 weight [ 0.00768573  0.39711644 -0.16285376  1.25076425  2.01373103]\n",
      "n_iter 766 loss 0.02113687719286322 weight [ 0.0076197   0.39674343 -0.16317608  1.25137445  2.01465357]\n",
      "n_iter 767 loss 0.021126144160522126 weight [ 0.0075539   0.39637067 -0.16349811  1.25198421  2.01557538]\n",
      "n_iter 768 loss 0.021115430802117228 weight [ 0.00748832  0.39599815 -0.16381986  1.25259353  2.01649647]\n",
      "n_iter 769 loss 0.021104737065775446 weight [ 0.00742297  0.39562587 -0.16414133  1.2532024   2.01741682]\n",
      "n_iter 770 loss 0.021094062899801654 weight [ 0.00735784  0.39525384 -0.16446251  1.25381082  2.01833646]\n",
      "n_iter 771 loss 0.02108340825267786 weight [ 0.00729294  0.39488204 -0.1647834   1.2544188   2.01925537]\n",
      "n_iter 772 loss 0.02107277307306252 weight [ 0.00722825  0.39451049 -0.16510401  1.25502634  2.02017356]\n",
      "n_iter 773 loss 0.021062157309789765 weight [ 0.00716379  0.39413918 -0.16542433  1.25563344  2.02109102]\n",
      "n_iter 774 loss 0.02105156091186868 weight [ 0.00709955  0.39376812 -0.16574437  1.2562401   2.02200777]\n",
      "n_iter 775 loss 0.021040983828482573 weight [ 0.00703553  0.39339729 -0.16606413  1.25684632  2.0229238 ]\n",
      "n_iter 776 loss 0.02103042600898828 weight [ 0.00697173  0.3930267  -0.1663836   1.2574521   2.02383911]\n",
      "n_iter 777 loss 0.021019887402915386 weight [ 0.00690815  0.39265636 -0.16670279  1.25805744  2.0247537 ]\n",
      "n_iter 778 loss 0.021009367959965553 weight [ 0.00684479  0.39228625 -0.16702169  1.25866234  2.02566758]\n",
      "n_iter 779 loss 0.020998867630011794 weight [ 0.00678165  0.39191639 -0.16734031  1.25926681  2.02658075]\n",
      "n_iter 780 loss 0.020988386363097734 weight [ 0.00671872  0.39154677 -0.16765865  1.25987083  2.0274932 ]\n",
      "n_iter 781 loss 0.020977924109436965 weight [ 0.00665601  0.39117739 -0.16797671  1.26047443  2.02840494]\n",
      "n_iter 782 loss 0.020967480819412274 weight [ 0.00659352  0.39080824 -0.16829449  1.26107759  2.02931597]\n",
      "n_iter 783 loss 0.02095705644357497 weight [ 0.00653124  0.39043934 -0.16861198  1.26168031  2.03022629]\n",
      "n_iter 784 loss 0.020946650932644206 weight [ 0.00646917  0.39007068 -0.1689292   1.26228261  2.0311359 ]\n",
      "n_iter 785 loss 0.020936264237506264 weight [ 0.00640732  0.38970226 -0.16924613  1.26288447  2.0320448 ]\n",
      "n_iter 786 loss 0.02092589630921384 weight [ 0.00634568  0.38933408 -0.16956278  1.26348589  2.032953  ]\n",
      "n_iter 787 loss 0.02091554709898542 weight [ 0.00628426  0.38896613 -0.16987915  1.26408689  2.03386049]\n",
      "n_iter 788 loss 0.020905216558204542 weight [ 0.00622305  0.38859843 -0.17019525  1.26468746  2.03476728]\n",
      "n_iter 789 loss 0.020894904638419122 weight [ 0.00616204  0.38823097 -0.17051106  1.26528759  2.03567337]\n",
      "n_iter 790 loss 0.020884611291340813 weight [ 0.00610125  0.38786374 -0.17082659  1.2658873   2.03657875]\n",
      "n_iter 791 loss 0.020874336468844283 weight [ 0.00604067  0.38749675 -0.17114185  1.26648658  2.03748343]\n",
      "n_iter 792 loss 0.020864080122966568 weight [ 0.0059803   0.38713001 -0.17145682  1.26708543  2.03838742]\n",
      "n_iter 793 loss 0.020853842205906428 weight [ 0.00592013  0.3867635  -0.17177152  1.26768386  2.0392907 ]\n",
      "n_iter 794 loss 0.020843622670023616 weight [ 0.00586018  0.38639723 -0.17208594  1.26828186  2.04019329]\n",
      "n_iter 795 loss 0.020833421467838297 weight [ 0.00580043  0.3860312  -0.17240008  1.26887944  2.04109518]\n",
      "n_iter 796 loss 0.020823238552030318 weight [ 0.00574089  0.3856654  -0.17271395  1.26947659  2.04199638]\n",
      "n_iter 797 loss 0.020813073875438633 weight [ 0.00568155  0.38529985 -0.17302754  1.27007331  2.04289689]\n",
      "n_iter 798 loss 0.020802927391060533 weight [ 0.00562242  0.38493453 -0.17334085  1.27066962  2.0437967 ]\n",
      "n_iter 799 loss 0.02079279905205115 weight [ 0.0055635   0.38456945 -0.17365388  1.2712655   2.04469582]\n",
      "n_iter 800 loss 0.02078268881172267 weight [ 0.00550478  0.3842046  -0.17396664  1.27186096  2.04559424]\n",
      "n_iter 801 loss 0.020772596623543773 weight [ 0.00544626  0.38384    -0.17427913  1.272456    2.04649198]\n",
      "n_iter 802 loss 0.020762522441138978 weight [ 0.00538794  0.38347563 -0.17459133  1.27305062  2.04738903]\n",
      "n_iter 803 loss 0.02075246621828799 weight [ 0.00532983  0.3831115  -0.17490327  1.27364482  2.0482854 ]\n",
      "n_iter 804 loss 0.02074242790892508 weight [ 0.00527192  0.3827476  -0.17521493  1.2742386   2.04918108]\n",
      "n_iter 805 loss 0.020732407467138458 weight [ 0.00521421  0.38238395 -0.17552631  1.27483196  2.05007607]\n",
      "n_iter 806 loss 0.020722404847169636 weight [ 0.0051567   0.38202053 -0.17583742  1.2754249   2.05097038]\n",
      "n_iter 807 loss 0.020712420003412804 weight [ 0.00509939  0.38165734 -0.17614825  1.27601743  2.051864  ]\n",
      "n_iter 808 loss 0.020702452890414225 weight [ 0.00504228  0.38129439 -0.17645882  1.27660954  2.05275694]\n",
      "n_iter 809 loss 0.020692503462871593 weight [ 0.00498537  0.38093168 -0.17676911  1.27720124  2.05364921]\n",
      "n_iter 810 loss 0.020682571675633435 weight [ 0.00492866  0.3805692  -0.17707912  1.27779252  2.05454079]\n",
      "n_iter 811 loss 0.020672657483698505 weight [ 0.00487214  0.38020696 -0.17738886  1.27838339  2.05543169]\n",
      "n_iter 812 loss 0.02066276084221514 weight [ 0.00481583  0.37984496 -0.17769834  1.27897385  2.05632192]\n",
      "n_iter 813 loss 0.020652881706480695 weight [ 0.0047597   0.37948319 -0.17800754  1.27956389  2.05721147]\n",
      "n_iter 814 loss 0.02064302003194091 weight [ 0.00470378  0.37912166 -0.17831646  1.28015352  2.05810034]\n",
      "n_iter 815 loss 0.02063317577418932 weight [ 0.00464804  0.37876036 -0.17862512  1.28074274  2.05898854]\n",
      "n_iter 816 loss 0.020623348888966688 weight [ 0.00459251  0.37839929 -0.1789335   1.28133155  2.05987607]\n",
      "n_iter 817 loss 0.020613539332160334 weight [ 0.00453716  0.37803846 -0.17924162  1.28191995  2.06076292]\n",
      "n_iter 818 loss 0.020603747059803612 weight [ 0.00448201  0.37767787 -0.17954946  1.28250794  2.0616491 ]\n",
      "n_iter 819 loss 0.020593972028075298 weight [ 0.00442706  0.37731751 -0.17985704  1.28309552  2.06253462]\n",
      "n_iter 820 loss 0.020584214193298988 weight [ 0.00437229  0.37695739 -0.18016434  1.28368269  2.06341946]\n",
      "n_iter 821 loss 0.020574473511942564 weight [ 0.00431772  0.37659749 -0.18047138  1.28426946  2.06430363]\n",
      "n_iter 822 loss 0.020564749940617535 weight [ 0.00426333  0.37623784 -0.18077814  1.28485582  2.06518714]\n",
      "n_iter 823 loss 0.020555043436078523 weight [ 0.00420914  0.37587842 -0.18108464  1.28544178  2.06606998]\n",
      "n_iter 824 loss 0.020545353955222664 weight [ 0.00415514  0.37551923 -0.18139087  1.28602732  2.06695216]\n",
      "n_iter 825 loss 0.020535681455089033 weight [ 0.00410132  0.37516027 -0.18169683  1.28661247  2.06783367]\n",
      "n_iter 826 loss 0.020526025892858086 weight [ 0.0040477   0.37480155 -0.18200252  1.28719721  2.06871452]\n",
      "n_iter 827 loss 0.020516387225851078 weight [ 0.00399426  0.37444306 -0.18230794  1.28778155  2.06959471]\n",
      "n_iter 828 loss 0.020506765411529494 weight [ 0.00394101  0.3740848  -0.1826131   1.28836549  2.07047423]\n",
      "n_iter 829 loss 0.02049716040749453 weight [ 0.00388795  0.37372678 -0.18291799  1.28894902  2.0713531 ]\n",
      "n_iter 830 loss 0.02048757217148646 weight [ 0.00383507  0.37336899 -0.18322262  1.28953215  2.07223131]\n",
      "n_iter 831 loss 0.020478000661384145 weight [ 0.00378238  0.37301144 -0.18352697  1.29011488  2.07310886]\n",
      "n_iter 832 loss 0.02046844583520445 weight [ 0.00372987  0.37265411 -0.18383106  1.29069722  2.07398575]\n",
      "n_iter 833 loss 0.020458907651101692 weight [ 0.00367755  0.37229702 -0.18413489  1.29127915  2.07486199]\n",
      "n_iter 834 loss 0.0204493860673671 weight [ 0.00362542  0.37194016 -0.18443845  1.29186069  2.07573757]\n",
      "n_iter 835 loss 0.020439881042428266 weight [ 0.00357346  0.37158353 -0.18474174  1.29244182  2.0766125 ]\n",
      "n_iter 836 loss 0.020430392534848574 weight [ 0.00352169  0.37122713 -0.18504477  1.29302256  2.07748677]\n",
      "n_iter 837 loss 0.020420920503326716 weight [ 0.0034701   0.37087097 -0.18534754  1.29360291  2.0783604 ]\n",
      "n_iter 838 loss 0.020411464906696104 weight [ 0.0034187   0.37051504 -0.18565004  1.29418285  2.07923337]\n",
      "n_iter 839 loss 0.020402025703924364 weight [ 0.00336747  0.37015934 -0.18595228  1.29476241  2.08010569]\n",
      "n_iter 840 loss 0.02039260285411276 weight [ 0.00331643  0.36980387 -0.18625425  1.29534156  2.08097737]\n",
      "n_iter 841 loss 0.02038319631649571 weight [ 0.00326556  0.36944863 -0.18655596  1.29592033  2.08184839]\n",
      "n_iter 842 loss 0.020373806050440244 weight [ 0.00321488  0.36909362 -0.18685741  1.2964987   2.08271877]\n",
      "n_iter 843 loss 0.020364432015445456 weight [ 0.00316437  0.36873884 -0.18715859  1.29707667  2.0835885 ]\n",
      "n_iter 844 loss 0.020355074171142 weight [ 0.00311405  0.36838429 -0.18745951  1.29765426  2.08445759]\n",
      "n_iter 845 loss 0.020345732477291573 weight [ 0.0030639   0.36802998 -0.18776017  1.29823145  2.08532604]\n",
      "n_iter 846 loss 0.020336406893786383 weight [ 0.00301393  0.36767589 -0.18806057  1.29880826  2.08619384]\n",
      "n_iter 847 loss 0.02032709738064862 weight [ 0.00296413  0.36732204 -0.18836071  1.29938467  2.087061  ]\n",
      "n_iter 848 loss 0.02031780389802998 weight [ 0.00291451  0.36696841 -0.18866058  1.29996069  2.08792752]\n",
      "n_iter 849 loss 0.02030852640621112 weight [ 0.00286507  0.36661501 -0.1889602   1.30053633  2.0887934 ]\n",
      "n_iter 850 loss 0.020299264865601165 weight [ 0.0028158   0.36626185 -0.18925955  1.30111157  2.08965864]\n",
      "n_iter 851 loss 0.02029001923673718 weight [ 0.00276671  0.36590891 -0.18955864  1.30168643  2.09052324]\n",
      "n_iter 852 loss 0.02028078948028371 weight [ 0.00271779  0.3655562  -0.18985748  1.3022609   2.0913872 ]\n",
      "n_iter 853 loss 0.020271575557032216 weight [ 0.00266905  0.36520372 -0.19015605  1.30283499  2.09225053]\n",
      "n_iter 854 loss 0.020262377427900626 weight [ 0.00262048  0.36485147 -0.19045437  1.30340869  2.09311322]\n",
      "n_iter 855 loss 0.020253195053932815 weight [ 0.00257208  0.36449945 -0.19075242  1.303982    2.09397528]\n",
      "n_iter 856 loss 0.02024402839629811 weight [ 0.00252385  0.36414766 -0.19105022  1.30455493  2.09483671]\n",
      "n_iter 857 loss 0.020234877416290795 weight [ 0.0024758   0.3637961  -0.19134775  1.30512747  2.0956975 ]\n",
      "n_iter 858 loss 0.020225742075329633 weight [ 0.00242791  0.36344476 -0.19164503  1.30569963  2.09655766]\n",
      "n_iter 859 loss 0.020216622334957364 weight [ 0.0023802   0.36309365 -0.19194206  1.30627141  2.0974172 ]\n",
      "n_iter 860 loss 0.020207518156840215 weight [ 0.00233266  0.36274277 -0.19223882  1.30684281  2.0982761 ]\n",
      "n_iter 861 loss 0.02019842950276744 weight [ 0.00228528  0.36239212 -0.19253533  1.30741383  2.09913437]\n",
      "n_iter 862 loss 0.020189356334650813 weight [ 0.00223808  0.3620417  -0.19283158  1.30798446  2.09999202]\n",
      "n_iter 863 loss 0.02018029861452415 weight [ 0.00219104  0.3616915  -0.19312757  1.30855471  2.10084904]\n",
      "n_iter 864 loss 0.02017125630454285 weight [ 0.00214418  0.36134153 -0.19342331  1.30912459  2.10170543]\n",
      "n_iter 865 loss 0.020162229366983416 weight [ 2.09747639e-03  3.60991792e-01 -1.93718787e-01  1.30969408e+00\n",
      "  2.10256120e+00]\n",
      "n_iter 866 loss 0.020153217764242927 weight [ 2.05094305e-03  3.60642277e-01 -1.94014011e-01  1.31026320e+00\n",
      "  2.10341635e+00]\n",
      "n_iter 867 loss 0.020144221458838676 weight [ 2.00457620e-03  3.60292989e-01 -1.94308980e-01  1.31083194e+00\n",
      "  2.10427087e+00]\n",
      "n_iter 868 loss 0.020135240413407602 weight [ 1.95837535e-03  3.59943928e-01 -1.94603694e-01  1.31140030e+00\n",
      "  2.10512477e+00]\n",
      "n_iter 869 loss 0.02012627459070586 weight [ 1.91233999e-03  3.59595094e-01 -1.94898153e-01  1.31196829e+00\n",
      "  2.10597805e+00]\n",
      "n_iter 870 loss 0.020117323953608362 weight [ 1.86646962e-03  3.59246486e-01 -1.95192357e-01  1.31253589e+00\n",
      "  2.10683071e+00]\n",
      "n_iter 871 loss 0.020108388465108314 weight [ 1.82076373e-03  3.58898104e-01 -1.95486307e-01  1.31310313e+00\n",
      "  2.10768275e+00]\n",
      "n_iter 872 loss 0.02009946808831676 weight [ 1.77522184e-03  3.58549948e-01 -1.95780003e-01  1.31366999e+00\n",
      "  2.10853417e+00]\n",
      "n_iter 873 loss 0.02009056278646208 weight [ 1.72984345e-03  3.58202019e-01 -1.96073445e-01  1.31423647e+00\n",
      "  2.10938498e+00]\n",
      "n_iter 874 loss 0.020081672522889588 weight [ 1.68462806e-03  3.57854314e-01 -1.96366634e-01  1.31480258e+00\n",
      "  2.11023517e+00]\n",
      "n_iter 875 loss 0.02007279726106107 weight [ 1.63957518e-03  3.57506836e-01 -1.96659569e-01  1.31536832e+00\n",
      "  2.11108474e+00]\n",
      "n_iter 876 loss 0.0200639369645543 weight [ 1.59468432e-03  3.57159583e-01 -1.96952252e-01  1.31593368e+00\n",
      "  2.11193370e+00]\n",
      "n_iter 877 loss 0.020055091597062612 weight [ 1.54995500e-03  3.56812555e-01 -1.97244682e-01  1.31649867e+00\n",
      "  2.11278205e+00]\n",
      "n_iter 878 loss 0.020046261122394462 weight [ 1.50538671e-03  3.56465752e-01 -1.97536860e-01  1.31706329e+00\n",
      "  2.11362978e+00]\n",
      "n_iter 879 loss 0.020037445504472957 weight [ 1.46097898e-03  3.56119174e-01 -1.97828785e-01  1.31762754e+00\n",
      "  2.11447690e+00]\n",
      "n_iter 880 loss 0.020028644707335428 weight [ 1.41673131e-03  3.55772821e-01 -1.98120459e-01  1.31819142e+00\n",
      "  2.11532341e+00]\n",
      "n_iter 881 loss 0.020019858695132987 weight [ 1.37264324e-03  3.55426693e-01 -1.98411882e-01  1.31875493e+00\n",
      "  2.11616931e+00]\n",
      "n_iter 882 loss 0.02001108743213008 weight [ 1.32871427e-03  3.55080789e-01 -1.98703053e-01  1.31931807e+00\n",
      "  2.11701460e+00]\n",
      "n_iter 883 loss 0.02000233088270405 weight [ 1.28494393e-03  3.54735109e-01 -1.98993974e-01  1.31988085e+00\n",
      "  2.11785929e+00]\n",
      "n_iter 884 loss 0.019993589011344712 weight [ 1.24133173e-03  3.54389654e-01 -1.99284644e-01  1.32044325e+00\n",
      "  2.11870336e+00]\n",
      "n_iter 885 loss 0.019984861782653907 weight [ 1.19787720e-03  3.54044422e-01 -1.99575063e-01  1.32100529e+00\n",
      "  2.11954683e+00]\n",
      "n_iter 886 loss 0.019976149161345068 weight [ 1.15457986e-03  3.53699414e-01 -1.99865233e-01  1.32156696e+00\n",
      "  2.12038970e+00]\n",
      "n_iter 887 loss 0.01996745111224281 weight [ 1.11143924e-03  3.53354630e-01 -2.00155153e-01  1.32212826e+00\n",
      "  2.12123196e+00]\n",
      "n_iter 888 loss 0.019958767600282482 weight [ 1.06845487e-03  3.53010069e-01 -2.00444824e-01  1.32268920e+00\n",
      "  2.12207361e+00]\n",
      "n_iter 889 loss 0.019950098590509738 weight [ 1.02562627e-03  3.52665732e-01 -2.00734246e-01  1.32324977e+00\n",
      "  2.12291467e+00]\n",
      "n_iter 890 loss 0.019941444048080133 weight [ 9.82952979e-04  3.52321617e-01 -2.01023419e-01  1.32380998e+00\n",
      "  2.12375512e+00]\n",
      "n_iter 891 loss 0.019932803938258692 weight [ 9.40434522e-04  3.51977726e-01 -2.01312343e-01  1.32436983e+00\n",
      "  2.12459497e+00]\n",
      "n_iter 892 loss 0.01992417822641947 weight [ 8.98070434e-04  3.51634057e-01 -2.01601019e-01  1.32492931e+00\n",
      "  2.12543422e+00]\n",
      "n_iter 893 loss 0.019915566878045174 weight [ 8.55860249e-04  3.51290611e-01 -2.01889447e-01  1.32548843e+00\n",
      "  2.12627287e+00]\n",
      "n_iter 894 loss 0.0199069698587267 weight [ 8.13803505e-04  3.50947388e-01 -2.02177628e-01  1.32604718e+00\n",
      "  2.12711092e+00]\n",
      "n_iter 895 loss 0.01989838713416276 weight [ 7.71899738e-04  3.50604387e-01 -2.02465561e-01  1.32660558e+00\n",
      "  2.12794838e+00]\n",
      "n_iter 896 loss 0.01988981867015943 weight [ 7.30148487e-04  3.50261608e-01 -2.02753247e-01  1.32716361e+00\n",
      "  2.12878524e+00]\n",
      "n_iter 897 loss 0.019881264432629785 weight [ 6.88549293e-04  3.49919051e-01 -2.03040686e-01  1.32772128e+00\n",
      "  2.12962150e+00]\n",
      "n_iter 898 loss 0.019872724387593438 weight [ 6.47101697e-04  3.49576716e-01 -2.03327879e-01  1.32827860e+00\n",
      "  2.13045717e+00]\n",
      "n_iter 899 loss 0.019864198501176177 weight [ 6.05805243e-04  3.49234602e-01 -2.03614826e-01  1.32883555e+00\n",
      "  2.13129224e+00]\n",
      "n_iter 900 loss 0.019855686739609534 weight [ 5.64659475e-04  3.48892710e-01 -2.03901526e-01  1.32939214e+00\n",
      "  2.13212672e+00]\n",
      "n_iter 901 loss 0.019847189069230383 weight [ 5.23663940e-04  3.48551040e-01 -2.04187981e-01  1.32994838e+00\n",
      "  2.13296061e+00]\n",
      "n_iter 902 loss 0.019838705456480556 weight [ 4.82818185e-04  3.48209590e-01 -2.04474191e-01  1.33050426e+00\n",
      "  2.13379391e+00]\n",
      "n_iter 903 loss 0.019830235867906422 weight [ 4.42121759e-04  3.47868362e-01 -2.04760156e-01  1.33105978e+00\n",
      "  2.13462662e+00]\n",
      "n_iter 904 loss 0.019821780270158493 weight [ 4.01574213e-04  3.47527354e-01 -2.05045875e-01  1.33161494e+00\n",
      "  2.13545873e+00]\n",
      "n_iter 905 loss 0.019813338629991042 weight [ 3.61175098e-04  3.47186568e-01 -2.05331351e-01  1.33216975e+00\n",
      "  2.13629026e+00]\n",
      "n_iter 906 loss 0.019804910914261686 weight [ 3.20923967e-04  3.46846001e-01 -2.05616582e-01  1.33272420e+00\n",
      "  2.13712120e+00]\n",
      "n_iter 907 loss 0.019796497089931028 weight [ 2.80820375e-04  3.46505655e-01 -2.05901569e-01  1.33327830e+00\n",
      "  2.13795156e+00]\n",
      "n_iter 908 loss 0.0197880971240622 weight [ 2.40863879e-04  3.46165529e-01 -2.06186312e-01  1.33383204e+00\n",
      "  2.13878132e+00]\n",
      "n_iter 909 loss 0.019779710983820562 weight [ 2.01054034e-04  3.45825623e-01 -2.06470812e-01  1.33438543e+00\n",
      "  2.13961051e+00]\n",
      "n_iter 910 loss 0.019771338636473235 weight [ 1.61390401e-04  3.45485938e-01 -2.06755069e-01  1.33493846e+00\n",
      "  2.14043910e+00]\n",
      "n_iter 911 loss 0.01976298004938876 weight [ 1.21872538e-04  3.45146471e-01 -2.07039084e-01  1.33549114e+00\n",
      "  2.14126712e+00]\n",
      "n_iter 912 loss 0.019754635190036708 weight [ 8.25000089e-05  3.44807225e-01 -2.07322855e-01  1.33604347e+00\n",
      "  2.14209455e+00]\n",
      "n_iter 913 loss 0.019746304025987274 weight [ 4.32723749e-05  3.44468197e-01 -2.07606385e-01  1.33659545e+00\n",
      "  2.14292140e+00]\n",
      "n_iter 914 loss 0.019737986524910933 weight [ 4.18920071e-06  3.44129389e-01 -2.07889672e-01  1.33714708e+00\n",
      "  2.14374767e+00]\n",
      "n_iter 915 loss 0.019729682654578026 weight [-3.47499481e-05  3.43790800e-01 -2.08172718e-01  1.33769835e+00\n",
      "  2.14457335e+00]\n",
      "n_iter 916 loss 0.019721392382858395 weight [-7.35455046e-05  3.43452430e-01 -2.08455522e-01  1.33824928e+00\n",
      "  2.14539846e+00]\n",
      "n_iter 917 loss 0.019713115677721018 weight [-1.12197901e-04  3.43114278e-01 -2.08738086e-01  1.33879985e+00\n",
      "  2.14622299e+00]\n",
      "n_iter 918 loss 0.019704852507233635 weight [-1.50707566e-04  3.42776346e-01 -2.09020408e-01  1.33935008e+00\n",
      "  2.14704694e+00]\n",
      "n_iter 919 loss 0.01969660283956234 weight [-1.89074931e-04  3.42438631e-01 -2.09302490e-01  1.33989995e+00\n",
      "  2.14787032e+00]\n",
      "n_iter 920 loss 0.019688366642971265 weight [-2.27300422e-04  3.42101135e-01 -2.09584332e-01  1.34044948e+00\n",
      "  2.14869312e+00]\n",
      "n_iter 921 loss 0.019680143885822156 weight [-2.65384465e-04  3.41763857e-01 -2.09865933e-01  1.34099866e+00\n",
      "  2.14951534e+00]\n",
      "n_iter 922 loss 0.019671934536574048 weight [-3.03327487e-04  3.41426796e-01 -2.10147295e-01  1.34154750e+00\n",
      "  2.15033699e+00]\n",
      "n_iter 923 loss 0.019663738563782875 weight [-3.41129910e-04  3.41089954e-01 -2.10428418e-01  1.34209599e+00\n",
      "  2.15115806e+00]\n",
      "n_iter 924 loss 0.01965555593610112 weight [-3.78792157e-04  3.40753329e-01 -2.10709301e-01  1.34264413e+00\n",
      "  2.15197856e+00]\n",
      "n_iter 925 loss 0.019647386622277434 weight [-4.16314649e-04  3.40416921e-01 -2.10989946e-01  1.34319192e+00\n",
      "  2.15279849e+00]\n",
      "n_iter 926 loss 0.0196392305911563 weight [-4.53697806e-04  3.40080731e-01 -2.11270352e-01  1.34373937e+00\n",
      "  2.15361785e+00]\n",
      "n_iter 927 loss 0.01963108781167765 weight [-4.90942046e-04  3.39744757e-01 -2.11550519e-01  1.34428648e+00\n",
      "  2.15443664e+00]\n",
      "n_iter 928 loss 0.01962295825287651 weight [-5.28047787e-04  3.39409001e-01 -2.11830449e-01  1.34483324e+00\n",
      "  2.15525486e+00]\n",
      "n_iter 929 loss 0.01961484188388266 weight [-5.65015444e-04  3.39073461e-01 -2.12110140e-01  1.34537966e+00\n",
      "  2.15607251e+00]\n",
      "n_iter 930 loss 0.01960673867392028 weight [-6.01845432e-04  3.38738138e-01 -2.12389595e-01  1.34592574e+00\n",
      "  2.15688959e+00]\n",
      "n_iter 931 loss 0.019598648592307547 weight [-6.38538164e-04  3.38403032e-01 -2.12668812e-01  1.34647147e+00\n",
      "  2.15770611e+00]\n",
      "n_iter 932 loss 0.019590571608456373 weight [-6.75094052e-04  3.38068141e-01 -2.12947792e-01  1.34701686e+00\n",
      "  2.15852206e+00]\n",
      "n_iter 933 loss 0.019582507691871955 weight [-7.11513507e-04  3.37733467e-01 -2.13226535e-01  1.34756191e+00\n",
      "  2.15933744e+00]\n",
      "n_iter 934 loss 0.0195744568121525 weight [-7.47796939e-04  3.37399009e-01 -2.13505042e-01  1.34810662e+00\n",
      "  2.16015226e+00]\n",
      "n_iter 935 loss 0.019566418938988858 weight [-7.83944755e-04  3.37064766e-01 -2.13783313e-01  1.34865099e+00\n",
      "  2.16096652e+00]\n",
      "n_iter 936 loss 0.019558394042164143 weight [-8.19957363e-04  3.36730739e-01 -2.14061348e-01  1.34919502e+00\n",
      "  2.16178021e+00]\n",
      "n_iter 937 loss 0.019550382091553447 weight [-8.55835167e-04  3.36396928e-01 -2.14339148e-01  1.34973871e+00\n",
      "  2.16259334e+00]\n",
      "n_iter 938 loss 0.019542383057123452 weight [-8.91578574e-04  3.36063332e-01 -2.14616712e-01  1.35028206e+00\n",
      "  2.16340591e+00]\n",
      "n_iter 939 loss 0.019534396908932102 weight [-9.27187985e-04  3.35729951e-01 -2.14894042e-01  1.35082507e+00\n",
      "  2.16421791e+00]\n",
      "n_iter 940 loss 0.019526423617128277 weight [-9.62663803e-04  3.35396784e-01 -2.15171136e-01  1.35136775e+00\n",
      "  2.16502936e+00]\n",
      "n_iter 941 loss 0.01951846315195142 weight [-9.98006428e-04  3.35063833e-01 -2.15447996e-01  1.35191008e+00\n",
      "  2.16584025e+00]\n",
      "n_iter 942 loss 0.01951051548373124 weight [-1.03321626e-03  3.34731096e-01 -2.15724622e-01  1.35245208e+00\n",
      "  2.16665058e+00]\n",
      "n_iter 943 loss 0.019502580582887376 weight [-1.06829370e-03  3.34398574e-01 -2.16001014e-01  1.35299375e+00\n",
      "  2.16746035e+00]\n",
      "n_iter 944 loss 0.019494658419928992 weight [-1.10323914e-03  3.34066266e-01 -2.16277172e-01  1.35353507e+00\n",
      "  2.16826957e+00]\n",
      "n_iter 945 loss 0.01948674896545457 weight [-1.13805297e-03  3.33734172e-01 -2.16553097e-01  1.35407607e+00\n",
      "  2.16907823e+00]\n",
      "n_iter 946 loss 0.019478852190151455 weight [-1.17273560e-03  3.33402292e-01 -2.16828789e-01  1.35461672e+00\n",
      "  2.16988633e+00]\n",
      "n_iter 947 loss 0.0194709680647956 weight [-1.20728742e-03  3.33070626e-01 -2.17104248e-01  1.35515705e+00\n",
      "  2.17069389e+00]\n",
      "n_iter 948 loss 0.019463096560251223 weight [-1.24170881e-03  3.32739174e-01 -2.17379475e-01  1.35569704e+00\n",
      "  2.17150088e+00]\n",
      "n_iter 949 loss 0.01945523764747046 weight [-1.27600017e-03  3.32407935e-01 -2.17654469e-01  1.35623669e+00\n",
      "  2.17230733e+00]\n",
      "n_iter 950 loss 0.01944739129749305 weight [-1.31016189e-03  3.32076909e-01 -2.17929231e-01  1.35677601e+00\n",
      "  2.17311322e+00]\n",
      "n_iter 951 loss 0.01943955748144605 weight [-1.34419436e-03  3.31746097e-01 -2.18203761e-01  1.35731500e+00\n",
      "  2.17391856e+00]\n",
      "n_iter 952 loss 0.019431736170543442 weight [-1.37809796e-03  3.31415497e-01 -2.18478060e-01  1.35785366e+00\n",
      "  2.17472335e+00]\n",
      "n_iter 953 loss 0.019423927336085854 weight [-1.41187308e-03  3.31085110e-01 -2.18752128e-01  1.35839199e+00\n",
      "  2.17552759e+00]\n",
      "n_iter 954 loss 0.019416130949460257 weight [-1.44552010e-03  3.30754936e-01 -2.19025965e-01  1.35892998e+00\n",
      "  2.17633128e+00]\n",
      "n_iter 955 loss 0.019408346982139595 weight [-1.47903942e-03  3.30424975e-01 -2.19299571e-01  1.35946765e+00\n",
      "  2.17713443e+00]\n",
      "n_iter 956 loss 0.019400575405682522 weight [-1.51243140e-03  3.30095226e-01 -2.19572946e-01  1.36000499e+00\n",
      "  2.17793702e+00]\n",
      "n_iter 957 loss 0.01939281619173305 weight [-1.54569644e-03  3.29765689e-01 -2.19846092e-01  1.36054199e+00\n",
      "  2.17873907e+00]\n",
      "n_iter 958 loss 0.01938506931202024 weight [-1.57883491e-03  3.29436364e-01 -2.20119008e-01  1.36107867e+00\n",
      "  2.17954058e+00]\n",
      "n_iter 959 loss 0.01937733473835791 weight [-1.61184720e-03  3.29107250e-01 -2.20391694e-01  1.36161502e+00\n",
      "  2.18034153e+00]\n",
      "n_iter 960 loss 0.019369612442644308 weight [-1.64473367e-03  3.28778349e-01 -2.20664150e-01  1.36215104e+00\n",
      "  2.18114195e+00]\n",
      "n_iter 961 loss 0.019361902396861794 weight [-1.67749471e-03  3.28449659e-01 -2.20936378e-01  1.36268673e+00\n",
      "  2.18194182e+00]\n",
      "n_iter 962 loss 0.019354204573076555 weight [-1.71013069e-03  3.28121180e-01 -2.21208377e-01  1.36322209e+00\n",
      "  2.18274115e+00]\n",
      "n_iter 963 loss 0.01934651894343826 weight [-1.74264199e-03  3.27792913e-01 -2.21480148e-01  1.36375713e+00\n",
      "  2.18353993e+00]\n",
      "n_iter 964 loss 0.019338845480179803 weight [-1.77502898e-03  3.27464857e-01 -2.21751690e-01  1.36429184e+00\n",
      "  2.18433818e+00]\n",
      "n_iter 965 loss 0.019331184155616975 weight [-1.80729203e-03  3.27137011e-01 -2.22023004e-01  1.36482623e+00\n",
      "  2.18513588e+00]\n",
      "n_iter 966 loss 0.01932353494214813 weight [-1.83943151e-03  3.26809376e-01 -2.22294090e-01  1.36536029e+00\n",
      "  2.18593305e+00]\n",
      "n_iter 967 loss 0.01931589781225393 weight [-1.87144779e-03  3.26481952e-01 -2.22564949e-01  1.36589403e+00\n",
      "  2.18672967e+00]\n",
      "n_iter 968 loss 0.019308272738497025 weight [-1.90334125e-03  3.26154738e-01 -2.22835581e-01  1.36642744e+00\n",
      "  2.18752576e+00]\n",
      "n_iter 969 loss 0.019300659693521748 weight [-1.93511225e-03  3.25827734e-01 -2.23105986e-01  1.36696053e+00\n",
      "  2.18832131e+00]\n",
      "n_iter 970 loss 0.019293058650053832 weight [-1.96676115e-03  3.25500941e-01 -2.23376164e-01  1.36749330e+00\n",
      "  2.18911632e+00]\n",
      "n_iter 971 loss 0.019285469580900088 weight [-1.99828832e-03  3.25174357e-01 -2.23646115e-01  1.36802574e+00\n",
      "  2.18991080e+00]\n",
      "n_iter 972 loss 0.019277892458948138 weight [-2.02969412e-03  3.24847983e-01 -2.23915841e-01  1.36855786e+00\n",
      "  2.19070474e+00]\n",
      "n_iter 973 loss 0.019270327257166105 weight [-2.06097893e-03  3.24521818e-01 -2.24185341e-01  1.36908966e+00\n",
      "  2.19149815e+00]\n",
      "n_iter 974 loss 0.019262773948602322 weight [-2.09214309e-03  3.24195863e-01 -2.24454615e-01  1.36962113e+00\n",
      "  2.19229102e+00]\n",
      "n_iter 975 loss 0.019255232506385028 weight [-2.12318698e-03  3.23870117e-01 -2.24723664e-01  1.37015229e+00\n",
      "  2.19308336e+00]\n",
      "n_iter 976 loss 0.01924770290372211 weight [-2.15411094e-03  3.23544580e-01 -2.24992487e-01  1.37068312e+00\n",
      "  2.19387517e+00]\n",
      "n_iter 977 loss 0.019240185113900774 weight [-2.18491534e-03  3.23219253e-01 -2.25261086e-01  1.37121364e+00\n",
      "  2.19466644e+00]\n",
      "n_iter 978 loss 0.019232679110287282 weight [-0.0022156   0.32289413 -0.22552946  1.37174383  2.19545719]\n",
      "n_iter 979 loss 0.01922518486632666 weight [-0.00224617  0.32256922 -0.22579761  1.37227371  2.1962474 ]\n",
      "n_iter 980 loss 0.01921770235554242 weight [-0.00227661  0.32224452 -0.22606554  1.37280327  2.19703709]\n",
      "n_iter 981 loss 0.019210231551536245 weight [-0.00230694  0.32192003 -0.22633324  1.3733325   2.19782624]\n",
      "n_iter 982 loss 0.019202772427987746 weight [-0.00233716  0.32159574 -0.22660072  1.37386143  2.19861487]\n",
      "n_iter 983 loss 0.019195324958654152 weight [-0.00236725  0.32127166 -0.22686797  1.37439003  2.19940297]\n",
      "n_iter 984 loss 0.01918788911737004 weight [-0.00239723  0.32094779 -0.227135    1.37491832  2.20019055]\n",
      "n_iter 985 loss 0.01918046487804705 weight [-0.00242709  0.32062413 -0.22740181  1.37544629  2.2009776 ]\n",
      "n_iter 986 loss 0.0191730522146736 weight [-0.00245683  0.32030068 -0.2276684   1.37597394  2.20176412]\n",
      "n_iter 987 loss 0.019165651101314643 weight [-0.00248646  0.31997743 -0.22793477  1.37650128  2.20255012]\n",
      "n_iter 988 loss 0.019158261512111353 weight [-0.00251597  0.31965439 -0.22820091  1.3770283   2.2033356 ]\n",
      "n_iter 989 loss 0.019150883421280848 weight [-0.00254537  0.31933155 -0.22846683  1.37755501  2.20412055]\n",
      "n_iter 990 loss 0.019143516803115948 weight [-0.00257465  0.31900893 -0.22873253  1.37808141  2.20490498]\n",
      "n_iter 991 loss 0.01913616163198489 weight [-0.00260382  0.31868651 -0.22899801  1.37860749  2.20568889]\n",
      "n_iter 992 loss 0.01912881788233103 weight [-0.00263287  0.31836429 -0.22926327  1.37913326  2.20647228]\n",
      "n_iter 993 loss 0.019121485528672613 weight [-0.00266181  0.31804229 -0.2295283   1.37965871  2.20725515]\n",
      "n_iter 994 loss 0.01911416454560248 weight [-0.00269064  0.31772048 -0.22979312  1.38018385  2.2080375 ]\n",
      "n_iter 995 loss 0.019106854907787792 weight [-0.00271935  0.31739889 -0.23005772  1.38070869  2.20881933]\n",
      "n_iter 996 loss 0.019099556589969792 weight [-0.00274795  0.3170775  -0.23032209  1.3812332   2.20960064]\n",
      "n_iter 997 loss 0.019092269566963513 weight [-0.00277644  0.31675632 -0.23058625  1.38175741  2.21038144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter 998 loss 0.01908499381365753 weight [-0.00280481  0.31643534 -0.23085019  1.38228131  2.21116171]\n",
      "n_iter 999 loss 0.019077729305013683 weight [-0.00283308  0.31611456 -0.23111391  1.3828049   2.21194148]\n"
     ]
    }
   ],
   "source": [
    "my_model.fit(X_train_trans,y_train,X_test_trans,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "alien-agreement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97611934, 0.08549018, 0.96358167, 0.99961665, 0.13940097,\n",
       "       0.1665466 , 0.89416622, 0.07802092, 0.60239436, 0.97229185,\n",
       "       0.01391255, 0.09255208, 0.00705656, 0.00812952, 0.91063247,\n",
       "       0.86154673, 0.99144502, 0.62420777, 0.01796802, 0.04543471,\n",
       "       0.01888624, 0.4689972 , 0.01124312, 0.99148394, 0.02558824])"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob = my_model.predict_proba(X_test_trans)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "lasting-symposium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_label = my_model.predict(X_test_trans)\n",
    "y_pred_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-spirituality",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "dedicated-dublin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {}\".format(accuracy_score(y_pred_label,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-alias",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "straight-disease",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.8831168831168831\n"
     ]
    }
   ],
   "source": [
    "print(\"Recall: {}\".format(recall_score(y_test,y_pred_label,average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-welsh",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "ceramic-crown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisionn: 0.8782051282051282\n"
     ]
    }
   ],
   "source": [
    "print(\"Precisionn: {}\".format(precision_score(y_test,y_pred_label,average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-single",
   "metadata": {},
   "source": [
    "### Compare with sklearn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "hollow-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "filled-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train_trans, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "neither-latest",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_pred = clf.predict(X_test_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-terror",
   "metadata": {},
   "source": [
    "### Sklearn metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "parliamentary-glossary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n",
      "Recall: 0.9545454545454546\n",
      "Precisionn: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {}\".format(accuracy_score(y_test,sklearn_pred)))\n",
    "print(\"Recall: {}\".format(recall_score(y_test,sklearn_pred,average='macro')))\n",
    "print(\"Precisionn: {}\".format(precision_score(y_test,sklearn_pred,average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-booking",
   "metadata": {},
   "source": [
    "#### We got different result becayse of different hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-allergy",
   "metadata": {},
   "source": [
    "## Problem 6: Plot a learning curve\n",
    "Look at the learning curve to see if the losses are falling properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "three-estonia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(16,9))\n",
    "plt.plot(np.arange(start=1, stop=1001, step=1),np.array(my_model.loss),label='Loss')\n",
    "plt.plot(np.arange(start=1, stop=1001, step=1),np.array(my_model.val_loss),label='Evaluation Loss',)\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-plasma",
   "metadata": {},
   "source": [
    "## Problem 7: Visualization of decision area\n",
    "Visualise the decision area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "primary-condition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "def decision_region(X, y, model, step=0.01, title='decision region', xlabel='xlabel', ylabel='ylabel', target_names=['versicolor', 'virginica']):\n",
    "    \"\"\"\n",
    "    Draw the determination area of the model that learned binary classification with two-dimensional features.\n",
    "    The background color is drawn from the estimated values of the trained model.\n",
    "    The points on the scatter plot are training or validation data.\n",
    "    Parameters\n",
    "    ----------------\n",
    "    X : ndarray, shape(n_samples, 2)\n",
    "        Feature value\n",
    "    y : ndarray, shape(n_samples,)\n",
    "        label\n",
    "    model : object\n",
    "        Insert the installed model of the learned model\n",
    "    step : float, (default : 0.1)\n",
    "        Set the interval to calculate the estimate\n",
    "    title : str\n",
    "        Give the text of the graph Title\n",
    "    xlabel, ylabel : str\n",
    "        Give the text of the axis label\n",
    "    target_names= : list of str\n",
    "        Give a list of legends\n",
    "    \"\"\"\n",
    "    # setting\n",
    "    scatter_color = ['red', 'blue']\n",
    "    contourf_color = ['pink', 'skyblue']\n",
    "    n_class = 2\n",
    "    # pred\n",
    "    mesh_f0, mesh_f1  = np.meshgrid(np.arange(np.min(X[:,0])-0.5, np.max(X[:,0])+0.5, step), np.arange(np.min(X[:,1])-0.5, np.max(X[:,1])+0.5, step))\n",
    "    mesh = np.c_[np.ravel(mesh_f0),np.ravel(mesh_f1)]\n",
    "    y_pred = model.predict(mesh).reshape(mesh_f0.shape)\n",
    "    # plot\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.contourf(mesh_f0, mesh_f1, y_pred, n_class-1, cmap=ListedColormap(contourf_color))\n",
    "    plt.contour(mesh_f0, mesh_f1, y_pred, n_class-1, colors='y', linewidths=3, alpha=0.5)\n",
    "    for i, target in enumerate(set(y)):\n",
    "        plt.scatter(X[y==target][:, 0], X[y==target][:, 1], s=80, color=scatter_color[i], label=target_names[i], marker='o')\n",
    "    patches = [mpatches.Patch(color=scatter_color[i], label=target_names[i]) for i in range(n_class)]\n",
    "    plt.legend(handles=patches)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-reservoir",
   "metadata": {},
   "source": [
    "#### I will plot the decision boundary using logistic regression sklearn with 2 attributes for easier implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "unique-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_X = df.loc[:,[\"sl\",\"pl\"]]\n",
    "my_Y = df.loc[:,\"species\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split( np.array(my_X), np.array(my_Y), test_size=0.25, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_trans = scaler.fit_transform(X_train)\n",
    "X_test_trans = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "suspected-distributor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr = LogisticRegression(random_state=0).fit(X_train_trans, y_train)\n",
    "y_predict = lgr.predict(X_test_trans)\n",
    "decision_region(X_train_trans,y_train,lgr,title=\"Logistic Regression\")\n",
    "decision_region(X_test_trans,y_predict,lgr,title=\"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-preserve",
   "metadata": {},
   "source": [
    "## Problem 8: Saving weights\n",
    "Let's save and load the learned weights for easy verification. Use the pickle module and NumPy's np.savez.\n",
    "\n",
    "[pickle — Serialization of Python objects — Python 3.7.4 documentation](https://docs.python.org/3/library/pickle.html)\n",
    "\n",
    "[numpy.savez — NumPy v1.17 Manual](https://numpy.org/doc/stable/reference/generated/numpy.savez.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-convert",
   "metadata": {},
   "source": [
    "### Use Pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "differential-looking",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(str(\"picke_weights\") +'.pickle', 'wb') as handle:\n",
    "    pickle.dump(my_model.getWeight(), handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "terminal-bubble",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.36898805,  1.36813324,  1.20688353, -0.46020166, -0.57843111])"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(str(\"picke_weights\") +'.pickle', 'rb') as handle:\n",
    "    w = pickle.load(handle)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-assist",
   "metadata": {},
   "source": [
    "### Use Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "foster-appeal",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"numpy_weights\",my_weights=my_model.getWeight())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "single-situation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.36898805  1.36813324  1.20688353 -0.46020166 -0.57843111]\n"
     ]
    }
   ],
   "source": [
    "w = np.load(\"numpy_weights\"+\".npz\")\n",
    "print(w[\"my_weights\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

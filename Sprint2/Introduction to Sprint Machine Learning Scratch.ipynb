{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "noticed-basement",
   "metadata": {},
   "source": [
    "# Introduction to Sprint Machine Learning Scratch\n",
    "\n",
    "# 1.About the Sprint\n",
    "## The purpose of this Sprint\n",
    "<li> Prepare for machine learning scratch </li>\n",
    "\n",
    "## How to learn\n",
    "<li> First, use scikit-learn to implement a machine learning program that will be implemented by scratch in future learning.</li>\n",
    "\n",
    "# 2. What is scratch?\n",
    "\n",
    "By combining the basic libraries provided in NumPy etc., you can create your own classes / functions equivalent to the functions implemented in the applied libraries such as scikit-learn. This is called scratch.​\n",
    "\n",
    "\n",
    "Through scratching, it is difficult to grasp just by moving a library such as scikit-learn, and we aim for a deep understanding of the algorithm. It also improves your coding skills, but that's not the main purpose.\n",
    "\n",
    "\n",
    "We are aiming for the following effects.\n",
    "\n",
    "\n",
    "<li>Make it easier to understand theory and mathematical formulas when encountering new methods\n",
    "<li>Reduce ambiguity in using libraries\n",
    "<li>Make existing implementations easier to read\n",
    "\n",
    "This time, first, we will implement the machine learning program using scikit-learn without completely scratching it. Then, from the next time, we will gradually shift the implementation using scikit-learn to scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-dakota",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "frank-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-suite",
   "metadata": {},
   "source": [
    "We will implement the code using scikit-learn.\n",
    "\n",
    "\n",
    "Use the self-made function created in question 1 to divide the verification data. The holdout method may be used instead of cross Validation.\n",
    "\n",
    "<b> Classification problem </b>\n",
    "\n",
    "Classification scratches three methods.\n",
    "\n",
    "\n",
    "<li>Logistic regression</li>\n",
    "<li>SVM</li>\n",
    "<li>Decision tree</li>\n",
    "\n",
    "It can be used in scikit-learn from two types, LogisticRegression class and SGDClassifier class. Here, use the SGDClassifier class that calculates using the gradient descent method. Logistic regression can be calculated by setting loss = ”log” as an argument.\n",
    "\n",
    "The scikit-learn as a classifier that can be used in logistic regression LogisticRegression class and SGDClassifier have classes are available. SGDClassifier class, which is calculated using the gradient descent method. Logistic regression can be calculated by specifying loss=\"log\" as an argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-phone",
   "metadata": {},
   "source": [
    "## Problem 1: Scratch of train_test_split\n",
    "First, try scratching train_test_split of scikit-learn. Please implement the function based on the following template.\n",
    "\n",
    "\n",
    "sklearn.model_selection.train_test_split - scikit-learn stable version documentation\n",
    "\n",
    "\n",
    "Be sure to check if the created function train_test_split​\n",
    "```\n",
    "def scratch_train_test_split(X, y, train_size=0.8):\n",
    "    \"\"\"Divide the validation data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray\n",
    "      Training data (n_samples, n_features)\n",
    "    y : ndarray\n",
    "      Correct answer value (n_samples,)\n",
    "    train_size : float\n",
    "      Specify what percentage to use as a train (0 < train_size < 1)\n",
    "    Returns\n",
    "    -------\n",
    "    X_train : ndarray\n",
    "      Training data (n_samples, n_features)\n",
    "    X_test : ndarray\n",
    "      Validation data (n_samples, n_features)\n",
    "    y_train : ndarray\n",
    "      Correct answer value of training data (n_samples,)\n",
    "    y_test : ndarray\n",
    "      Correct value of verification data (n_samples,)\n",
    "    \"\"\"\n",
    "    # Write code here\n",
    "    pass\n",
    "    return X_train, X_test, y_train, y_test\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "meaningful-bradford",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(load_iris().data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "important-argument",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns =['sepal_length', 'sepal_width', 'petal_length', 'petal_width']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "competitive-visiting",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width\n",
       "0           5.1          3.5           1.4          0.2\n",
       "1           4.9          3.0           1.4          0.2\n",
       "2           4.7          3.2           1.3          0.2\n",
       "3           4.6          3.1           1.5          0.2\n",
       "4           5.0          3.6           1.4          0.2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "spiritual-psychiatry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "democratic-authentication",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Species\n",
       "0          0\n",
       "1          0\n",
       "2          0\n",
       "3          0\n",
       "4          0\n",
       "..       ...\n",
       "145        2\n",
       "146        2\n",
       "147        2\n",
       "148        2\n",
       "149        2\n",
       "\n",
       "[150 rows x 1 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = pd.DataFrame(load_iris().target)\n",
    "Y.columns = ['Species']\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "harmful-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scratch_train_test_split(X, y, train_size=0.8):\n",
    "    \"\"\"Divide the validation data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray\n",
    "      Training data (n_samples, n_features)\n",
    "    y : ndarray\n",
    "      Correct answer value (n_samples,)\n",
    "    train_size : float\n",
    "      Specify what percentage to use as a train (0 < train_size < 1)\n",
    "    Returns\n",
    "    -------\n",
    "    X_train : ndarray\n",
    "      Training data (n_samples, n_features)\n",
    "    X_test : ndarray\n",
    "      Validation data (n_samples, n_features)\n",
    "    y_train : ndarray\n",
    "      Correct answer value of training data (n_samples,)\n",
    "    y_test : ndarray\n",
    "      Correct value of verification data (n_samples,)\n",
    "    \"\"\"\n",
    "    # Write code here\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-inquiry",
   "metadata": {},
   "source": [
    "### Since we want to make a binary classification, we will use only the following two objective variables. All four types of features are used.\n",
    "\n",
    "\n",
    "virgicolor and virginica\n",
    "\n",
    "The other two are artificial datasets with two feature values. With the following code we can create the explanatory variableXand the objective variable y. Let's call them \"Simple Data Set 1\" and \"Simple Data Set 2\". As there are only two features, visualization is easy.\n",
    "<br><br>\n",
    "\n",
    "<b> \n",
    "Simple data set 1 creation code </b>\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "np.random.seed(seed=0)\n",
    "n_samples = 500\n",
    "f0 = [-1, 2]\n",
    "f1 = [2, -1]\n",
    "cov = [[1.0,0.8], [0.8, 1.0]]\n",
    "f0 = np.random.multivariate_normal(f0, cov, n_samples // 2)\n",
    "f1 = np.random.multivariate_normal(f1, cov, n_samples // 2)\n",
    "X = np.concatenate([f0, f1])\n",
    "y = np.concatenate([\n",
    "    np.full(n_samples // 2, 1),\n",
    "    np.full(n_samples // 2, -1)\n",
    "])\n",
    "```\n",
    "\n",
    "<b> \n",
    "Simple data set 2 creation code </b>\n",
    "\n",
    "X = np.array([\n",
    "    <br>[-0.44699 , -2.8073  ],[-1.4621  , -2.4586  ],\n",
    "    <br>[ 0.10645 ,  1.9242  ],[-3.5944  , -4.0112  ],\n",
    "    <br>[-0.9888  ,  4.5718  ],[-3.1625  , -3.9606  ],\n",
    "    <br>[ 0.56421 ,  0.72888 ],[-0.60216 ,  8.4636  ],\n",
    "    <br>[-0.61251 , -0.75345 ],[-0.73535 , -2.2718  ],\n",
    "    <br>[-0.80647 , -2.2135  ],[ 0.86291 ,  2.3946  ],\n",
    "    <br>[-3.1108  ,  0.15394 ],[-2.9362  ,  2.5462  ],\n",
    "    <br>[-0.57242 , -2.9915  ],[ 1.4771  ,  3.4896  ],\n",
    "    <br>[ 0.58619 ,  0.37158 ],[ 0.6017  ,  4.3439  ],\n",
    "    <br>[-2.1086  ,  8.3428  ],[-4.1013  , -4.353   ],\n",
    "    <br>[-1.9948  , -1.3927  ],[ 0.35084 , -0.031994],\n",
    "    <br>[ 0.96765 ,  7.8929  ],[-1.281   , 15.6824  ],\n",
    "    <br>[ 0.96765 , 10.083   ],[ 1.3763  ,  1.3347  ],\n",
    "    <br>[-2.234   , -2.5323  ],[-2.9452  , -1.8219  ],\n",
    "    <br>[ 0.14654 , -0.28733 ],[ 0.5461  ,  5.8245  ],\n",
    "    <br>[-0.65259 ,  9.3444  ],[ 0.59912 ,  5.3524  ],\n",
    "    <br>[ 0.50214 , -0.31818 ],[-3.0603  , -3.6461  ],\n",
    "    <br>[-6.6797  ,  0.67661 ],[-2.353   , -0.72261 ],\n",
    "    <br>[ 1.1319  ,  2.4023  ],[-0.12243 ,  9.0162  ],\n",
    "    <br>[-2.5677  , 13.1779  ],[ 0.057313,  5.4681  ],\n",
    "])\n",
    "<br>\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
    "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-airplane",
   "metadata": {},
   "source": [
    "## Problem 2: Creating a code to solve the classification problem\n",
    "\n",
    "## Regression problem\n",
    "Regression then scratches one type.\n",
    "\n",
    "<li> Linear regression </li>\n",
    "\n",
    "For linear regression, use SGDRegressor class, which is calculated using gradient descent.\n",
    "\n",
    "\n",
    "sklearn.linear_model.SGDRegressor - scikit-lear stable version documentation\n",
    "\n",
    "\n",
    "The data set is from the House Prices competition as in the pre-study period.\n",
    "\n",
    "\n",
    "House Prices: Advanced Regression Techniques\n",
    "\n",
    "\n",
    "Download train.csv and use SalePriceas the objective variable andGrLivAreaand YearBuiltas the explanatory variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-finish",
   "metadata": {},
   "source": [
    "### Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "disturbed-astronomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = scratch_train_test_split(np.array(X),np.array(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-connection",
   "metadata": {},
   "source": [
    "### Dataset 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "clinical-activation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(seed=0)\n",
    "n_samples = 500\n",
    "f0 = [-1, 2]\n",
    "f1 = [2, -1]\n",
    "cov = [[1.0,0.8], [0.8, 1.0]]\n",
    "f0 = np.random.multivariate_normal(f0, cov, n_samples // 2)\n",
    "f1 = np.random.multivariate_normal(f1, cov, n_samples // 2)\n",
    "X_2 = np.concatenate([f0, f1])\n",
    "y_2 = np.concatenate([\n",
    "    np.full(n_samples // 2, 1),\n",
    "    np.full(n_samples // 2, -1)\n",
    "])\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = scratch_train_test_split(X_2,y_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-greenhouse",
   "metadata": {},
   "source": [
    "### Dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "rotary-supervisor",
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = np.array([\n",
    "    [-0.44699 , -2.8073  ],[-1.4621  , -2.4586  ],\n",
    "    [ 0.10645 ,  1.9242  ],[-3.5944  , -4.0112  ],\n",
    "    [-0.9888  ,  4.5718  ],[-3.1625  , -3.9606  ],\n",
    "    [ 0.56421 ,  0.72888 ],[-0.60216 ,  8.4636  ],\n",
    "    [-0.61251 , -0.75345 ],[-0.73535 , -2.2718  ],\n",
    "    [-0.80647 , -2.2135  ],[ 0.86291 ,  2.3946  ],\n",
    "    [-3.1108  ,  0.15394 ],[-2.9362  ,  2.5462  ],\n",
    "    [-0.57242 , -2.9915  ],[ 1.4771  ,  3.4896  ],\n",
    "    [ 0.58619 ,  0.37158 ],[ 0.6017  ,  4.3439  ],\n",
    "    [-2.1086  ,  8.3428  ],[-4.1013  , -4.353   ],\n",
    "    [-1.9948  , -1.3927  ],[ 0.35084 , -0.031994],\n",
    "    [ 0.96765 ,  7.8929  ],[-1.281   , 15.6824  ],\n",
    "    [ 0.96765 , 10.083   ],[ 1.3763  ,  1.3347  ],\n",
    "    [-2.234   , -2.5323  ],[-2.9452  , -1.8219  ],\n",
    "    [ 0.14654 , -0.28733 ],[ 0.5461  ,  5.8245  ],\n",
    "    [-0.65259 ,  9.3444  ],[ 0.59912 ,  5.3524  ],\n",
    "    [ 0.50214 , -0.31818 ],[-3.0603  , -3.6461  ],\n",
    "    [-6.6797  ,  0.67661 ],[-2.353   , -0.72261 ],\n",
    "    [ 1.1319  ,  2.4023  ],[-0.12243 ,  9.0162  ],\n",
    "    [-2.5677  , 13.1779  ],[ 0.057313,  5.4681  ],\n",
    "])\n",
    "y3 = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
    "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "\n",
    "X3_train, X3_test, y3_train, y3_test = scratch_train_test_split(X3,y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-settle",
   "metadata": {},
   "source": [
    "### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "exempt-marina",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Classifier\n",
      "Dataset 1 accuracy: 0.94\n",
      "Dataset 2 accuracy: 1.0\n",
      "Dataset 3 accuracy: 0.5714285714285714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\envs\\blending\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "clf = make_pipeline(StandardScaler(),SGDClassifier(max_iter=1000,tol=1e-3))\n",
    "\n",
    "print(\"SGD Classifier\")\n",
    "# Predict for first dataset\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred_data1 = clf.predict(X_test)\n",
    "\n",
    "print(\"Dataset 1 accuracy:\",accuracy_score(y_test, y_pred_data1))\n",
    "\n",
    "#Predict for second dataset\n",
    "clf.fit(X2_train,y2_train)\n",
    "y_pred_data2 = clf.predict(X2_test)\n",
    "print(\"Dataset 2 accuracy:\",accuracy_score(y2_test, y_pred_data2))\n",
    "\n",
    "#Predict for third dataset\n",
    "clf.fit(X3_train,y3_train)\n",
    "y_pred_data3 = clf.predict(X3_test)\n",
    "print(\"Dataset 3 accuracy:\",accuracy_score(y3_test, y_pred_data3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-berlin",
   "metadata": {},
   "source": [
    "### Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "opened-fraud",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classifier\n",
      "Dataset 1 accuracy: 0.98\n",
      "Dataset 2 accuracy: 1.0\n",
      "Dataset 3 accuracy: 0.5714285714285714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\envs\\blending\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = make_pipeline(StandardScaler(),SVC(gamma='auto'))\n",
    "\n",
    "print(\"SVM Classifier\")\n",
    "# Predict for first dataset\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred_data1 = clf.predict(X_test)\n",
    "\n",
    "print(\"Dataset 1 accuracy:\",accuracy_score(y_test, y_pred_data1))\n",
    "\n",
    "#Predict for second dataset\n",
    "clf.fit(X2_train,y2_train)\n",
    "y_pred_data2 = clf.predict(X2_test)\n",
    "print(\"Dataset 2 accuracy:\",accuracy_score(y2_test, y_pred_data2))\n",
    "\n",
    "#Predict for third dataset\n",
    "clf.fit(X3_train,y3_train)\n",
    "y_pred_data3 = clf.predict(X3_test)\n",
    "print(\"Dataset 3 accuracy:\",accuracy_score(y3_test, y_pred_data3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-proof",
   "metadata": {},
   "source": [
    "### Method 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "documented-helen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier\n",
      "Dataset 1 accuracy: 0.96\n",
      "Dataset 2 accuracy: 1.0\n",
      "Dataset 3 accuracy: 0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state = 0)\n",
    "\n",
    "print(\"Decision Tree Classifier\")\n",
    "# Predict for first dataset\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred_data1 = clf.predict(X_test)\n",
    "\n",
    "print(\"Dataset 1 accuracy:\",accuracy_score(y_test, y_pred_data1))\n",
    "\n",
    "#Predict for second dataset\n",
    "clf.fit(X2_train,y2_train)\n",
    "y_pred_data2 = clf.predict(X2_test)\n",
    "print(\"Dataset 2 accuracy:\",accuracy_score(y2_test, y_pred_data2))\n",
    "\n",
    "#Predict for third dataset\n",
    "clf.fit(X3_train,y3_train)\n",
    "y_pred_data3 = clf.predict(X3_test)\n",
    "print(\"Dataset 3 accuracy:\",accuracy_score(y3_test, y_pred_data3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-consideration",
   "metadata": {},
   "source": [
    "## Regression problem\n",
    "Regression then scratches one type.\n",
    "\n",
    "\n",
    "<li> Linear regression </li>\n",
    "\n",
    "For linear regression, use SGDRegressor class, which is calculated using gradient descent.\n",
    "\n",
    "\n",
    "[sklearn.linear_model.SGDRegressor - scikit-lear stable version documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)\n",
    "\n",
    "\n",
    "The data set is from the House Prices competition as in the pre-study period.\n",
    "\n",
    "\n",
    "[House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)\n",
    "\n",
    "\n",
    "Downloadtrain.csvand use SalePriceas the objective variable and GrLivArea and YearBuilt as the explanatory variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-terminology",
   "metadata": {},
   "source": [
    "## Problem 3: Creating a code to solve the regression problem\n",
    "Create code to train and estimate the House Prices data set with linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "relevant-opinion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "featured-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dimensional-highlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_X = data.loc[:,[\"GrLivArea\",\"YearBuilt\"]]\n",
    "my_Y = data.loc[:,\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "hired-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split( np.array(my_X), np.array(my_Y), test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "alpine-crime",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1095, 2)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_reg[:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "combined-acrylic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1095,)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_reg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "right-wright",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1095, 2)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_reg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fatal-navigator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365, 2)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_reg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "unnecessary-swift",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\envs\\blending\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5732564762.679452"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(),SGDClassifier(max_iter=1000,tol=1e-3))\n",
    "# Learn\n",
    "clf.fit(X_train_reg, y_train_reg.reshape(-1,1))\n",
    "#Predict\n",
    "y_predict = clf.predict(X_test_reg)\n",
    "# # Evaluation\n",
    "mean_squared_error(y_test_reg,y_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
